ðŸ“˜ Scenario #101: Pod Communication Failure Due to Network Policy Misconfiguration
Category: Networking
Environment: K8s v1.22, GKE
Scenario Summary: Pods failed to communicate due to a misconfigured NetworkPolicy that blocked ingress traffic.
What Happened: A newly applied NetworkPolicy was too restrictive, preventing communication between certain pods within the same namespace.
Diagnosis Steps:
	â€¢ Used kubectl get networkpolicies to inspect the NetworkPolicy.
	â€¢ Identified that the ingress rules were overly restrictive and did not allow traffic between pods that needed to communicate.
Root Cause: The NetworkPolicy did not account for the required communication between pods.
Fix/Workaround:
	â€¢ Updated the NetworkPolicy to allow the necessary ingress traffic between the affected pods.
	â€¢ Re-applied the NetworkPolicy and tested communication.
Lessons Learned: Network policies need to be tested thoroughly, especially in multi-tenant or complex networking environments.
How to Avoid:
	â€¢ Use staging environments to test NetworkPolicy changes.
	â€¢ Apply policies incrementally and monitor network traffic.

ðŸ“˜ Scenario #102: DNS Resolution Failure Due to CoreDNS Pod Crash
Category: Networking
Environment: K8s v1.21, Azure AKS
Scenario Summary: DNS resolution failed across the cluster after CoreDNS pods crashed unexpectedly.
What Happened: CoreDNS pods were crashed due to resource exhaustion, leading to DNS resolution failure for all services.
Diagnosis Steps:
	â€¢ Used kubectl get pods -n kube-system to check the status of CoreDNS pods.
	â€¢ Found that CoreDNS pods were in a crash loop due to memory resource limits being set too low.
Root Cause: CoreDNS resource limits were too restrictive, causing it to run out of memory.
Fix/Workaround:
	â€¢ Increased memory limits for CoreDNS pods.
	â€¢ Restarted the CoreDNS pods and verified DNS resolution functionality.
Lessons Learned: Ensure CoreDNS has sufficient resources to handle DNS queries for large clusters.
How to Avoid:
	â€¢ Regularly monitor CoreDNS metrics for memory and CPU usage.
	â€¢ Adjust resource limits based on cluster size and traffic patterns.

ðŸ“˜ Scenario #103: Network Latency Due to Misconfigured Service Type
Category: Networking
Environment: K8s v1.18, AWS EKS
Scenario Summary: High network latency occurred because a service was incorrectly configured as a NodePortinstead of a LoadBalancer.
What Happened: Services behind a NodePort experienced high latency due to traffic being routed through each node instead of through an optimized load balancer.
Diagnosis Steps:
	â€¢ Checked the service configuration and identified that the service type was set to NodePort.
	â€¢ Verified that traffic was hitting every node, causing uneven load distribution and high latency.
Root Cause: Incorrect service type that did not provide efficient load balancing.
Fix/Workaround:
	â€¢ Changed the service type to LoadBalancer, which properly routed traffic through a managed load balancer.
	â€¢ Traffic was distributed evenly, and latency was reduced.
Lessons Learned: Choose the correct service type based on traffic patterns and resource requirements.
How to Avoid:
	â€¢ Review service types based on the expected traffic and scalability.
	â€¢ Use a LoadBalancer for production environments requiring high availability.

ðŸ“˜ Scenario #104: Inconsistent Pod-to-Pod Communication Due to MTU Mismatch
Category: Networking
Environment: K8s v1.20, GKE
Scenario Summary: Pod-to-pod communication became inconsistent due to a mismatch in Maximum Transmission Unit (MTU) settings across nodes.
What Happened: Network packets were being fragmented or dropped due to inconsistent MTU settings between nodes.
Diagnosis Steps:
	â€¢ Verified MTU settings on each node using ifconfig and noticed discrepancies between nodes.
	â€¢ Used ping with varying packet sizes to identify where fragmentation or packet loss occurred.
Root Cause: MTU mismatch between nodes and network interfaces.
Fix/Workaround:
	â€¢ Aligned MTU settings across all nodes in the cluster.
	â€¢ Rebooted the nodes to apply the new MTU configuration.
Lessons Learned: Consistent MTU settings are crucial for reliable network communication.
How to Avoid:
	â€¢ Ensure that MTU settings are consistent across all network interfaces in the cluster.
	â€¢ Test network connectivity regularly to ensure that no fragmentation occurs.

ðŸ“˜ Scenario #105: Service Discovery Failure Due to DNS Pod Resource Limits
Category: Networking
Environment: K8s v1.19, Azure AKS
Scenario Summary: Service discovery failed across the cluster due to DNS pod resource limits being exceeded.
What Happened: The DNS service was unable to resolve names due to resource limits being hit on the CoreDNS pods, causing failures in service discovery.
Diagnosis Steps:
	â€¢ Checked CoreDNS pod resource usage and logs, revealing that the memory limit was being exceeded.
	â€¢ Found that DNS requests were timing out, and pods were unable to discover services.
Root Cause: CoreDNS pods hit resource limits, leading to failures in service resolution.
Fix/Workaround:
	â€¢ Increased memory and CPU limits for CoreDNS pods.
	â€¢ Restarted CoreDNS pods and verified that DNS resolution was restored.
Lessons Learned: Service discovery requires sufficient resources to avoid failure.
How to Avoid:
	â€¢ Regularly monitor CoreDNS metrics and adjust resource limits accordingly.
	â€¢ Scale CoreDNS replicas based on cluster size and traffic.

ðŸ“˜ Scenario #106: Pod IP Collision Due to Insufficient IP Range
Category: Networking
Environment: K8s v1.21, GKE
Scenario Summary: Pod IP collisions occurred due to insufficient IP range allocation for the cluster.
What Happened: Pods started having overlapping IPs, causing communication failures between pods.
Diagnosis Steps:
	â€¢ Analyzed pod IPs and discovered that there were overlaps due to an insufficient IP range in the CNI plugin.
	â€¢ Identified that the IP range allocated during cluster creation was too small for the number of pods.
Root Cause: Incorrect IP range allocation when the cluster was initially created.
Fix/Workaround:
	â€¢ Increased the pod network CIDR and restarted the cluster.
	â€¢ Re-deployed the affected pods to new IPs without collisions.
Lessons Learned: Plan IP ranges appropriately during cluster creation to accommodate scaling.
How to Avoid:
	â€¢ Ensure that the IP range for pods is large enough to accommodate future scaling needs.
	â€¢ Monitor IP allocation and usage metrics for early detection of issues.

ðŸ“˜ Scenario #107: Network Bottleneck Due to Single Node in NodePool
Category: Networking
Environment: K8s v1.23, AWS EKS
Scenario Summary: A network bottleneck occurred due to excessive traffic being handled by a single node in the node pool.
What Happened: One node in the node pool was handling all the traffic for multiple pods, leading to CPU and network saturation.
Diagnosis Steps:
	â€¢ Checked node utilization with kubectl top node and identified a single node with high CPU and network load.
	â€¢ Verified the load distribution across the node pool and found uneven traffic handling.
Root Cause: The cluster autoscaler did not scale the node pool correctly due to resource limits on the instance type.
Fix/Workaround:
	â€¢ Increased the size of the node pool and added more nodes with higher resource capacity.
	â€¢ Rebalanced the pods across nodes and monitored for stability.
Lessons Learned: Autoscaler configuration and node resource distribution are critical for handling high traffic.
How to Avoid:
	â€¢ Ensure that the cluster autoscaler is correctly configured to balance resource load across all nodes.
	â€¢ Monitor traffic patterns and node utilization regularly.

ðŸ“˜ Scenario #108: Network Partitioning Due to CNI Plugin Failure
Category: Networking
Environment: K8s v1.18, GKE
Scenario Summary: A network partition occurred when the CNI plugin failed, preventing pods from communicating with each other.
What Happened: The CNI plugin failed to configure networking correctly, causing network partitions within the cluster.
Diagnosis Steps:
	â€¢ Checked CNI plugin logs and found that the plugin was failing to initialize network interfaces for new pods.
	â€¢ Verified pod network connectivity and found that they could not reach services in other namespaces.
Root Cause: Misconfiguration or failure of the CNI plugin, causing networking issues.
Fix/Workaround:
	â€¢ Reinstalled the CNI plugin and applied the correct network configuration.
	â€¢ Re-deployed the affected pods after ensuring the network configuration was correct.
Lessons Learned: Ensure that the CNI plugin is properly configured and functioning.
How to Avoid:
	â€¢ Regularly test the CNI plugin and monitor logs for failures.
	â€¢ Use redundant networking setups to avoid single points of failure.

ðŸ“˜ Scenario #109: Misconfigured Ingress Resource Causing SSL Errors
Category: Networking
Environment: K8s v1.22, Azure AKS
Scenario Summary: SSL certificate errors occurred due to a misconfigured Ingress resource.
What Happened: The Ingress resource had incorrect SSL certificate annotations, causing SSL handshake failures for external traffic.
Diagnosis Steps:
	â€¢ Inspected Ingress resource configuration and identified the wrong certificate annotations.
	â€¢ Verified SSL errors in the logs, confirming SSL handshake issues.
Root Cause: Incorrect SSL certificate annotations in the Ingress resource.
Fix/Workaround:
	â€¢ Corrected the SSL certificate annotations in the Ingress configuration.
	â€¢ Re-applied the Ingress resource and verified successful SSL handshakes.
Lessons Learned: Double-check SSL-related annotations and configurations for ingress resources.
How to Avoid:
	â€¢ Use automated certificate management tools like cert-manager for better SSL certificate handling.
	â€¢ Test SSL connections before deploying ingress resources in production.

ðŸ“˜ Scenario #110: Cluster Autoscaler Fails to Scale Nodes Due to Incorrect IAM Role Permissions
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: The cluster autoscaler failed to scale the number of nodes in response to resource shortages due to missing IAM role permissions for managing EC2 instances.
What Happened: The cluster autoscaler tried to add nodes to the cluster, but due to insufficient IAM permissions, it was unable to interact with EC2 to provision new instances. This led to insufficient resources, affecting pod scheduling.
Diagnosis Steps:
	â€¢ Checked kubectl describe pod and noted that pods were in pending state due to resource shortages.
	â€¢ Analyzed the IAM roles and found that the permissions required by the cluster autoscaler to manage EC2 instances were missing.
Root Cause: Missing IAM role permissions for the cluster autoscaler prevented node scaling.
Fix/Workaround:
	â€¢ Updated the IAM role associated with the cluster autoscaler to include the necessary permissions for EC2 instance provisioning.
	â€¢ Restarted the autoscaler and confirmed that new nodes were added successfully.
Lessons Learned: Ensure that the cluster autoscaler has the required permissions to scale nodes in cloud environments.
How to Avoid:
	â€¢ Regularly review IAM permissions and role configurations for essential services like the cluster autoscaler.
	â€¢ Automate IAM permission audits to catch configuration issues early.

ðŸ“˜ Scenario #111: DNS Resolution Failure Due to Incorrect Pod IP Allocation
Category: Networking
Environment: K8s v1.21, GKE
Scenario Summary: DNS resolution failed due to incorrect IP allocation in the clusterâ€™s CNI plugin.
What Happened: Pods were allocated IPs outside the expected range, causing DNS queries to fail since the DNS service was not able to route correctly.
Diagnosis Steps:
	â€¢ Reviewed the IP range configuration for the CNI plugin and verified that IPs allocated to pods were outside the defined CIDR block.
	â€¢ Observed that pods with incorrect IP addresses couldnâ€™t register with CoreDNS.
Root Cause: Misconfiguration of the CNI pluginâ€™s IP allocation settings.
Fix/Workaround:
	â€¢ Reconfigured the CNI plugin to correctly allocate IPs within the defined range.
	â€¢ Re-deployed affected pods with new IPs that were correctly assigned.
Lessons Learned: Always verify IP range configuration when setting up or scaling CNI plugins.
How to Avoid:
	â€¢ Check IP allocation settings regularly and use monitoring tools to track IP usage.
	â€¢ Ensure CNI plugin configurations align with network architecture requirements.

ðŸ“˜ Scenario #112: Failed Pod-to-Service Communication Due to Port Binding Conflict
Category: Networking
Environment: K8s v1.18, AWS EKS
Scenario Summary: Pods couldnâ€™t communicate with services because of a port binding conflict.
What Happened: A service was configured with a port that was already in use by another pod, causing connectivity issues.
Diagnosis Steps:
	â€¢ Inspected service and pod configurations using kubectl describe to identify the port conflict.
	â€¢ Found that the service port conflicted with the port used by a previously deployed pod.
Root Cause: Port binding conflict caused the service to be unreachable from the pod.
Fix/Workaround:
	â€¢ Changed the port for the service to a free port and re-applied the service configuration.
	â€¢ Verified that pod communication was restored.
Lessons Learned: Properly manage port allocations and avoid conflicts.
How to Avoid:
	â€¢ Use port management strategies and avoid hardcoding ports in services and pods.
	â€¢ Automate port management and checking within deployment pipelines.

ðŸ“˜ Scenario #113: Pod Eviction Due to Network Resource Constraints
Category: Networking
Environment: K8s v1.19, GKE
Scenario Summary: A pod was evicted due to network resource constraints, specifically limited bandwidth.
What Happened: The pod was evicted by the kubelet due to network resource limits being exceeded, leading to a failure in service availability.
Diagnosis Steps:
	â€¢ Used kubectl describe pod to investigate the eviction event and noted network-related resource constraints in the pod eviction message.
	â€¢ Checked node network resource limits and found bandwidth throttling was causing evictions.
Root Cause: Insufficient network bandwidth allocation for the pod.
Fix/Workaround:
	â€¢ Increased network bandwidth limits on the affected node pool.
	â€¢ Re-scheduled the pod on a node with higher bandwidth availability.
Lessons Learned: Network bandwidth limits can impact pod availability and performance.
How to Avoid:
	â€¢ Monitor and adjust network resource allocations regularly.
	â€¢ Use appropriate pod resource requests and limits to prevent evictions.

ðŸ“˜ Scenario #114: Intermittent Network Disconnects Due to MTU Mismatch Between Nodes
Category: Networking
Environment: K8s v1.20, Azure AKS
Scenario Summary: Intermittent network disconnects occurred due to MTU mismatches between different nodes in the cluster.
What Happened: Network packets were being dropped or fragmented between nodes with different MTU settings, causing network instability.
Diagnosis Steps:
	â€¢ Used ping with large payloads to identify packet loss.
	â€¢ Discovered that the MTU was mismatched between the nodes and the network interface.
Root Cause: MTU mismatch between nodes in the cluster.
Fix/Workaround:
	â€¢ Reconfigured the MTU settings on all nodes to match the network interface requirements.
	â€¢ Rebooted nodes to apply the new MTU settings.
Lessons Learned: Consistent MTU settings across all nodes are crucial for stable networking.
How to Avoid:
	â€¢ Ensure that the MTU configuration is uniform across all cluster nodes.
	â€¢ Regularly monitor and verify MTU settings during upgrades.

ðŸ“˜ Scenario #115: Service Load Balancer Failing to Route Traffic to New Pods
Category: Networking
Environment: K8s v1.22, Google GKE
Scenario Summary: Service load balancer failed to route traffic to new pods after scaling up.
What Happened: After scaling up the application pods, the load balancer continued to route traffic to old, terminated pods.
Diagnosis Steps:
	â€¢ Verified pod readiness using kubectl get pods and found that new pods were marked as ready.
	â€¢ Inspected the load balancer configuration and found it was not properly refreshing its backend pool.
Root Cause: The serviceâ€™s load balancer backend pool wasnâ€™t updated when the new pods were created.
Fix/Workaround:
	â€¢ Manually refreshed the load balancerâ€™s backend pool configuration.
	â€¢ Monitored the traffic routing to ensure that it was properly balanced across all pods.
Lessons Learned: Load balancer backends need to be automatically updated with new pods.
How to Avoid:
	â€¢ Configure the load balancer to auto-refresh backend pools on pod changes.
	â€¢ Use health checks to ensure only healthy pods are routed traffic.

ðŸ“˜ Scenario #116: Network Traffic Drop Due to Overlapping CIDR Blocks
Category: Networking
Environment: K8s v1.19, AWS EKS
Scenario Summary: Network traffic dropped due to overlapping CIDR blocks between the VPC and Kubernetes pod network.
What Happened: Overlapping IP ranges between the VPC and pod network caused routing issues and dropped traffic between pods and external services.
Diagnosis Steps:
	â€¢ Reviewed the network configuration and identified the overlap in CIDR blocks.
	â€¢ Used kubectl get pods -o wide to inspect pod IPs and found overlaps with the VPC CIDR block.
Root Cause: Incorrect CIDR block configuration during the cluster setup.
Fix/Workaround:
	â€¢ Reconfigured the pod network CIDR block to avoid overlap with the VPC.
	â€¢ Re-deployed the affected pods and confirmed that traffic flow resumed.
Lessons Learned: Plan CIDR block allocations carefully to avoid conflicts.
How to Avoid:
	â€¢ Plan IP address allocations for both the VPC and Kubernetes network in advance.
	â€¢ Double-check CIDR blocks during the cluster setup phase.

ðŸ“˜ Scenario #117: Misconfigured DNS Resolvers Leading to Service Discovery Failure
Category: Networking
Environment: K8s v1.21, DigitalOcean Kubernetes
Scenario Summary: Service discovery failed due to misconfigured DNS resolvers.
What Happened: A misconfigured DNS resolver in the CoreDNS configuration caused service discovery to fail for some internal services.
Diagnosis Steps:
	â€¢ Checked CoreDNS logs and found that it was unable to resolve certain internal services.
	â€¢ Verified that the DNS resolver settings were pointing to incorrect upstream DNS servers.
Root Cause: Incorrect DNS resolver configuration in the CoreDNS config map.
Fix/Workaround:
	â€¢ Corrected the DNS resolver settings in the CoreDNS configuration.
	â€¢ Re-applied the configuration and verified that service discovery was restored.
Lessons Learned: Always validate DNS resolver configurations during cluster setup.
How to Avoid:
	â€¢ Use default DNS settings if unsure about custom resolver configurations.
	â€¢ Regularly verify DNS functionality within the cluster.

ðŸ“˜ Scenario #118: Intermittent Latency Due to Overloaded Network Interface
Category: Networking
Environment: K8s v1.22, AWS EKS
Scenario Summary: Intermittent network latency occurred due to an overloaded network interface on a single node.
What Happened: One node had high network traffic and was not able to handle the load, causing latency spikes.
Diagnosis Steps:
	â€¢ Checked node resource utilization and identified that the network interface was saturated.
	â€¢ Verified that the traffic was not being distributed evenly across the nodes.
Root Cause: Imbalanced network traffic distribution across the node pool.
Fix/Workaround:
	â€¢ Rebalanced the pod distribution across nodes to reduce load on the overloaded network interface.
	â€¢ Increased network interface resources on the affected node.
Lessons Learned: Proper traffic distribution is key to maintaining low latency.
How to Avoid:
	â€¢ Use autoscaling to dynamically adjust the number of nodes based on traffic load.
	â€¢ Monitor network interface usage closely and optimize traffic distribution.

ðŸ“˜ Scenario #119: Pod Disconnection During Network Partition
Category: Networking
Environment: K8s v1.20, Google GKE
Scenario Summary: Pods were disconnected during a network partition between nodes in the cluster.
What Happened: A temporary network partition between nodes led to pods becoming disconnected from other services.
Diagnosis Steps:
	â€¢ Used kubectl get events to identify the network partition event.
	â€¢ Checked network logs and found that the partition was caused by a temporary routing failure.
Root Cause: Network partition caused pods to lose communication with the rest of the cluster.
Fix/Workaround:
	â€¢ Re-established network connectivity and ensured all nodes could communicate with each other.
	â€¢ Re-scheduled the disconnected pods to different nodes to restore connectivity.
Lessons Learned: Network partitioning can cause severe communication issues between pods.
How to Avoid:
	â€¢ Use redundant network paths and monitor network stability.
	â€¢ Enable pod disruption budgets to ensure availability during network issues.


ðŸ“˜ Scenario #121: Pod-to-Pod Communication Blocked by Network Policies
Category: Networking
Environment: K8s v1.21, AWS EKS
Scenario Summary: Pod-to-pod communication was blocked due to overly restrictive network policies.
What Happened: A network policy was misconfigured, preventing certain pods from communicating with each other despite being within the same namespace.
Diagnosis Steps:
	â€¢ Used kubectl get networkpolicy to inspect the network policies in place.
	â€¢ Found that a policy restricted traffic between pods in the same namespace.
	â€¢ Reviewed policy rules and discovered an incorrect egress restriction.
Root Cause: Misconfigured egress rule in the network policy.
Fix/Workaround:
	â€¢ Modified the network policy to allow traffic between the pods.
	â€¢ Applied the updated policy and verified that communication was restored.
Lessons Learned: Ensure network policies are tested thoroughly before being deployed in production.
How to Avoid:
	â€¢ Use dry-run functionality when applying network policies.
	â€¢ Continuously test policies in a staging environment before production rollout.

ðŸ“˜ Scenario #122: Unresponsive External API Due to DNS Resolution Failure
Category: Networking
Environment: K8s v1.22, DigitalOcean Kubernetes
Scenario Summary: External API calls from the pods failed due to DNS resolution issues for the external domain.
What Happened: DNS queries for an external API failed due to an incorrect DNS configuration in CoreDNS.
Diagnosis Steps:
	â€¢ Checked CoreDNS logs and found that DNS queries for the external API domain were timing out.
	â€¢ Used nslookup to check DNS resolution and found that the query was being routed incorrectly.
Root Cause: Misconfigured upstream DNS server in the CoreDNS configuration.
Fix/Workaround:
	â€¢ Corrected the upstream DNS server settings in CoreDNS.
	â€¢ Restarted CoreDNS pods to apply the new configuration.
Lessons Learned: Proper DNS resolution setup is critical for communication with external APIs.
How to Avoid:
	â€¢ Regularly monitor CoreDNS health and ensure DNS settings are correctly configured.
	â€¢ Use automated health checks to detect DNS issues early.

ðŸ“˜ Scenario #123: Load Balancer Health Checks Failing After Pod Update
Category: Networking
Environment: K8s v1.19, GCP Kubernetes Engine
Scenario Summary: Load balancer health checks failed after updating a pod due to incorrect readiness probe configuration.
What Happened: After deploying a new version of the application, the load balancerâ€™s health checks started failing, causing traffic to be routed to unhealthy pods.
Diagnosis Steps:
	â€¢ Reviewed the load balancer logs and observed failed health checks on newly deployed pods.
	â€¢ Inspected the podâ€™s readiness probe and found that it was configured incorrectly, leading to premature success.
Root Cause: Incorrect readiness probe causing the pod to be marked healthy before it was ready to serve traffic.
Fix/Workaround:
	â€¢ Corrected the readiness probe configuration to reflect the actual application startup time.
	â€¢ Redeployed the updated pods and verified that they passed the health checks.
Lessons Learned: Always validate readiness probes after updates to avoid traffic disruption.
How to Avoid:
	â€¢ Test readiness probes extensively during staging before updating production.
	â€¢ Implement rolling updates to avoid downtime during pod updates.

ðŸ“˜ Scenario #124: Pod Network Performance Degradation After Node Upgrade
Category: Networking
Environment: K8s v1.21, Azure AKS
Scenario Summary: Network performance degraded after an automatic node upgrade, causing latency in pod communication.
What Happened: After an upgrade to a node pool, there was significant latency in network communication between pods, impacting application performance.
Diagnosis Steps:
	â€¢ Checked pod network latency using ping and found increased latency between pods.
	â€¢ Examined node and CNI logs, identifying an issue with the upgraded network interface drivers.
Root Cause: Incompatible network interface drivers following the node upgrade.
Fix/Workaround:
	â€¢ Rolled back the node upgrade and manually updated the network interface drivers on the nodes.
	â€¢ Verified that network performance improved after driver updates.
Lessons Learned: Be cautious when performing automatic upgrades in production environments.
How to Avoid:
	â€¢ Manually test upgrades in a staging environment before applying them to production.
	â€¢ Ensure compatibility of network drivers with the Kubernetes version being used.

ðŸ“˜ Scenario #125: Service IP Conflict Due to CIDR Overlap
Category: Networking
Environment: K8s v1.20, GKE
Scenario Summary: A service IP conflict occurred due to overlapping CIDR blocks, preventing correct routing of traffic to the service.
What Happened: A new service was assigned an IP within a CIDR range already in use by another service, causing traffic to be routed incorrectly.
Diagnosis Steps:
	â€¢ Used kubectl get svc to check the assigned service IPs.
	â€¢ Noticed the overlapping IP range between the two services.
Root Cause: Overlap in CIDR blocks for services in the same network.
Fix/Workaround:
	â€¢ Reconfigured the service CIDR range to avoid conflicts.
	â€¢ Redeployed services with new IP assignments.
Lessons Learned: Plan service CIDR allocations carefully to avoid conflicts.
How to Avoid:
	â€¢ Use a dedicated service CIDR block to ensure that IPs are allocated without overlap.
	â€¢ Automate IP range checks before service creation.
