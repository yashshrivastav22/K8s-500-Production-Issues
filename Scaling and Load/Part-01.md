```
üìò Scenario #401: HPA Didn't Scale Due to Missing Metrics Server
Category: Scaling & Load
Environment: Kubernetes v1.22, Minikube
Summary: Horizontal Pod Autoscaler (HPA) didn‚Äôt scale pods as expected.
What Happened: HPA showed unknown metrics and pod count remained constant despite CPU stress.
Diagnosis Steps:
	‚Ä¢ kubectl get hpa showed Metrics not available.
	‚Ä¢ Confirmed metrics-server not installed.
Root Cause: Metrics server was missing, which is required by HPA for decision making.
Fix/Workaround:
	‚Ä¢ Installed metrics-server using official manifests.
Lessons Learned: HPA silently fails without metrics-server.
How to Avoid:
	‚Ä¢ Include metrics-server in base cluster setup.
	‚Ä¢ Monitor HPA status regularly.

üìò Scenario #402: CPU Throttling Prevented Effective Autoscaling
Category: Scaling & Load
Environment: Kubernetes v1.24, EKS, Burstable QoS
Summary: Application CPU throttled even under low usage, leading to delayed scaling.
What Happened: HPA didn‚Äôt trigger scale-up due to misleading low CPU usage stats.
Diagnosis Steps:
	‚Ä¢ Metrics showed low CPU, but app performance was poor.
	‚Ä¢ kubectl top pod confirmed low utilization.
	‚Ä¢ cgroups showed heavy throttling.
Root Cause: CPU limits were set too close to requests, causing throttling.
Fix/Workaround:
	‚Ä¢ Increased CPU limits or removed them entirely for key services.
Lessons Learned: CPU throttling can suppress scaling metrics.
How to Avoid:
	‚Ä¢ Monitor cgroup throttling stats.
	‚Ä¢ Tune CPU requests/limits carefully.

üìò Scenario #403: Overprovisioned Pods Starved the Cluster
Category: Scaling & Load
Environment: Kubernetes v1.21, on-prem
Summary: Aggressively overprovisioned pod resources led to failed scheduling and throttling.
What Happened: Apps were deployed with excessive CPU/memory, blocking HPA and new workloads.
Diagnosis Steps:
	‚Ä¢ kubectl describe node: Insufficient CPU errors.
	‚Ä¢ Top nodes showed 50% actual usage, 100% requested.
Root Cause: Reserved resources were never used but blocked the scheduler.
Fix/Workaround:
	‚Ä¢ Adjusted requests/limits based on real usage.
Lessons Learned: Resource requests ‚â† real consumption.
How to Avoid:
	‚Ä¢ Right-size pods using VPA recommendations or Prometheus usage data.

üìò Scenario #404: HPA and VPA Conflicted, Causing Flapping
Category: Scaling & Load
Environment: Kubernetes v1.25, GKE
Summary: HPA scaled replicas based on CPU while VPA changed pod resources dynamically, creating instability.
What Happened: HPA scaled up, VPA shrank resources ‚Üí load spike ‚Üí HPA scaled again.
Diagnosis Steps:
	‚Ä¢ Logs showed frequent pod terminations and creations.
	‚Ä¢ Pod count flapped repeatedly.
Root Cause: HPA and VPA were configured on the same deployment without proper coordination.
Fix/Workaround:
	‚Ä¢ Disabled VPA on workloads using HPA.
Lessons Learned: HPA and VPA should be used carefully together.
How to Avoid:
	‚Ä¢ Use HPA for scale-out and VPA for fixed-size workloads.
	‚Ä¢ Avoid combining on the same object.

üìò Scenario #405: Cluster Autoscaler Didn't Scale Due to Pod Affinity Rules
Category: Scaling & Load
Environment: Kubernetes v1.23, AWS EKS
Summary: Workloads couldn't be scheduled and CA didn‚Äôt scale nodes because affinity rules restricted placement.
What Happened: Pods failed to schedule and were stuck in Pending, but no scale-out occurred.
Diagnosis Steps:
	‚Ä¢ Events: FailedScheduling with affinity violations.
	‚Ä¢ CA logs: ‚Äúno matching node group‚Äù.
Root Cause: Pod anti-affinity restricted nodes that CA could provision.
Fix/Workaround:
	‚Ä¢ Relaxed anti-affinity or labeled node groups appropriately.
Lessons Learned: Affinity rules affect autoscaler decisions.
How to Avoid:
	‚Ä¢ Use soft affinity (preferredDuringScheduling) where possible.
	‚Ä¢ Monitor unschedulable pods with alerting.

üìò Scenario #406: Load Test Crashed Cluster Due to Insufficient Node Quotas
Category: Scaling & Load
Environment: Kubernetes v1.24, AKS
Summary: Stress test resulted in API server crash due to unthrottled pod burst.
What Happened: Locust load test created hundreds of pods, exceeding node count limits.
Diagnosis Steps:
	‚Ä¢ API server latency spiked, etcd logs flooded.
	‚Ä¢ Cluster hit node quota limit on Azure.
Root Cause: No upper limit on replica count during load test; hit cloud provider limits.
Fix/Workaround:
	‚Ä¢ Added maxReplicas to HPA.
	‚Ä¢ Throttled CI tests.
Lessons Learned: CI/CD and load tests should obey cluster quotas.
How to Avoid:
	‚Ä¢ Monitor node count vs quota in metrics.
	‚Ä¢ Set maxReplicas in HPA and cap CI workloads.

üìò Scenario #407: Scale-To-Zero Caused Cold Starts and SLA Violations
Category: Scaling & Load
Environment: Kubernetes v1.25, KEDA + Knative
Summary: Pods scaled to zero, but requests during cold start breached SLA.
What Happened: First request after inactivity hit cold-start delay of ~15s.
Diagnosis Steps:
	‚Ä¢ Prometheus response latency showed spikes after idle periods.
	‚Ä¢ Knative logs: cold-start events.
Root Cause: Cold starts on scale-from-zero under high latency constraint.
Fix/Workaround:
	‚Ä¢ Added minReplicaCount: 1 to high-SLA services.
Lessons Learned: Scale-to-zero saves cost, but not for latency-sensitive apps.
How to Avoid:
	‚Ä¢ Use minReplicaCount and warmers for performance-critical services.

üìò Scenario #408: Misconfigured Readiness Probe Blocked HPA Scaling
Category: Scaling & Load
Environment: Kubernetes v1.24, DigitalOcean
Summary: HPA didn‚Äôt scale pods because readiness probes failed and metrics were not reported.
What Happened: Misconfigured probe returned 404, making pods invisible to HPA.
Diagnosis Steps:
	‚Ä¢ kubectl describe pod: readiness failed.
	‚Ä¢ kubectl get hpa: no metrics available.
Root Cause: Failed readiness probes excluded pods from metrics aggregation.
Fix/Workaround:
	‚Ä¢ Corrected readiness endpoint in manifest.
Lessons Learned: HPA only sees "ready" pods.
How to Avoid:
	‚Ä¢ Validate probe paths before production.
	‚Ä¢ Monitor readiness failures via alerts.

üìò Scenario #409: Custom Metrics Adapter Crashed, Breaking Custom HPA
Category: Scaling & Load
Environment: Kubernetes v1.25, Prometheus Adapter
Summary: Custom HPA didn‚Äôt function after metrics adapter pod crashed silently.
What Happened: HPA relying on Prometheus metrics didn't scale for hours.
Diagnosis Steps:
	‚Ä¢ kubectl get hpa: metric unavailable.
	‚Ä¢ Checked prometheus-adapter logs: crashloop backoff.
Root Cause: Misconfigured rules in adapter config caused panic.
Fix/Workaround:
	‚Ä¢ Fixed Prometheus query in adapter configmap.
Lessons Learned: Custom HPA is fragile to adapter errors.
How to Avoid:
	‚Ä¢ Set alerts on prometheus-adapter health.
	‚Ä¢ Validate custom queries before deploy.

üìò Scenario #410: Application Didn‚Äôt Handle Scale-In Gracefully
Category: Scaling & Load
Environment: Kubernetes v1.22, Azure AKS
Summary: App lost in-flight requests during scale-down, causing 5xx spikes.
What Happened: Pods were terminated abruptly during autoscaling down, mid-request.
Diagnosis Steps:
	‚Ä¢ Observed 502/504 errors in logs during scale-in events.
	‚Ä¢ No termination hooks present.
Root Cause: No preStop hooks or graceful shutdown handling in the app.
Fix/Workaround:
	‚Ä¢ Implemented preStop hook with delay.
	‚Ä¢ Added graceful shutdown in app logic.
Lessons Learned: Scale-in should be as graceful as scale-out.
How to Avoid:
	‚Ä¢ Always include termination handling in apps.
	‚Ä¢ Use terminationGracePeriodSeconds wisely.


üìò Scenario #411: Cluster Autoscaler Ignored Pod PriorityClasses
Category: Scaling & Load
Environment: Kubernetes v1.25, AWS EKS with PriorityClasses
Summary: Low-priority workloads blocked scaling of high-priority ones due to misconfigured Cluster Autoscaler.
What Happened: High-priority pods remained pending, even though Cluster Autoscaler was active.
Diagnosis Steps:
	‚Ä¢ kubectl get pods --all-namespaces | grep Pending showed stuck critical workloads.
	‚Ä¢ CA logs indicated scale-up denied due to resource reservation by lower-priority pods.
Root Cause: Default CA config didn't preempt lower-priority pods.
Fix/Workaround:
	‚Ä¢ Enabled preemption.
	‚Ä¢ Re-tuned PriorityClass definitions to align with business SLAs.
Lessons Learned: CA doesn‚Äôt preempt unless explicitly configured.
How to Avoid:
	‚Ä¢ Validate PriorityClass behavior in test environments.
	‚Ä¢ Use preemptionPolicy: PreemptLowerPriority for critical workloads.

üìò Scenario #412: ReplicaSet Misalignment Led to Excessive Scale-Out
Category: Scaling & Load
Environment: Kubernetes v1.23, GKE
Summary: A stale ReplicaSet with label mismatches caused duplicate pod scale-out.
What Happened: Deployment scaled twice the required pod count after an upgrade.
Diagnosis Steps:
	‚Ä¢ kubectl get replicasets showed multiple active sets with overlapping match labels.
	‚Ä¢ Pod count exceeded expected limits.
Root Cause: A new deployment overlapped labels with an old one; HPA acted on both.
Fix/Workaround:
	‚Ä¢ Cleaned up old ReplicaSets.
	‚Ä¢ Scoped matchLabels more tightly.
Lessons Learned: Label discipline is essential for reliable scaling.
How to Avoid:
	‚Ä¢ Use distinct labels per version or release.
	‚Ä¢ Automate cleanup of unused ReplicaSets.

üìò Scenario #413: StatefulSet Didn't Scale Due to PodDisruptionBudget
Category: Scaling & Load
Environment: Kubernetes v1.26, AKS
Summary: StatefulSet couldn‚Äôt scale-in during node pressure due to a restrictive PDB.
What Happened: Nodes under memory pressure tried to evict pods, but eviction was blocked.
Diagnosis Steps:
	‚Ä¢ Checked kubectl describe pdb and kubectl get evictions.
	‚Ä¢ Events showed "Cannot evict pod as it would violate PDB".
Root Cause: PDB defined minAvailable: 100%, preventing any disruption.
Fix/Workaround:
	‚Ä¢ Adjusted PDB to tolerate one pod disruption.
Lessons Learned: Aggressive PDBs block both scaling and upgrades.
How to Avoid:
	‚Ä¢ Use realistic minAvailable or maxUnavailable settings.
	‚Ä¢ Review PDB behavior in test scaling operations.

üìò Scenario #414: Horizontal Pod Autoscaler Triggered by Wrong Metric
Category: Scaling & Load
Environment: Kubernetes v1.24, DigitalOcean
Summary: HPA used memory instead of CPU, causing unnecessary scale-ups.
What Happened: Application scaled even under light CPU usage due to memory caching behavior.
Diagnosis Steps:
	‚Ä¢ HPA target: memory utilization.
	‚Ä¢ kubectl top pods: memory always high due to in-memory cache.
Root Cause: Application design led to consistently high memory usage.
Fix/Workaround:
	‚Ä¢ Switched HPA to CPU metric.
	‚Ä¢ Tuned caching logic in application.
Lessons Learned: Choose scaling metrics that reflect true load.
How to Avoid:
	‚Ä¢ Profile application behavior before configuring HPA.
	‚Ä¢ Avoid memory-based autoscaling unless necessary.

üìò Scenario #415: Prometheus Scraper Bottlenecked Custom HPA Metrics
Category: Scaling & Load
Environment: Kubernetes v1.25, custom metrics + Prometheus Adapter
Summary: Delays in Prometheus scraping caused lag in HPA reactions.
What Happened: HPA lagged 1‚Äì2 minutes behind actual load spike.
Diagnosis Steps:
	‚Ä¢ prometheus-adapter logs showed stale data timestamps.
	‚Ä¢ HPA scale-up occurred after delay.
Root Cause: Scrape interval was 60s, making HPA respond too slowly.
Fix/Workaround:
	‚Ä¢ Reduced scrape interval for critical metrics.
Lessons Learned: Scrape intervals affect autoscaler agility.
How to Avoid:
	‚Ä¢ Match Prometheus scrape intervals with HPA polling needs.
	‚Ä¢ Use rate() or avg_over_time() to smooth metrics.

üìò Scenario #416: Kubernetes Downscaled During Rolling Update
Category: Scaling & Load
Environment: Kubernetes v1.23, on-prem
Summary: Pods were prematurely scaled down during rolling deployment.
What Happened: Rolling update caused a drop in available replicas, triggering autoscaler.
Diagnosis Steps:
	‚Ä¢ Observed spike in 5xx errors during update.
	‚Ä¢ HPA decreased replica count despite live traffic.
Root Cause: Deployment strategy interfered with autoscaling logic.
Fix/Workaround:
	‚Ä¢ Tuned maxUnavailable and minReadySeconds.
	‚Ä¢ Added load-based HPA stabilization window.
Lessons Learned: HPA must be aligned with rolling deployment behavior.
How to Avoid:
	‚Ä¢ Use behavior.scaleDown.stabilizationWindowSeconds.
	‚Ä¢ Monitor scaling decisions during rollouts.

üìò Scenario #417: KEDA Failed to Scale on Kafka Lag Metric
Category: Scaling & Load
Environment: Kubernetes v1.26, KEDA + Kafka
Summary: Consumers didn‚Äôt scale out despite Kafka topic lag.
What Happened: High message lag persisted but consumer replicas remained at baseline.
Diagnosis Steps:
	‚Ä¢ kubectl get scaledobject showed no trigger activation.
	‚Ä¢ Logs: authentication to Kafka metrics endpoint failed.
Root Cause: Incorrect TLS cert in KEDA trigger config.
Fix/Workaround:
	‚Ä¢ Updated Kafka trigger auth to use correct secret.
Lessons Learned: External metric sources require secure, stable access.
How to Avoid:
	‚Ä¢ Validate all trigger auth and endpoints before production.
	‚Ä¢ Alert on trigger activation failures.

üìò Scenario #418: Spike in Load Exceeded Pod Init Time
Category: Scaling & Load
Environment: Kubernetes v1.24, self-hosted
Summary: Sudden burst of traffic overwhelmed services due to slow pod boot time.
What Happened: HPA triggered scale-out, but pods took too long to start. Users got errors.
Diagnosis Steps:
	‚Ä¢ Noticed gap between scale-out and readiness.
	‚Ä¢ Startup probes took 30s+ per pod.
Root Cause: App container had heavy init routines.
Fix/Workaround:
	‚Ä¢ Optimized Docker image layers and moved setup to init containers.
Lessons Learned: Scale-out isn‚Äôt instant; pod readiness matters.
How to Avoid:
	‚Ä¢ Track ReadySeconds vs ReplicaScale delay.
	‚Ä¢ Pre-pull images and optimize pod init time.

üìò Scenario #419: Overuse of Liveness Probes Disrupted Load Balance
Category: Scaling & Load
Environment: Kubernetes v1.21, bare metal
Summary: Misfiring liveness probes killed healthy pods during load test.
What Happened: Sudden scale-out introduced new pods, which were killed due to false negatives on liveness probes.
Diagnosis Steps:
	‚Ä¢ Pod logs showed probe failures under high CPU.
	‚Ä¢ Readiness was OK, liveness killed them anyway.
Root Cause: CPU starvation during load caused probe timeouts.
Fix/Workaround:
	‚Ä¢ Increased probe timeoutSeconds and failureThreshold.
Lessons Learned: Under load, even health checks need headroom.
How to Avoid:
	‚Ä¢ Separate readiness from liveness logic.
	‚Ä¢ Gracefully handle CPU-heavy workloads.

üìò Scenario #420: Scale-In Happened Before Queue Was Drained
Category: Scaling & Load
Environment: Kubernetes v1.26, RabbitMQ + consumers
Summary: Consumers scaled in while queue still had unprocessed messages.
What Happened: Queue depth remained, but pods were terminated.
Diagnosis Steps:
	‚Ä¢ Observed message backlog after autoscaler scale-in.
	‚Ä¢ Consumers had no shutdown hook to drain queue.
Root Cause: Scale-in triggered without consumer workload cleanup.
Fix/Workaround:
	‚Ä¢ Added preStop hook to finish queue processing.
Lessons Learned: Consumers must handle shutdown gracefully.
How to Avoid:
	‚Ä¢ Track message queues with KEDA or custom metrics.
	‚Ä¢ Add drain() logic on signal trap in consumer code.

üìò Scenario #421: Node Drain Race Condition During Scale Down
Category: Scaling & Load
Environment: Kubernetes v1.23, GKE
Summary: Node drain raced with pod termination, causing pod loss.
What Happened: Pods were terminated while the node was still draining, leading to data loss.
Diagnosis Steps:
	‚Ä¢ kubectl describe node showed multiple eviction races.
	‚Ä¢ Pod logs showed abrupt termination without graceful shutdown.
Root Cause: Scale-down process didn‚Äôt wait for node draining to complete fully.
Fix/Workaround:
	‚Ä¢ Adjusted terminationGracePeriodSeconds for pods.
	‚Ä¢ Introduced node draining delay in scaling policy.
Lessons Learned: Node draining should be synchronized with pod termination.
How to Avoid:
	‚Ä¢ Use PodDisruptionBudget to ensure safe scaling.
	‚Ä¢ Implement pod graceful shutdown hooks.

üìò Scenario #422: HPA Disabled Due to Missing Resource Requests
Category: Scaling & Load
Environment: Kubernetes v1.22, AWS EKS
Summary: Horizontal Pod Autoscaler (HPA) failed to trigger because resource requests weren‚Äôt set.
What Happened: HPA couldn‚Äôt scale pods up despite high traffic due to missing CPU/memory resource requests.
Diagnosis Steps:
	‚Ä¢ kubectl describe deployment revealed missing resources.requests.
	‚Ä¢ Logs indicated HPA couldn‚Äôt fetch metrics without resource requests.
Root Cause: Missing resource request fields prevented HPA from making scaling decisions.
Fix/Workaround:
	‚Ä¢ Set proper resources.requests in the deployment YAML.
Lessons Learned: Always define resource requests to enable autoscaling.
How to Avoid:
	‚Ä¢ Define resource requests/limits for every pod.
	‚Ä¢ Enable autoscaling based on requests/limits.

üìò Scenario #423: Unexpected Overprovisioning of Pods
Category: Scaling & Load
Environment: Kubernetes v1.24, DigitalOcean
Summary: Unnecessary pod scaling due to misconfigured resource limits.
What Happened: Pods scaled up unnecessarily due to excessively high resource limits.
Diagnosis Steps:
	‚Ä¢ HPA logs showed frequent scale-ups even during low load.
	‚Ä¢ Resource limits were higher than actual usage.
Root Cause: Overestimated resource limits in pod configuration.
Fix/Workaround:
	‚Ä¢ Reduced resource limits to more realistic values.
Lessons Learned: Proper resource allocation helps prevent scaling inefficiencies.
How to Avoid:
	‚Ä¢ Monitor resource consumption patterns before setting limits.
	‚Ä¢ Use Kubernetes resource usage metrics to adjust configurations.

üìò Scenario #424: Autoscaler Failed During StatefulSet Upgrade
Category: Scaling & Load
Environment: Kubernetes v1.25, AKS
Summary: Horizontal scaling issues occurred during rolling upgrade of StatefulSet.
What Happened: StatefulSet failed to scale out during a rolling upgrade, causing delayed availability of new pods.
Diagnosis Steps:
	‚Ä¢ Observed kubectl get pods showing delayed stateful pod restarts.
	‚Ä¢ HPA did not trigger due to stuck pod state.
Root Cause: Rolling upgrade conflicted with autoscaler logic due to StatefulSet constraints.
Fix/Workaround:
	‚Ä¢ Adjusted StatefulSet rollingUpdate strategy.
	‚Ä¢ Tuned autoscaler thresholds for more aggressive scaling.
Lessons Learned: Ensure compatibility between scaling and StatefulSet updates.
How to Avoid:
	‚Ä¢ Test upgrade and scaling processes in staging environments.
	‚Ä¢ Separate stateful workloads from stateless ones for scaling flexibility.

üìò Scenario #425: Inadequate Load Distribution in a Multi-AZ Setup
Category: Scaling & Load
Environment: Kubernetes v1.27, AWS EKS
Summary: Load balancing wasn‚Äôt even across availability zones, leading to inefficient scaling.
What Happened: More traffic hit one availability zone (AZ), causing scaling delays in the other AZs.
Diagnosis Steps:
	‚Ä¢ Analyzed kubectl describe svc and found skewed traffic distribution.
	‚Ä¢ Observed insufficient pod presence in multiple AZs.
Root Cause: The Kubernetes service didn‚Äôt properly distribute traffic across AZs.
Fix/Workaround:
	‚Ä¢ Updated service to use topologySpreadConstraints for better AZ distribution.
Lessons Learned: Multi-AZ distribution requires proper spread constraints for effective scaling.
How to Avoid:
	‚Ä¢ Use topologySpreadConstraints in services to ensure balanced load.
	‚Ä¢ Review multi-AZ architecture for traffic efficiency.
```
