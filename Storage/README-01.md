üìò Scenario #301: PVC Stuck in Terminating State After Node Crash
Category: Storage
Environment: Kubernetes v1.22, EBS CSI Driver on EKS
Summary: A node crash caused a PersistentVolumeClaim (PVC) to be stuck in Terminating, blocking pod deletion.
What Happened: The node hosting the pod with the PVC crashed and never returned. The volume was still attached, and Kubernetes couldn‚Äôt cleanly unmount or delete it.
Diagnosis Steps:
	‚Ä¢ Described the PVC: status was Terminating.
	‚Ä¢ Checked finalizers on the PVC object.
	‚Ä¢ Verified the volume was still attached to the crashed node via AWS Console.
Root Cause: The volume attachment record wasn‚Äôt cleaned up due to the ungraceful node failure.
Fix/Workaround:
	‚Ä¢ Manually removed the PVC finalizers.
	‚Ä¢ Used aws ec2 detach-volume to forcibly detach.
Lessons Learned: Finalizers can block PVC deletion in edge cases.
How to Avoid:
	‚Ä¢ Use the external-attacher CSI sidecar with leader election.
	‚Ä¢ Implement automation to detect and clean up stuck attachments.

üìò Scenario #302: Data Corruption on HostPath Volumes
Category: Storage
Environment: Kubernetes v1.20, Bare Metal
Summary: Multiple pods sharing a HostPath volume led to inconsistent file states and eventual corruption.
What Happened: Two pods were writing to the same HostPath volume concurrently, which wasn‚Äôt designed for concurrent write access. Files became corrupted due to race conditions.
Diagnosis Steps:
	‚Ä¢ Identified common HostPath mount across pods.
	‚Ä¢ Checked application logs ‚Äî showed file write conflicts.
	‚Ä¢ Inspected corrupted data on disk.
Root Cause: Lack of coordination and access control on shared HostPath.
Fix/Workaround:
	‚Ä¢ Moved workloads to CSI-backed volumes with ReadWriteOnce enforcement.
	‚Ä¢ Ensured only one pod accessed a volume at a time.
Lessons Learned: HostPath volumes offer no isolation or locking guarantees.
How to Avoid:
	‚Ä¢ Use CSI volumes with enforced access modes.
	‚Ä¢ Avoid HostPath unless absolutely necessary.

üìò Scenario #303: Volume Mount Fails Due to Node Affinity Mismatch
Category: Storage
Environment: Kubernetes v1.23, GCE PD on GKE
Summary: A pod was scheduled on a node that couldn‚Äôt access the persistent disk due to zone mismatch.
What Happened: A StatefulSet PVC was bound to a disk in us-central1-a, but the pod got scheduled in us-central1-b, causing volume mount failure.
Diagnosis Steps:
	‚Ä¢ Described pod: showed MountVolume.MountDevice failed.
	‚Ä¢ Described PVC and PV: zone mismatch confirmed.
	‚Ä¢ Looked at scheduler decisions ‚Äî no awareness of volume zone.
Root Cause: Scheduler was unaware of zone constraints on the PV.
Fix/Workaround:
	‚Ä¢ Added topology.kubernetes.io/zone node affinity to match PV.
	‚Ä¢ Ensured StatefulSets used storage classes with volume binding mode WaitForFirstConsumer.
Lessons Learned: Without delayed binding, PVs can bind in zones that don‚Äôt match future pods.
How to Avoid:
	‚Ä¢ Use WaitForFirstConsumer for dynamic provisioning.
	‚Ä¢ Always define zone-aware topology constraints.

üìò Scenario #304: PVC Not Rescheduled After Node Deletion
Category: Storage
Environment: Kubernetes v1.21, Azure Disk CSI
Summary: A StatefulSet pod failed to reschedule after its node was deleted, due to Azure disk still being attached.
What Happened: A pod using Azure Disk was on a node that was manually deleted. Azure did not automatically detach the disk, so rescheduling failed.
Diagnosis Steps:
	‚Ä¢ Pod stuck in ContainerCreating.
	‚Ä¢ CSI logs showed "Volume is still attached to another node".
	‚Ä¢ Azure Portal confirmed volume was attached.
Root Cause: Manual node deletion bypassed volume detachment logic.
Fix/Workaround:
	‚Ä¢ Detached the disk from the Azure console.
	‚Ä¢ Recreated pod successfully on another node.
Lessons Learned: Manual infrastructure changes can break Kubernetes assumptions.
How to Avoid:
	‚Ä¢ Use automation/scripts for safe node draining and deletion.
	‚Ä¢ Monitor CSI detachment status on node removal.

üìò Scenario #305: Long PVC Rebinding Time on StatefulSet Restart
Category: Storage
Environment: Kubernetes v1.24, Rook Ceph
Summary: Restarting a StatefulSet with many PVCs caused long downtime due to slow rebinding.
What Happened: A 20-replica StatefulSet was restarted, and each pod waited for its PVC to rebind and attach. Ceph mount operations were sequential and slow.
Diagnosis Steps:
	‚Ä¢ Pods stuck at Init stage for 15‚Äì20 minutes.
	‚Ä¢ Ceph logs showed delayed attachment per volume.
	‚Ä¢ Described PVCs: bound but not mounted.
Root Cause: Sequential volume mount throttling and inefficient CSI attach policies.
Fix/Workaround:
	‚Ä¢ Tuned CSI attach concurrency.
	‚Ä¢ Split the StatefulSet into smaller chunks.
Lessons Learned: Large-scale StatefulSets need volume attach tuning.
How to Avoid:
	‚Ä¢ Parallelize pod restarts using partitioned rollouts.
	‚Ä¢ Monitor CSI mount throughput.

üìò Scenario #306: CSI Volume Plugin Crash Loops Due to Secret Rotation
Category: Storage
Environment: Kubernetes v1.25, Vault CSI Provider
Summary: Volume plugin entered crash loop after secret provider‚Äôs token was rotated unexpectedly.
What Happened: A service account used by the Vault CSI plugin had its token rotated mid-operation. The plugin couldn‚Äôt fetch new credentials and crashed.
Diagnosis Steps:
	‚Ä¢ CrashLoopBackOff on csi-vault-provider pods.
	‚Ä¢ Logs showed "401 Unauthorized" from Vault.
	‚Ä¢ Verified service account token changed recently.
Root Cause: No logic in plugin to handle token change or re-auth.
Fix/Workaround:
	‚Ä¢ Restarted the CSI plugin pods.
	‚Ä¢ Upgraded plugin to a version with token refresh logic.
Lessons Learned: CSI providers must gracefully handle credential rotations.
How to Avoid:
	‚Ä¢ Use projected service account tokens with auto-refresh.
	‚Ä¢ Monitor plugin health on secret rotations.

üìò Scenario #307: ReadWriteMany PVCs Cause IO Bottlenecks
Category: Storage
Environment: Kubernetes v1.23, NFS-backed PVCs
Summary: Heavy read/write on a shared PVC caused file IO contention and throttling across pods.
What Happened: Multiple pods used a shared ReadWriteMany PVC for scratch space. Concurrent writes led to massive IO wait times and high pod latency.
Diagnosis Steps:
	‚Ä¢ High pod latency and CPU idle time.
	‚Ä¢ Checked NFS server: high disk and network usage.
	‚Ä¢ Application logs showed timeouts.
Root Cause: No coordination or locking on shared writable volume.
Fix/Workaround:
	‚Ä¢ Partitioned workloads to use isolated volumes.
	‚Ä¢ Added cache layer for reads.
Lessons Learned: RWX volumes are not always suitable for concurrent writes.
How to Avoid:
	‚Ä¢ Use RWX volumes for read-shared data only.
	‚Ä¢ Avoid writes unless using clustered filesystems (e.g., CephFS).

üìò Scenario #308: PVC Mount Timeout Due to PodSecurityPolicy
Category: Storage
Environment: Kubernetes v1.21, PSP Enabled Cluster
Summary: A pod couldn‚Äôt mount a volume because PodSecurityPolicy (PSP) rejected required fsGroup.
What Happened: A storage class required fsGroup for volume mount permissions. The pod didn‚Äôt set it, and PSP disallowed dynamic group assignment.
Diagnosis Steps:
	‚Ä¢ Pod stuck in CreateContainerConfigError.
	‚Ä¢ Events showed ‚Äúpod rejected by PSP‚Äù.
	‚Ä¢ Storage class required fsGroup.
Root Cause: Incompatible PSP with volume mount security requirements.
Fix/Workaround:
	‚Ä¢ Modified PSP to allow required fsGroup range.
	‚Ä¢ Updated pod security context.
Lessons Learned: Storage plugins often need security context alignment.
How to Avoid:
	‚Ä¢ Review storage class requirements.
	‚Ä¢ Align security policies with volume specs.

üìò Scenario #309: Orphaned PVs After Namespace Deletion
Category: Storage
Environment: Kubernetes v1.20, Self-Hosted
Summary: Deleting a namespace did not clean up PersistentVolumes, leading to leaked storage.
What Happened: A team deleted a namespace with PVCs, but the associated PVs (with Retain policy) remained and weren‚Äôt cleaned up.
Diagnosis Steps:
	‚Ä¢ Listed all PVs: found orphaned volumes in Released state.
	‚Ä¢ Checked reclaim policy: Retain.
Root Cause: Manual cleanup required for Retain policy.
Fix/Workaround:
	‚Ä¢ Deleted old PVs and disks manually.
	‚Ä¢ Changed reclaim policy to Delete for dynamic volumes.
Lessons Learned: Reclaim policy should match cleanup expectations.
How to Avoid:
	‚Ä¢ Use Delete unless you need manual volume recovery.
	‚Ä¢ Monitor Released PVs for leaks.

üìò Scenario #310: StorageClass Misconfiguration Blocks Dynamic Provisioning
Category: Storage
Environment: Kubernetes v1.25, GKE
Summary: New PVCs failed to bind due to a broken default StorageClass with incorrect parameters.
What Happened: A recent update modified the default StorageClass to use a non-existent disk type. All PVCs created with default settings failed provisioning.
Diagnosis Steps:
	‚Ä¢ PVCs in Pending state.
	‚Ä¢ Checked events: ‚Äúfailed to provision volume with StorageClass‚Äù.
	‚Ä¢ Described StorageClass: invalid parameter type: ssd2.
Root Cause: Mistyped disk type in StorageClass definition.
Fix/Workaround:
	‚Ä¢ Corrected StorageClass parameters.
	‚Ä¢ Manually bound PVCs with valid classes.
Lessons Learned: Default StorageClass affects many workloads.
How to Avoid:
	‚Ä¢ Validate StorageClass on cluster upgrades.
	‚Ä¢ Use automated tests for provisioning paths.

üìò Scenario #311: StatefulSet Volume Cloning Results in Data Leakage
Category: Storage
Environment: Kubernetes v1.24, CSI Volume Cloning enabled
Summary: Cloning PVCs between StatefulSet pods led to shared data unexpectedly appearing in new replicas.
What Happened: Engineers used volume cloning to duplicate data for new pods. They assumed data would be copied and isolated. However, clones preserved file locks and session metadata, which caused apps to behave erratically.
Diagnosis Steps:
	‚Ä¢ New pods accessed old session data unexpectedly.
	‚Ä¢ lsblk and md5sum on cloned volumes showed identical data.
	‚Ä¢ Verified cloning was done via StorageClass that didn't support true snapshot isolation.
Root Cause: Misunderstanding of cloning behavior ‚Äî logical clone ‚â† deep copy.
Fix/Workaround:
	‚Ä¢ Stopped cloning and switched to backup/restore-based provisioning.
	‚Ä¢ Used rsync with integrity checks instead.
Lessons Learned: Not all clones are deep copies; understand your CSI plugin's clone semantics.
How to Avoid:
	‚Ä¢ Use cloning only for stateless data unless supported thoroughly.
	‚Ä¢ Validate cloned volume content before production use.

üìò Scenario #312: Volume Resize Not Reflected in Mounted Filesystem
Category: Storage
Environment: Kubernetes v1.22, OpenEBS
Summary: Volume expansion was successful on the PV, but pods didn‚Äôt see the increased space.
What Happened: After increasing PVC size, the PV reflected the new size, but df -h inside the pod still showed the old size.
Diagnosis Steps:
	‚Ä¢ Checked PVC and PV: showed expanded size.
	‚Ä¢ Pod logs indicated no disk space.
	‚Ä¢ mount inside pod showed volume was mounted but not resized.
Root Cause: Filesystem resize not triggered automatically.
Fix/Workaround:
	‚Ä¢ Restarted pod to remount the volume and trigger resize.
	‚Ä¢ Verified resize2fs logs in CSI driver.
Lessons Learned: Volume resizing may require pod restarts depending on CSI driver.
How to Avoid:
	‚Ä¢ Schedule a rolling restart after volume resize operations.
	‚Ä¢ Check if your CSI driver supports online filesystem resizing.

üìò Scenario #313: CSI Controller Pod Crash Due to Log Overflow
Category: Storage
Environment: Kubernetes v1.23, Longhorn
Summary: The CSI controller crashed repeatedly due to unbounded logging filling up ephemeral storage.
What Happened: A looped RPC error generated thousands of log lines per second. Node /var/log/containers hit 100% disk usage.
Diagnosis Steps:
	‚Ä¢ kubectl describe pod: showed OOMKilled and failed to write logs.
	‚Ä¢ Checked node disk: /var was full.
	‚Ä¢ Logs rotated too slowly.
Root Cause: Verbose logging + missing log throttling + small disk.
Fix/Workaround:
	‚Ä¢ Added log rate limits via CSI plugin config.
	‚Ä¢ Increased node ephemeral storage.
Lessons Learned: Logging misconfigurations can become outages.
How to Avoid:
	‚Ä¢ Monitor log volume and disk usage.
	‚Ä¢ Use log rotation and retention policies.

üìò Scenario #314: PVs Stuck in Released Due to Missing Finalizer Removal
Category: Storage
Environment: Kubernetes v1.21, NFS
Summary: PVCs were deleted, but PVs remained stuck in Released, preventing reuse.
What Happened: PVC deletion left behind PVs marked as Released, and the NFS driver didn‚Äôt remove finalizers, blocking clean-up.
Diagnosis Steps:
	‚Ä¢ Listed PVs: showed Released, with kubernetes.io/pv-protection finalizer still present.
	‚Ä¢ Couldn‚Äôt bind new PVCs due to status: Released.
Root Cause: Driver didn‚Äôt implement Delete reclaim logic properly.
Fix/Workaround:
	‚Ä¢ Patched PVs to remove finalizers.
	‚Ä¢ Recycled or deleted volumes manually.
Lessons Learned: Some drivers require manual cleanup unless fully CSI-compliant.
How to Avoid:
	‚Ä¢ Use CSI drivers with full lifecycle support.
	‚Ä¢ Monitor PV statuses regularly.

üìò Scenario #315: CSI Driver DaemonSet Deployment Missing Tolerations for Taints
Category: Storage
Environment: Kubernetes v1.25, Bare Metal
Summary: CSI Node plugin DaemonSet didn‚Äôt deploy on all nodes due to missing taint tolerations.
What Happened: Storage nodes were tainted (node-role.kubernetes.io/storage:NoSchedule), and the CSI DaemonSet didn‚Äôt tolerate it, so pods failed to mount volumes.
Diagnosis Steps:
	‚Ä¢ CSI node pods not scheduled on certain nodes.
	‚Ä¢ Checked node taints vs DaemonSet tolerations.
	‚Ä¢ Pods stuck in Pending.
Root Cause: Taint/toleration mismatch in CSI node plugin manifest.
Fix/Workaround:
	‚Ä¢ Added required tolerations to DaemonSet.
Lessons Learned: Storage plugins must tolerate relevant node taints to function correctly.
How to Avoid:
	‚Ä¢ Review node taints and CSI tolerations during setup.
	‚Ä¢ Use node affinity and tolerations for critical system components.

üìò Scenario #316: Mount Propagation Issues with Sidecar Containers
Category: Storage
Environment: Kubernetes v1.22, GKE
Summary: Sidecar containers didn‚Äôt see mounted volumes due to incorrect mountPropagation settings.
What Happened: An app container wrote to a mounted path, but sidecar container couldn‚Äôt read the changes.
Diagnosis Steps:
	‚Ä¢ Logs in sidecar showed empty directory.
	‚Ä¢ Checked volumeMounts: missing mountPropagation: Bidirectional.
Root Cause: Default mount propagation is None, blocking volume visibility between containers.
Fix/Workaround:
	‚Ä¢ Added mountPropagation: Bidirectional to shared volumeMounts.
Lessons Learned: Without correct propagation, shared volumes don‚Äôt work across containers.
How to Avoid:
	‚Ä¢ Understand container mount namespaces.
	‚Ä¢ Always define propagation when using shared mounts.

üìò Scenario #317: File Permissions Reset on Pod Restart
Category: Storage
Environment: Kubernetes v1.20, CephFS
Summary: Pod volume permissions reset after each restart, breaking application logic.
What Happened: App wrote files with specific UID/GID. After restart, files were inaccessible due to CephFS resetting ownership.
Diagnosis Steps:
	‚Ä¢ Compared ls -l before/after restart.
	‚Ä¢ Storage class used fsGroup: 9999 by default.
Root Cause: PodSecurityContext didn't override fsGroup, so default applied every time.
Fix/Workaround:
	‚Ä¢ Set explicit securityContext.fsGroup in pod spec.
Lessons Learned: CSI plugins may enforce ownership unless overridden.
How to Avoid:
	‚Ä¢ Always declare expected ownership with securityContext.

üìò Scenario #318: Volume Mount Succeeds but Application Can't Write
Category: Storage
Environment: Kubernetes v1.23, EBS
Summary: Volume mounted correctly, but application failed to write due to filesystem mismatch.
What Happened: App expected xfs but volume formatted as ext4. Some operations silently failed or corrupted.
Diagnosis Steps:
	‚Ä¢ Application logs showed invalid argument on file ops.
	‚Ä¢ CSI driver defaulted to ext4.
	‚Ä¢ Verified with df -T.
Root Cause: Application compatibility issue with default filesystem.
Fix/Workaround:
	‚Ä¢ Used storage class parameter to specify xfs.
Lessons Learned: Filesystem types matter for certain workloads.
How to Avoid:
	‚Ä¢ Align volume formatting with application expectations.

üìò Scenario #319: Volume Snapshot Restore Includes Corrupt Data
Category: Storage
Environment: Kubernetes v1.24, Velero + CSI Snapshots
Summary: Snapshot-based restore brought back corrupted state due to hot snapshot timing.
What Happened: Velero snapshot was taken during active write burst. Filesystem was inconsistent at time of snapshot.
Diagnosis Steps:
	‚Ä¢ App logs showed corrupted files after restore.
	‚Ä¢ Snapshot logs showed no quiescing.
	‚Ä¢ Restore replayed same state.
Root Cause: No pre-freeze or app-level quiescing before snapshot.
Fix/Workaround:
	‚Ä¢ Paused writes before snapshot.
	‚Ä¢ Enabled filesystem freeze hook in Velero plugin.
Lessons Learned: Snapshots must be coordinated with app state.
How to Avoid:
	‚Ä¢ Use pre/post hooks for consistent snapshotting.

üìò Scenario #320: Zombie Volumes Occupying Cloud Quota
Category: Storage
Environment: Kubernetes v1.25, AWS EBS
Summary: Deleted PVCs didn‚Äôt release volumes due to failed detach steps, leading to quota exhaustion.
What Happened: PVCs were deleted, but EBS volumes stayed in-use, blocking provisioning of new ones due to quota limits.
Diagnosis Steps:
	‚Ä¢ Checked AWS Console: volumes remained.
	‚Ä¢ Described events: detach errors during node crash.
Root Cause: CSI driver missed final detach due to abrupt node termination.
Fix/Workaround:
	‚Ä¢ Manually detached and deleted volumes.
	‚Ä¢ Adjusted controller retry limits.
Lessons Learned: Cloud volumes may silently linger even after PVC/PV deletion.
How to Avoid:
	‚Ä¢ Use cloud resource monitoring.
	‚Ä¢ Add alerts for orphaned volumes.

üìò Scenario #321: Volume Snapshot Garbage Collection Fails
Category: Storage
Environment: Kubernetes v1.25, CSI Snapshotter with Velero
Summary: Volume snapshots piled up because snapshot objects were not getting garbage collected after use.
What Happened: Snapshots triggered via Velero remained in the cluster even after restore, eventually exhausting cloud snapshot limits and storage quota.
Diagnosis Steps:
	‚Ä¢ Listed all VolumeSnapshots and VolumeSnapshotContents ‚Äî saw hundreds still in ReadyToUse: true state.
	‚Ä¢ Checked finalizers on snapshot objects ‚Äî found snapshot.storage.kubernetes.io/volumesnapshot not removed.
	‚Ä¢ Velero logs showed successful restore but no cleanup action.
Root Cause: Snapshot GC controller didn‚Äôt remove finalizers due to missing permissions in Velero's service account.
Fix/Workaround:
	‚Ä¢ Added required RBAC rules to Velero.
	‚Ä¢ Manually deleted stale snapshot objects.
Lessons Learned: Improperly configured snapshot permissions can stall GC.
How to Avoid:
	‚Ä¢ Always test snapshot and restore flows end-to-end.
	‚Ä¢ Enable automated cleanup in your backup tooling.

üìò Scenario #322: Volume Mount Delays Due to Node Drain Stale Attachment
Category: Storage
Environment: Kubernetes v1.23, AWS EBS CSI
Summary: Volumes took too long to attach on new nodes after pod rescheduling due to stale attachment metadata.
What Happened: After draining a node for maintenance, workloads failed over, but volume attachments still pointed to old node, causing delays in remount.
Diagnosis Steps:
	‚Ä¢ Described PV: still had attachedNode as drained one.
	‚Ä¢ Cloud logs showed volume in-use errors.
	‚Ä¢ CSI controller didn‚Äôt retry detach fast enough.
Root Cause: Controller had exponential backoff on detach retries.
Fix/Workaround:
	‚Ä¢ Reduced backoff limit in CSI controller config.
	‚Ä¢ Used manual detach via cloud CLI in emergencies.
Lessons Learned: Volume operations can get stuck in edge-node cases.
How to Avoid:
	‚Ä¢ Use health checks to ensure detach success before draining.
	‚Ä¢ Monitor VolumeAttachment objects during node ops.

üìò Scenario #323: Application Writes Lost After Node Reboot
Category: Storage
Environment: Kubernetes v1.21, Local Persistent Volumes
Summary: After a node reboot, pod restarted, but wrote to a different volume path, resulting in apparent data loss.
What Happened: Application data wasn‚Äôt persisted after a power cycle because the mount point dynamically changed.
Diagnosis Steps:
	‚Ä¢ Compared volume paths before and after reboot.
	‚Ä¢ Found PV had hostPath mount with no stable binding.
	‚Ä¢ Volume wasn‚Äôt pinned to specific disk partition.
Root Cause: Local PV was defined with generic hostPath, not using local volume plugin with device references.
Fix/Workaround:
	‚Ä¢ Refactored PV to use local with nodeAffinity.
	‚Ä¢ Explicitly mounted disk partitions.
Lessons Learned: hostPath should not be used for production data.
How to Avoid:
	‚Ä¢ Always use local storage plugin for node-local disks.
	‚Ä¢ Avoid loosely defined persistent paths.

üìò Scenario #324: Pod CrashLoop Due to Read-Only Volume Remount
Category: Storage
Environment: Kubernetes v1.22, GCP Filestore
Summary: Pod volume was remounted as read-only after a transient network disconnect, breaking app write logic.
What Happened: During a brief NFS outage, volume was remounted in read-only mode by the NFS client. Application kept crashing due to inability to write logs.
Diagnosis Steps:
	‚Ä¢ Checked mount logs: showed NFS remounted as read-only.
	‚Ä¢ kubectl describe pod: showed volume still mounted.
	‚Ä¢ Pod logs: permission denied on write.
Root Cause: NFS client behavior defaults to remount as read-only after timeout.
Fix/Workaround:
	‚Ä¢ Restarted pod to trigger clean remount.
	‚Ä¢ Tuned NFS mount options (soft, timeo, retry).
Lessons Learned: NFS remount behavior can silently switch access mode.
How to Avoid:
	‚Ä¢ Monitor for dmesg or NFS client remounts.
	‚Ä¢ Add alerts for unexpected read-only volume transitions.

üìò Scenario #325: Data Corruption on Shared Volume With Two Pods
Category: Storage
Environment: Kubernetes v1.23, NFS PVC shared by 2 pods
Summary: Two pods writing to the same volume caused inconsistent files and data loss.
What Happened: Both pods ran jobs writing to the same output files. Without file locking, one pod overwrote data from the other.
Diagnosis Steps:
	‚Ä¢ Logs showed incomplete file writes.
	‚Ä¢ File hashes changed mid-run.
	‚Ä¢ No mutual exclusion mechanism implemented.
Root Cause: Shared volume used without locking or coordination between pods.
Fix/Workaround:
	‚Ä¢ Refactored app logic to coordinate file writes via leader election.
	‚Ä¢ Used a queue-based processing system.
Lessons Learned: Shared volume access must be controlled explicitly.
How to Avoid:
	‚Ä¢ Never assume coordination when using shared volumes.
	‚Ä¢ Use per-pod PVCs or job-level locking.
