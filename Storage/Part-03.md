```
üìò Scenario #351: VolumeMount Conflict Between Init and Main Containers
Category: Storage
Environment: Kubernetes v1.25, containerized database restore job
Summary: Init container and main container used the same volume path but with different modes, causing the main container to crash.
What Happened: An init container wrote a backup file to a shared volume. The main container expected a clean mount, found conflicting content, and failed on startup.
Diagnosis Steps:
	‚Ä¢ Pod logs showed file already exists error.
	‚Ä¢ Examined pod manifest: both containers used the same volumeMount.path.
Root Cause: Shared volume path caused file conflicts between lifecycle stages.
Fix/Workaround:
	‚Ä¢ Used a subPath for the init container to isolate file writes.
	‚Ä¢ Moved backup logic to an external init job.
Lessons Learned: Volume sharing across containers must be carefully scoped.
How to Avoid:
	‚Ä¢ Always use subPath if write behavior differs.
	‚Ä¢ Isolate volume use per container stage when possible.

üìò Scenario #352: PVCs Stuck in ‚ÄúTerminating‚Äù Due to Finalizers
Category: Storage
Environment: Kubernetes v1.24, CSI driver with finalizer
Summary: After deleting PVCs, they remained in Terminating state indefinitely due to stuck finalizers.
What Happened: The CSI driver responsible for finalizer cleanup was crash-looping, preventing PVC finalizer execution.
Diagnosis Steps:
	‚Ä¢ PVCs had finalizer external-attacher.csi.driver.io.
	‚Ä¢ CSI pod logs showed repeated panics due to malformed config.
Root Cause: Driver bug prevented cleanup logic, blocking PVC deletion.
Fix/Workaround:
	‚Ä¢ Patched the driver deployment.
	‚Ä¢ Manually removed finalizers using kubectl patch.
Lessons Learned: CSI finalizer bugs can block resource lifecycle.
How to Avoid:
	‚Ä¢ Regularly update CSI drivers.
	‚Ä¢ Monitor PVC lifecycle duration metrics.

üìò Scenario #353: Misconfigured ReadOnlyMany Mount Blocks Write Operations
Category: Storage
Environment: Kubernetes v1.23, NFS volume
Summary: Volume mounted as ReadOnlyMany blocked necessary write operations, despite NFS server allowing writes.
What Happened: VolumeMount was incorrectly marked as readOnly: true. Application failed on write attempts.
Diagnosis Steps:
	‚Ä¢ Application logs: read-only filesystem.
	‚Ä¢ Pod manifest showed readOnly: true.
Root Cause: Misconfiguration in the volumeMounts spec.
Fix/Workaround:
	‚Ä¢ Updated the manifest to readOnly: false.
Lessons Learned: Read-only flags silently break expected behavior.
How to Avoid:
	‚Ä¢ Validate volume mount flags in CI.
	‚Ä¢ Use initContainer to test mount behavior.

üìò Scenario #354: In-Tree Plugin PVs Lost After Driver Migration
Category: Storage
Environment: Kubernetes v1.26, in-tree to CSI migration
Summary: Existing in-tree volumes became unrecognized after enabling CSI migration.
What Happened: Migrated GCE volumes to CSI plugin. Old PVs had legacy annotations and didn‚Äôt bind correctly.
Diagnosis Steps:
	‚Ä¢ PVs showed Unavailable state.
	‚Ä¢ Migration feature gates enabled but missing annotations.
Root Cause: Backward incompatibility in migration logic for pre-existing PVs.
Fix/Workaround:
	‚Ä¢ Manually edited PV annotations to match CSI requirements.
Lessons Learned: Migration feature gates must be tested in staging.
How to Avoid:
	‚Ä¢ Run migration with shadow mode first.
	‚Ä¢ Migrate PVs gradually using tools like pv-migrate.

üìò Scenario #355: Pod Deleted but Volume Still Mounted on Node
Category: Storage
Environment: Kubernetes v1.24, CSI
Summary: Pod was force-deleted, but its volume wasn‚Äôt unmounted from the node, blocking future pod scheduling.
What Happened: Force deletion bypassed CSI driver cleanup. Mount lingered and failed future pod volume attach.
Diagnosis Steps:
	‚Ä¢ kubectl describe node showed volume still attached.
	‚Ä¢ lsblk confirmed mount on node.
	‚Ä¢ Logs showed attach errors.
Root Cause: Orphaned mount due to force deletion.
Fix/Workaround:
	‚Ä¢ Manually unmounted the volume on node.
	‚Ä¢ Drained and rebooted the node.
Lessons Learned: Forced pod deletions should be last resort.
How to Avoid:
	‚Ä¢ Set up automated orphaned mount detection scripts.
	‚Ä¢ Use graceful deletion with finalizer handling.

üìò Scenario #356: Ceph RBD Volume Crashes Pods Under IOPS Saturation
Category: Storage
Environment: Kubernetes v1.23, Ceph CSI
Summary: Under heavy I/O, Ceph volumes became unresponsive, leading to kernel-level I/O errors in pods.
What Happened: Application workload created sustained random writes. Ceph cluster‚Äôs IOPS limit was reached.
Diagnosis Steps:
	‚Ä¢ dmesg logs: blk_update_request: I/O error.
	‚Ä¢ Pod logs: database fsync errors.
	‚Ä¢ Ceph health: HEALTH_WARN: slow ops.
Root Cause: Ceph RBD pool under-provisioned for the workload.
Fix/Workaround:
	‚Ä¢ Migrated to SSD-backed Ceph pools.
	‚Ä¢ Throttled application concurrency.
Lessons Learned: Distributed storage systems fail silently under stress.
How to Avoid:
	‚Ä¢ Benchmark storage before rollout.
	‚Ä¢ Alert on high RBD latency.

üìò Scenario #357: ReplicaSet Using PVCs Fails Due to VolumeClaimTemplate Misuse
Category: Storage
Environment: Kubernetes v1.25
Summary: Developer tried using volumeClaimTemplates in a ReplicaSet manifest, which isn‚Äôt supported.
What Happened: Deployment applied, but pods failed to create PVCs.
Diagnosis Steps:
	‚Ä¢ Controller logs: volumeClaimTemplates is not supported in ReplicaSet.
	‚Ä¢ No PVCs appeared in kubectl get pvc.
Root Cause: volumeClaimTemplates is only supported in StatefulSet.
Fix/Workaround:
	‚Ä¢ Refactored ReplicaSet to StatefulSet.
Lessons Learned: Not all workload types support dynamic PVCs.
How to Avoid:
	‚Ä¢ Use workload reference charts during manifest authoring.
	‚Ä¢ Validate manifests with policy engines like OPA.

üìò Scenario #358: Filesystem Type Mismatch During Volume Attach
Category: Storage
Environment: Kubernetes v1.24, ext4 vs xfs
Summary: A pod failed to start because the PV expected ext4 but the node formatted it as xfs.
What Happened: Pre-provisioned disk had xfs, but StorageClass defaulted to ext4.
Diagnosis Steps:
	‚Ä¢ Attach logs: mount failed: wrong fs type.
	‚Ä¢ blkid on node showed xfs.
Root Cause: Filesystem mismatch between PV and node assumptions.
Fix/Workaround:
	‚Ä¢ Reformatted disk to ext4.
	‚Ä¢ Aligned StorageClass with PV fsType.
Lessons Learned: Filesystem types must match across the stack.
How to Avoid:
	‚Ä¢ Explicitly set fsType in StorageClass.
	‚Ä¢ Document provisioner formatting logic.

üìò Scenario #359: iSCSI Volumes Fail After Node Kernel Upgrade
Category: Storage
Environment: Kubernetes v1.26, CSI iSCSI plugin
Summary: Post-upgrade, all pods using iSCSI volumes failed to mount due to kernel module incompatibility.
What Happened: Kernel upgrade removed or broke iscsi_tcp module needed by CSI driver.
Diagnosis Steps:
	‚Ä¢ CSI logs: no such device iscsi_tcp.
	‚Ä¢ modprobe iscsi_tcp failed.
	‚Ä¢ Pod events: mount timeout.
Root Cause: Node image didn‚Äôt include required kernel modules post-upgrade.
Fix/Workaround:
	‚Ä¢ Installed open-iscsi and related modules.
	‚Ä¢ Rebooted node.
Lessons Learned: OS updates can break CSI compatibility.
How to Avoid:
	‚Ä¢ Pin node kernel versions.
	‚Ä¢ Run upgrade simulations in canary clusters.

üìò Scenario #360: PVs Not Deleted After PVC Cleanup Due to Retain Policy
Category: Storage
Environment: Kubernetes v1.23, AWS EBS
Summary: After PVCs were deleted, underlying PVs and disks remained, leading to cloud resource sprawl.
What Happened: Retain policy on the PV preserved the disk after PVC was deleted.
Diagnosis Steps:
	‚Ä¢ kubectl get pv showed status Released.
	‚Ä¢ Disk still visible in AWS console.
Root Cause: PV reclaimPolicy was Retain, not Delete.
Fix/Workaround:
	‚Ä¢ Manually deleted PVs and EBS volumes.
Lessons Learned: Retain policy needs operational follow-up.
How to Avoid:
	‚Ä¢ Use Delete policy unless manual cleanup is required.
	‚Ä¢ Audit dangling PVs regularly.

üìò Scenario #361: Concurrent Pod Scheduling on the Same PVC Causes Mount Conflict
Category: Storage
Environment: Kubernetes v1.24, AWS EBS, ReadWriteOnce PVC
Summary: Two pods attempted to use the same PVC simultaneously, causing one pod to be stuck in ContainerCreating.
What Happened: A deployment scale-up triggered duplicate pods trying to mount the same EBS volume on different nodes.
Diagnosis Steps:
	‚Ä¢ One pod was running, the other stuck in ContainerCreating.
	‚Ä¢ Events showed Volume is already attached to another node.
Root Cause: EBS supports ReadWriteOnce, not multi-node attach.
Fix/Workaround:
	‚Ä¢ Added anti-affinity to restrict pod scheduling to a single node.
	‚Ä¢ Used EFS (ReadWriteMany) for workloads needing shared storage.
Lessons Learned: Not all storage supports multi-node access.
How to Avoid:
	‚Ä¢ Understand volume access modes.
	‚Ä¢ Use StatefulSets or anti-affinity for PVC sharing.

üìò Scenario #362: StatefulSet Pod Replacement Fails Due to PVC Retention
Category: Storage
Environment: Kubernetes v1.23, StatefulSet with volumeClaimTemplates
Summary: Deleted a StatefulSet pod manually, but new pod failed due to existing PVC conflict.
What Happened: PVC persisted after pod deletion due to StatefulSet retention policy.
Diagnosis Steps:
	‚Ä¢ kubectl get pvc showed PVC still bound.
	‚Ä¢ New pod stuck in Pending.
Root Cause: StatefulSet retains PVCs unless explicitly deleted.
Fix/Workaround:
	‚Ä¢ Deleted old PVC manually to let StatefulSet recreate it.
Lessons Learned: Stateful PVCs are tightly coupled to pod identity.
How to Avoid:
	‚Ä¢ Use persistentVolumeReclaimPolicy: Delete only when data can be lost.
	‚Ä¢ Automate cleanup for failed StatefulSet replacements.

üìò Scenario #363: HostPath Volume Access Leaks Host Data into Container
Category: Storage
Environment: Kubernetes v1.22, single-node dev cluster
Summary: HostPath volume mounted the wrong directory, exposing sensitive host data to the container.
What Happened: Misconfigured path / instead of /data allowed container full read access to host.
Diagnosis Steps:
	‚Ä¢ Container listed host files under /mnt/host.
	‚Ä¢ Pod manifest showed path: /.
Root Cause: Typo in the volume path.
Fix/Workaround:
	‚Ä¢ Corrected volume path in manifest.
	‚Ä¢ Revoked pod access.
Lessons Learned: HostPath has minimal safety nets.
How to Avoid:
	‚Ä¢ Avoid using HostPath unless absolutely necessary.
	‚Ä¢ Validate mount paths through automated policies.

üìò Scenario #364: CSI Driver Crashes When Node Resource Is Deleted Prematurely
Category: Storage
Environment: Kubernetes v1.25, custom CSI driver
Summary: Deleting a node object before the CSI driver detached volumes caused crash loops.
What Happened: Admin manually deleted a node before volume detach completed.
Diagnosis Steps:
	‚Ä¢ CSI logs showed panic due to missing node metadata.
	‚Ä¢ Pods remained in Terminating.
Root Cause: Driver attempted to clean up mounts from a non-existent node resource.
Fix/Workaround:
	‚Ä¢ Waited for CSI driver to timeout and self-recover.
	‚Ä¢ Rebooted node to forcibly detach volumes.
Lessons Learned: Node deletion should follow strict lifecycle policies.
How to Avoid:
	‚Ä¢ Use node cordon + drain before deletion.
	‚Ä¢ Monitor CSI cleanup completion before proceeding.

üìò Scenario #365: Retained PV Blocks New Claim Binding with Identical Name
Category: Storage
Environment: Kubernetes v1.21, NFS
Summary: A PV stuck in Released state with Retain policy blocked new PVCs from binding with the same name.
What Happened: Deleted old PVC and recreated a new one with the same name, but it stayed Pending.
Diagnosis Steps:
	‚Ä¢ PV was in Released, PVC was Pending.
	‚Ä¢ Events: PVC is not bound.
Root Cause: Retained PV still owned the identity, blocking rebinding.
Fix/Workaround:
	‚Ä¢ Manually deleted the old PV to allow dynamic provisioning.
Lessons Learned: Retain policies require admin cleanup.
How to Avoid:
	‚Ä¢ Use Delete policy for short-lived PVCs.
	‚Ä¢ Automate orphan PV audits.

üìò Scenario #366: CSI Plugin Panic on Missing Mount Option
Category: Storage
Environment: Kubernetes v1.26, custom CSI plugin
Summary: Missing mountOptions in StorageClass led to runtime nil pointer exception in CSI driver.
What Happened: StorageClass defined mountOptions: null, causing driver to crash during attach.
Diagnosis Steps:
	‚Ä¢ CSI logs showed panic: nil pointer dereference.
	‚Ä¢ StorageClass YAML had an empty mountOptions: field.
Root Cause: Plugin didn't check for nil before reading options.
Fix/Workaround:
	‚Ä¢ Removed mountOptions: from manifest.
	‚Ä¢ Patched CSI driver to add nil checks.
Lessons Learned: CSI drivers must gracefully handle incomplete specs.
How to Avoid:
	‚Ä¢ Validate StorageClass manifests.
	‚Ä¢ Write defensive CSI plugin code.

üìò Scenario #367: Pod Fails to Mount Volume Due to SELinux Context Mismatch
Category: Storage
Environment: Kubernetes v1.24, RHEL with SELinux enforcing
Summary: Pod failed to mount volume due to denied SELinux permissions.
What Happened: Volume was created with an incorrect SELinux context, preventing pod access.
Diagnosis Steps:
	‚Ä¢ Pod logs: permission denied.
	‚Ä¢ dmesg showed SELinux AVC denial.
Root Cause: Volume not labeled with container_file_t.
Fix/Workaround:
	‚Ä¢ Relabeled volume with chcon -Rt container_file_t /data.
Lessons Learned: SELinux can silently block mounts.
How to Avoid:
	‚Ä¢ Use CSI drivers that support SELinux integration.
	‚Ä¢ Validate volume contexts post-provisioning.

üìò Scenario #368: VolumeExpansion on Bound PVC Fails Due to Pod Running
Category: Storage
Environment: Kubernetes v1.25, GCP PD
Summary: PVC resize operation failed because the pod using it was still running.
What Happened: Tried to resize a PVC while its pod was active.
Diagnosis Steps:
	‚Ä¢ PVC showed Resizing then back to Bound.
	‚Ä¢ Events: PVC resize failed while volume in use.
Root Cause: Filesystem resize required pod to restart.
Fix/Workaround:
	‚Ä¢ Deleted pod to trigger offline volume resize.
	‚Ä¢ PVC then showed FileSystemResizePending ‚Üí Bound.
Lessons Learned: Some resizes need pod restart.
How to Avoid:
	‚Ä¢ Plan PVC expansion during maintenance.
	‚Ä¢ Use fsResizePolicy: "OnRestart" if supported.

üìò Scenario #369: CSI Driver Memory Leak on Volume Detach Loop
Category: Storage
Environment: Kubernetes v1.24, external CSI
Summary: CSI plugin leaked memory due to improper garbage collection on detach failure loop.
What Happened: Detach failed repeatedly due to stale metadata, causing plugin to grow in memory use.
Diagnosis Steps:
	‚Ä¢ Plugin memory exceeded 1GB.
	‚Ä¢ Logs showed repeated detach failed with no backoff.
Root Cause: Driver retry loop without cleanup or GC.
Fix/Workaround:
	‚Ä¢ Restarted CSI plugin.
	‚Ä¢ Patched driver to implement exponential backoff.
Lessons Learned: CSI error paths need memory safety.
How to Avoid:
	‚Ä¢ Stress-test CSI paths for failure.
	‚Ä¢ Add Prometheus memory alerts for plugins.

üìò Scenario #370: Volume Mount Timeout Due to Slow Cloud API
Category: Storage
Environment: Kubernetes v1.23, Azure Disk CSI
Summary: During a cloud outage, Azure Disk operations timed out, blocking pod mounts.
What Happened: Pods remained in ContainerCreating due to delayed volume attachment.
Diagnosis Steps:
	‚Ä¢ Event logs: timed out waiting for attach.
	‚Ä¢ Azure portal showed degraded disk API service.
Root Cause: Cloud provider API latency blocked CSI attach.
Fix/Workaround:
	‚Ä¢ Waited for Azure API to stabilize.
	‚Ä¢ Used local PVs for critical workloads moving forward.
Lessons Learned: Cloud API reliability is a hidden dependency.
How to Avoid:
	‚Ä¢ Use local volumes or ephemeral storage for high-availability needs.
	‚Ä¢ Monitor CSI attach/detach durations.

üìò Scenario #371: Volume Snapshot Restore Misses Application Consistency
Category: Storage
Environment: Kubernetes v1.26, Velero with CSI VolumeSnapshot
Summary: Snapshot restore completed successfully, but restored app data was corrupt.
What Happened: A volume snapshot was taken while the database was mid-write. Restore completed, but database wouldn't start due to file inconsistencies.
Diagnosis Steps:
	‚Ä¢ Restored volume had missing WAL files.
	‚Ä¢ Database logs showed corruption errors.
	‚Ä¢ Snapshot logs showed no pre-freeze hook execution.
Root Cause: No coordination between snapshot and application quiescence.
Fix/Workaround:
	‚Ä¢ Integrated pre-freeze and post-thaw hooks via Velero Restic.
	‚Ä¢ Enabled application-aware backups.
Lessons Learned: Volume snapshot ‚â† app-consistent backup.
How to Avoid:
	‚Ä¢ Use app-specific backup tools or hooks.
	‚Ä¢ Never snapshot during heavy write activity.

üìò Scenario #372: File Locking Issue Between Multiple Pods on NFS
Category: Storage
Environment: Kubernetes v1.22, NFS with ReadWriteMany
Summary: Two pods wrote to the same file concurrently, causing lock conflicts and data loss.
What Happened: Lack of advisory file locking on the NFS server led to race conditions between pods.
Diagnosis Steps:
	‚Ä¢ Log files had overlapping, corrupted data.
	‚Ä¢ File locks were not honored.
Root Cause: POSIX locks not enforced reliably over NFS.
Fix/Workaround:
	‚Ä¢ Introduced flock-based locking in application code.
	‚Ä¢ Used local persistent volume instead for critical data.
Lessons Learned: NFS doesn‚Äôt guarantee strong file locking semantics.
How to Avoid:
	‚Ä¢ Architect apps to handle distributed file access carefully.
	‚Ä¢ Avoid shared writable files unless absolutely needed.

üìò Scenario #373: Pod Reboots Erase Data on EmptyDir Volume
Category: Storage
Environment: Kubernetes v1.24, default EmptyDir
Summary: Pod restarts caused in-memory volume to be wiped, resulting in lost logs.
What Happened: Logging container used EmptyDir with memory medium. Node rebooted, and logs were lost.
Diagnosis Steps:
	‚Ä¢ Post-reboot, EmptyDir was reinitialized.
	‚Ä¢ Logs had disappeared from the container volume.
Root Cause: EmptyDir with medium: Memory is ephemeral and tied to node lifecycle.
Fix/Workaround:
	‚Ä¢ Switched to hostPath for logs or persisted to object storage.
Lessons Learned: Understand EmptyDir behavior before using for critical data.
How to Avoid:
	‚Ä¢ Use PVs or centralized logging for durability.
	‚Ä¢ Avoid medium: Memory unless necessary.

üìò Scenario #374: PVC Resize Fails on In-Use Block Device
Category: Storage
Environment: Kubernetes v1.25, CSI with block mode
Summary: PVC expansion failed for a block device while pod was still running.
What Happened: Attempted to resize a raw block volume without terminating the consuming pod.
Diagnosis Steps:
	‚Ä¢ PVC stuck in Resizing.
	‚Ä¢ Logs: device busy.
Root Cause: Some storage providers require offline resizing for block devices.
Fix/Workaround:
	‚Ä¢ Stopped the pod and retried resize.
Lessons Learned: Raw block volumes behave differently than filesystem PVCs.
How to Avoid:
	‚Ä¢ Schedule maintenance windows for volume changes.
	‚Ä¢ Know volume mode differences.

üìò Scenario #375: Default StorageClass Prevents PVC Binding to Custom Class
Category: Storage
Environment: Kubernetes v1.23, GKE
Summary: A PVC remained in Pending because the default StorageClass kept getting assigned instead of a custom one.
What Happened: PVC YAML didn‚Äôt specify storageClassName, so the default one was used.
Diagnosis Steps:
	‚Ä¢ PVC described with wrong StorageClass.
	‚Ä¢ Events: no matching PV.
Root Cause: Default StorageClass mismatch with intended PV type.
Fix/Workaround:
	‚Ä¢ Explicitly set storageClassName in the PVC.
Lessons Learned: Implicit defaults can cause hidden behavior.
How to Avoid:
	‚Ä¢ Always specify StorageClass explicitly in manifests.
	‚Ä¢ Audit your cluster‚Äôs default classes.
```
