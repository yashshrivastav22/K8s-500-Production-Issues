```
ðŸ“˜ Scenario #76: Kubernetes Master Node Unresponsive After High Load
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: The Kubernetes master node became unresponsive under high load due to excessive API server calls and high memory usage.
What Happened: The Kubernetes master node was overwhelmed by API calls and high memory consumption, leading to a failure to respond to management requests.
Diagnosis Steps:
	â€¢ Checked the control plane resource usage and found high memory and CPU consumption on the master node.
	â€¢ Analyzed API server logs and found a spike in incoming requests.
Root Cause: Excessive incoming requests caused API server memory to spike, rendering the master node unresponsive.
Fix/Workaround:
	â€¢ Implemented API rate limiting to control excessive calls.
	â€¢ Increased the memory allocated to the master node.
Lessons Learned: Ensure that the control plane is protected against overloads and is properly scaled.
How to Avoid:
	â€¢ Use API rate limiting and load balancing techniques for the master node.
	â€¢ Consider separating the control plane and worker nodes for better scalability.

ðŸ“˜ Scenario #77: Failed Pod Restart Due to Inadequate Node Affinity
Category: Cluster Management
Environment: K8s v1.24, GKE
Scenario Summary: Pods failed to restart on available nodes due to overly strict node affinity rules.
What Happened: A pod failed to restart after a node failure because the node affinity rules were too strict, preventing the pod from being scheduled on any available nodes.
Diagnosis Steps:
	â€¢ Checked pod logs and observed affinity errors in scheduling.
	â€¢ Analyzed the affinity settings in the pod spec and found restrictive affinity rules.
Root Cause: Strict node affinity rules prevented the pod from being scheduled on available nodes.
Fix/Workaround:
	â€¢ Relaxed the node affinity rules in the pod spec.
	â€¢ Redeployed the pod, and it successfully restarted on an available node.
Lessons Learned: Carefully configure node affinity rules to allow flexibility during pod rescheduling.
How to Avoid:
	â€¢ Use less restrictive affinity rules for better pod rescheduling flexibility.
	â€¢ Test affinity rules during node maintenance and scaling operations.

ðŸ“˜ Scenario #78: ReplicaSet Scaling Issues Due to Resource Limits
Category: Cluster Management
Environment: K8s v1.19, AWS EKS
Scenario Summary: The ReplicaSet failed to scale due to insufficient resources on the nodes.
What Happened: When attempting to scale a ReplicaSet, new pods failed to schedule due to a lack of available resources on the nodes.
Diagnosis Steps:
	â€¢ Checked the resource usage on the nodes and found they were running at full capacity.
	â€¢ Analyzed ReplicaSet scaling events and observed failures to schedule new pods.
Root Cause: Insufficient node resources to accommodate new pods due to high resource consumption by existing workloads.
Fix/Workaround:
	â€¢ Added more nodes to the cluster to handle the increased workload.
	â€¢ Adjusted resource requests and limits to ensure efficient resource allocation.
Lessons Learned: Regularly monitor cluster resource usage and scale proactively based on demand.
How to Avoid:
	â€¢ Enable cluster autoscaling to handle scaling issues automatically.
	â€¢ Set proper resource requests and limits for pods to avoid resource exhaustion.

ðŸ“˜ Scenario #79: Missing Namespace After Cluster Upgrade
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: A namespace was missing after performing a cluster upgrade.
What Happened: After upgrading the cluster, a namespace that was present before the upgrade was no longer available.
Diagnosis Steps:
	â€¢ Checked the cluster upgrade logs and identified that a namespace deletion had occurred during the upgrade process.
	â€¢ Verified with backup and confirmed the namespace was inadvertently deleted during the upgrade.
Root Cause: An issue during the cluster upgrade process led to the unintentional deletion of a namespace.
Fix/Workaround:
	â€¢ Restored the missing namespace from backups.
	â€¢ Investigated and fixed the upgrade process to prevent future occurrences.
Lessons Learned: Always backup critical resources before performing upgrades and test the upgrade process thoroughly.
How to Avoid:
	â€¢ Backup namespaces and other critical resources before upgrading.
	â€¢ Review upgrade logs carefully to identify any unexpected deletions or changes.

ðŸ“˜ Scenario #80: Inefficient Resource Usage Due to Misconfigured Horizontal Pod Autoscaler
Category: Cluster Management
Environment: K8s v1.23, Azure AKS
Scenario Summary: The Horizontal Pod Autoscaler (HPA) was inefficiently scaling due to misconfigured metrics.
What Happened: HPA did not scale pods appropriately, either under-scaling or over-scaling, due to incorrect metric definitions.
Diagnosis Steps:
	â€¢ Checked HPA configuration and identified incorrect CPU utilization metrics.
	â€¢ Monitored metrics-server logs and found that the metrics were inconsistent.
Root Cause: HPA was configured to scale based on inaccurate or inappropriate metrics, leading to inefficient scaling behavior.
Fix/Workaround:
	â€¢ Reconfigured the HPA to scale based on correct metrics (e.g., memory, custom metrics).
	â€¢ Verified that the metrics-server was reporting accurate data.
Lessons Learned: Always ensure that the right metrics are used for scaling to avoid inefficient scaling behavior.
How to Avoid:
	â€¢ Regularly review HPA configurations and metrics definitions.
	â€¢ Test scaling behavior under different load conditions.

ðŸ“˜ Scenario #81: Pod Disruption Due to Unavailable Image Registry
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: Pods could not start because the image registry was temporarily unavailable, causing image pull failures.
What Happened: Pods failed to pull images because the registry was down for maintenance, leading to deployment failures.
Diagnosis Steps:
	â€¢ Checked the pod status using kubectl describe pod and identified image pull errors.
	â€¢ Investigated the registry status and found scheduled downtime for maintenance.
Root Cause: The container registry was temporarily unavailable due to maintenance, and the pods could not access the required images.
Fix/Workaround:
	â€¢ Manually downloaded the images from a secondary registry.
	â€¢ Temporarily used a local image registry until the primary registry was back online.
Lessons Learned: Ensure that alternate image registries are available in case of downtime.
How to Avoid:
	â€¢ Implement multiple image registries for high availability.
	â€¢ Use image pull policies that allow fallback to local caches.

ðŸ“˜ Scenario #82: Pod Fails to Start Due to Insufficient Resource Requests
Category: Cluster Management
Environment: K8s v1.20, AWS EKS
Scenario Summary: Pods failed to start because their resource requests were too low, preventing the scheduler from assigning them to nodes.
What Happened: The pods had very low resource requests, causing the scheduler to fail to assign them to available nodes.
Diagnosis Steps:
	â€¢ Checked pod status and found them stuck in Pending.
	â€¢ Analyzed the resource requests and found that they were too low to meet the node's capacity requirements.
Root Cause: The resource requests were set too low, preventing proper pod scheduling.
Fix/Workaround:
	â€¢ Increased the resource requests in the pod spec.
	â€¢ Reapplied the configuration, and the pods were scheduled successfully.
Lessons Learned: Always ensure that resource requests are appropriately set for your workloads.
How to Avoid:
	â€¢ Use resource limits and requests based on accurate usage data from monitoring tools.
	â€¢ Set resource requests in line with expected workload sizes.

ðŸ“˜ Scenario #83: Horizontal Pod Autoscaler Under-Scaling During Peak Load
Category: Cluster Management
Environment: K8s v1.22, GKE
Scenario Summary: HPA failed to scale the pods appropriately during a sudden spike in load.
What Happened: The HPA did not scale the pods properly during a traffic surge due to incorrect metric thresholds.
Diagnosis Steps:
	â€¢ Checked HPA settings and identified that the CPU utilization threshold was too high.
	â€¢ Verified the metric server was reporting correct metrics.
Root Cause: Incorrect scaling thresholds set in the HPA configuration.
Fix/Workaround:
	â€¢ Adjusted HPA thresholds to scale more aggressively under higher loads.
	â€¢ Increased the replica count to handle the peak load.
Lessons Learned: HPA thresholds should be fine-tuned based on expected load patterns.
How to Avoid:
	â€¢ Regularly review and adjust HPA configurations to reflect actual workload behavior.
	â€¢ Use custom metrics for better scaling control.

ðŸ“˜ Scenario #84: Pod Eviction Due to Node Disk Pressure
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: Pods were evicted due to disk pressure on the node, causing service interruptions.
What Happened: A node ran out of disk space due to logs and other data consuming the disk, resulting in pod evictions.
Diagnosis Steps:
	â€¢ Checked node resource usage and found disk space was exhausted.
	â€¢ Reviewed pod eviction events and found they were due to disk pressure.
Root Cause: The node disk was full, causing the kubelet to evict pods to free up resources.
Fix/Workaround:
	â€¢ Increased disk capacity on the affected node.
	â€¢ Cleared unnecessary logs and old data from the disk.
Lessons Learned: Ensure adequate disk space is available, especially for logging and temporary data.
How to Avoid:
	â€¢ Monitor disk usage closely and set up alerts for disk pressure.
	â€¢ Implement log rotation and clean-up policies to avoid disk exhaustion.

ðŸ“˜ Scenario #85: Failed Node Drain Due to In-Use Pods
Category: Cluster Management
Environment: K8s v1.22, Azure AKS
Scenario Summary: A node failed to drain due to pods that were in use, preventing the drain operation from completing.
What Happened: When attempting to drain a node, the operation failed because some pods were still in use or had pending termination grace periods.
Diagnosis Steps:
	â€¢ Ran kubectl describe node and checked pod evictions.
	â€¢ Identified pods that were in the middle of long-running processes or had insufficient termination grace periods.
Root Cause: Pods with long-running tasks or improper termination grace periods caused the drain to hang.
Fix/Workaround:
	â€¢ Increased termination grace periods for the affected pods.
	â€¢ Forced the node drain operation after ensuring that the pods could safely terminate.
Lessons Learned: Ensure that pods with long-running tasks have adequate termination grace periods.
How to Avoid:
	â€¢ Configure appropriate termination grace periods for all pods.
	â€¢ Monitor node draining and ensure pods can gracefully shut down.

ðŸ“˜ Scenario #86: Cluster Autoscaler Not Scaling Up
Category: Cluster Management
Environment: K8s v1.20, GKE
Scenario Summary: The cluster autoscaler failed to scale up the node pool despite high resource demand.
What Happened: The cluster autoscaler did not add nodes when resource utilization reached critical levels.
Diagnosis Steps:
	â€¢ Checked the autoscaler logs and found that scaling events were not triggered.
	â€¢ Reviewed the node pool configuration and autoscaler settings.
Root Cause: The autoscaler was not configured with sufficient thresholds or permissions to scale up the node pool.
Fix/Workaround:
	â€¢ Adjusted the scaling thresholds in the autoscaler configuration.
	â€¢ Verified the correct IAM permissions for the autoscaler to scale the node pool.
Lessons Learned: Ensure the cluster autoscaler is correctly configured and has the right permissions.
How to Avoid:
	â€¢ Regularly review cluster autoscaler configuration and permissions.
	â€¢ Monitor scaling behavior to ensure it functions as expected during high load.

ðŸ“˜ Scenario #87: Pod Network Connectivity Issues After Node Reboot
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: Pods lost network connectivity after a node reboot, causing communication failures between services.
What Happened: After a node was rebooted, the networking components failed to re-establish proper connectivity for the pods.
Diagnosis Steps:
	â€¢ Checked pod logs and found connection timeouts between services.
	â€¢ Investigated the node and found networking components (e.g., CNI plugin) were not properly re-initialized after the reboot.
Root Cause: The CNI plugin did not properly re-initialize after the node reboot, causing networking failures.
Fix/Workaround:
	â€¢ Manually restarted the CNI plugin on the affected node.
	â€¢ Ensured that the CNI plugin was configured to restart properly after a node reboot.
Lessons Learned: Ensure that critical components like CNI plugins are resilient to node reboots.
How to Avoid:
	â€¢ Configure the CNI plugin to restart automatically after node reboots.
	â€¢ Monitor networking components to ensure they are healthy after reboots.

ðŸ“˜ Scenario #88: Insufficient Permissions Leading to Unauthorized Access Errors
Category: Cluster Management
Environment: K8s v1.22, GKE
Scenario Summary: Unauthorized access errors occurred due to missing permissions in RBAC configurations.
What Happened: Pods failed to access necessary resources due to misconfigured RBAC policies, resulting in permission-denied errors.
Diagnosis Steps:
	â€¢ Reviewed the RBAC policy logs and identified missing permissions for service accounts.
	â€¢ Checked the roles and role bindings associated with the pods.
Root Cause: RBAC policies did not grant the required permissions to the service accounts.
Fix/Workaround:
	â€¢ Updated the RBAC roles and bindings to include the necessary permissions for the pods.
	â€¢ Applied the updated RBAC configurations and confirmed access.
Lessons Learned: RBAC configurations should be thoroughly tested to ensure correct permissions.
How to Avoid:
	â€¢ Implement a least-privilege access model and audit RBAC policies regularly.
	â€¢ Use automated tools to test and verify RBAC configurations.

ðŸ“˜ Scenario #89: Failed Pod Upgrade Due to Incompatible API Versions
Category: Cluster Management
Environment: K8s v1.19, AWS EKS
Scenario Summary: A pod upgrade failed because it was using deprecated APIs not supported in the new version.
What Happened: When upgrading to a new Kubernetes version, a pod upgrade failed due to deprecated APIs in use.
Diagnosis Steps:
	â€¢ Checked the pod spec and identified deprecated API versions in use.
	â€¢ Verified the Kubernetes changelog for API deprecations in the new version.
Root Cause: The pod was using APIs that were deprecated in the new Kubernetes version, causing the upgrade to fail.
Fix/Workaround:
	â€¢ Updated the pod spec to use supported API versions.
	â€¢ Reapplied the deployment with the updated APIs.
Lessons Learned: Regularly review Kubernetes changelogs for deprecated API versions.
How to Avoid:
	â€¢ Implement a process to upgrade and test all components for compatibility before applying changes.
	â€¢ Use tools like kubectl deprecations to identify deprecated APIs.

ðŸ“˜ Scenario #90: High CPU Utilization Due to Inefficient Application Code
Category: Cluster Management
Environment: K8s v1.21, Azure AKS
Scenario Summary: A container's high CPU usage was caused by inefficient application code, leading to resource exhaustion.
What Happened: An application was running inefficient code that caused excessive CPU consumption, impacting the entire node's performance.
Diagnosis Steps:
	â€¢ Monitored the pod's resource usage and found high CPU utilization.
	â€¢ Analyzed application logs and identified inefficient loops in the code.
Root Cause: The application code had an inefficient algorithm that led to high CPU consumption.
Fix/Workaround:
	â€¢ Optimized the application code to reduce CPU consumption.
	â€¢ Redeployed the application with the optimized code.
Lessons Learned: Application code optimization is essential for ensuring efficient resource usage.
How to Avoid:
	â€¢ Regularly profile application code for performance bottlenecks.
	â€¢ Set CPU limits and requests to prevent resource exhaustion.

ðŸ“˜ Scenario #91: Resource Starvation Due to Over-provisioned Pods
Category: Cluster Management
Environment: K8s v1.20, AWS EKS
Scenario Summary: Resource starvation occurred on nodes because pods were over-provisioned, consuming more resources than expected.
What Happened: Pods were allocated more resources than necessary, causing resource contention on the nodes.
Diagnosis Steps:
	â€¢ Analyzed node and pod resource utilization.
	â€¢ Found that the CPU and memory resources for several pods were unnecessarily high, leading to resource starvation for other pods.
Root Cause: Incorrect resource requests and limits set for the pods, causing resource over-allocation.
Fix/Workaround:
	â€¢ Reduced resource requests and limits based on actual usage metrics.
	â€¢ Re-deployed the pods with optimized resource configurations.
Lessons Learned: Accurate resource requests and limits should be based on actual usage data.
How to Avoid:
	â€¢ Regularly monitor resource utilization and adjust requests/limits accordingly.
	â€¢ Use vertical pod autoscalers for better resource distribution.

ðŸ“˜ Scenario #92: Unscheduled Pods Due to Insufficient Affinity Constraints
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: Pods were not scheduled due to overly strict affinity rules that limited the nodes available for deployment.
What Happened: The affinity rules were too restrictive, preventing pods from being scheduled on available nodes.
Diagnosis Steps:
	â€¢ Reviewed pod deployment spec and found strict affinity constraints.
	â€¢ Verified available nodes and found that no nodes met the pod's affinity requirements.
Root Cause: Overly restrictive affinity settings that limited pod scheduling.
Fix/Workaround:
	â€¢ Adjusted the affinity rules to be less restrictive.
	â€¢ Applied changes and verified the pods were scheduled correctly.
Lessons Learned: Affinity constraints should balance optimal placement with available resources.
How to Avoid:
	â€¢ Regularly review and adjust affinity/anti-affinity rules based on cluster capacity.
	â€¢ Test deployment configurations in staging before applying to production.

ðŸ“˜ Scenario #93: Pod Readiness Probe Failure Due to Slow Initialization
Category: Cluster Management
Environment: K8s v1.22, Azure AKS
Scenario Summary: Pods failed their readiness probes during initialization, causing traffic to be routed to unhealthy instances.
What Happened: The pods had a slow initialization time, but the readiness probe timeout was set too low, causing premature failure.
Diagnosis Steps:
	â€¢ Checked pod events and logs, discovering that readiness probes were failing due to long startup times.
	â€¢ Increased the timeout period for the readiness probe and observed that the pods began to pass the probe after startup.
Root Cause: Readiness probe timeout was set too low for the pod's initialization process.
Fix/Workaround:
	â€¢ Increased the readiness probe timeout and delay parameters.
	â€¢ Re-applied the deployment, and the pods started passing readiness checks.
Lessons Learned: The readiness probe timeout should be configured according to the actual initialization time of the pod.
How to Avoid:
	â€¢ Monitor pod initialization times and adjust readiness probe configurations accordingly.
	â€¢ Use a gradual rollout for new deployments to avoid sudden failures.

ðŸ“˜ Scenario #94: Incorrect Ingress Path Handling Leading to 404 Errors
Category: Cluster Management
Environment: K8s v1.19, GKE
Scenario Summary: Incorrect path configuration in the ingress resource resulted in 404 errors for certain API routes.
What Happened: Ingress was misconfigured with incorrect path mappings, causing requests to certain API routes to return 404 errors.
Diagnosis Steps:
	â€¢ Checked ingress configuration using kubectl describe ingress and found mismatched path rules.
	â€¢ Verified the service endpoints and found that the routes were not properly configured in the ingress spec.
Root Cause: Incorrect path definitions in the ingress resource, causing requests to be routed incorrectly.
Fix/Workaround:
	â€¢ Fixed the path configuration in the ingress resource.
	â€¢ Re-applied the ingress configuration, and traffic was correctly routed.
Lessons Learned: Verify that ingress path definitions match the application routing.
How to Avoid:
	â€¢ Test ingress paths thoroughly before applying to production environments.
	â€¢ Use versioned APIs to ensure backward compatibility for routing paths.

ðŸ“˜ Scenario #95: Node Pool Scaling Failure Due to Insufficient Quotas
Category: Cluster Management
Environment: K8s v1.20, AWS EKS
Scenario Summary: Node pool scaling failed because the account exceeded resource quotas in AWS.
What Happened: When attempting to scale up a node pool, the scaling operation failed due to hitting AWS resource quotas.
Diagnosis Steps:
	â€¢ Reviewed the EKS and AWS console to identify quota limits.
	â€¢ Found that the account had exceeded the EC2 instance limit for the region.
Root Cause: Insufficient resource quotas in the AWS account.
Fix/Workaround:
	â€¢ Requested a quota increase from AWS support.
	â€¢ Once the request was approved, scaled the node pool successfully.
Lessons Learned: Monitor cloud resource quotas to ensure scaling operations are not blocked.
How to Avoid:
	â€¢ Keep track of resource quotas and request increases in advance.
	â€¢ Automate quota monitoring and alerting to avoid surprises during scaling.

ðŸ“˜ Scenario #96: Pod Crash Loop Due to Missing ConfigMap
Category: Cluster Management
Environment: K8s v1.21, Azure AKS
Scenario Summary: Pods entered a crash loop because a required ConfigMap was not present in the namespace.
What Happened: A pod configuration required a ConfigMap that was deleted by accident, causing the pod to crash due to missing configuration data.
Diagnosis Steps:
	â€¢ Checked pod logs and found errors indicating missing environment variables or configuration files.
	â€¢ Investigated the ConfigMap and found it had been accidentally deleted.
Root Cause: Missing ConfigMap due to accidental deletion.
Fix/Workaround:
	â€¢ Recreated the ConfigMap in the namespace.
	â€¢ Re-deployed the pods, and they started successfully.
Lessons Learned: Protect critical resources like ConfigMaps to prevent accidental deletion.
How to Avoid:
	â€¢ Use namespaces and resource quotas to limit accidental deletion of shared resources.
	â€¢ Implement stricter RBAC policies for sensitive resources.

ðŸ“˜ Scenario #97: Kubernetes API Server Slowness Due to Excessive Logging
Category: Cluster Management
Environment: K8s v1.22, GKE
Scenario Summary: The Kubernetes API server became slow due to excessive log generation from the kubelet and other components.
What Happened: Excessive logging from the kubelet and other components overwhelmed the API server, causing it to become slow and unresponsive.
Diagnosis Steps:
	â€¢ Monitored API server performance using kubectl top pod and noticed resource spikes.
	â€¢ Analyzed log files and found an unusually high number of log entries from the kubelet.
Root Cause: Excessive logging was causing resource exhaustion on the API server.
Fix/Workaround:
	â€¢ Reduced the verbosity of logs from the kubelet and other components.
	â€¢ Configured log rotation to prevent logs from consuming too much disk space.
Lessons Learned: Excessive logging can cause performance degradation if not properly managed.
How to Avoid:
	â€¢ Set appropriate logging levels for components based on usage.
	â€¢ Implement log rotation and retention policies to avoid overwhelming storage.

ðŸ“˜ Scenario #98: Pod Scheduling Failure Due to Taints and Tolerations Misconfiguration
Category: Cluster Management
Environment: K8s v1.19, AWS EKS
Scenario Summary: Pods failed to schedule because the taints and tolerations were misconfigured, preventing the scheduler from placing them on nodes.
What Happened: The nodes had taints that were not matched by the pod's tolerations, causing the pods to remain unscheduled.
Diagnosis Steps:
	â€¢ Used kubectl describe pod to investigate scheduling issues.
	â€¢ Found that the taints on the nodes did not match the tolerations set on the pods.
Root Cause: Misconfiguration of taints and tolerations in the node and pod specs.
Fix/Workaround:
	â€¢ Corrected the tolerations in the pod specs to match the taints on the nodes.
	â€¢ Re-applied the pods and verified that they were scheduled correctly.
Lessons Learned: Always ensure taints and tolerations are correctly configured in a multi-tenant environment.
How to Avoid:
	â€¢ Test taints and tolerations in a non-production environment.
	â€¢ Regularly audit and verify toleration settings to ensure proper pod placement.

ðŸ“˜ Scenario #99: Unresponsive Dashboard Due to High Resource Usage
Category: Cluster Management
Environment: K8s v1.20, Azure AKS
Scenario Summary: The Kubernetes dashboard became unresponsive due to high resource usage caused by a large number of requests.
What Happened: The Kubernetes dashboard was overwhelmed by too many requests, consuming excessive CPU and memory resources.
Diagnosis Steps:
	â€¢ Checked resource usage of the dashboard pod using kubectl top pod.
	â€¢ Found that the pod was using more resources than expected due to a large number of incoming requests.
Root Cause: The dashboard was not scaled to handle the volume of requests.
Fix/Workaround:
	â€¢ Scaled the dashboard deployment to multiple replicas to handle the load.
	â€¢ Adjusted resource requests and limits for the dashboard pod.
Lessons Learned: Ensure that the Kubernetes dashboard is properly scaled to handle expected traffic.
How to Avoid:
	â€¢ Implement horizontal scaling for the dashboard and other critical services.
	â€¢ Monitor the usage of the Kubernetes dashboard and scale as needed.

ðŸ“˜ Scenario #100: Resource Limits Causing Container Crashes
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: Containers kept crashing due to hitting resource limits set in their configurations.
What Happened: Containers were being killed because they exceeded their resource limits for memory and CPU.
Diagnosis Steps:
	â€¢ Used kubectl describe pod to find the resource limits and found that the limits were too low for the workload.
	â€¢ Analyzed container logs and found frequent OOMKilled events.
Root Cause: The resource limits set for the container were too low, causing the container to be terminated when it exceeded the limit.
Fix/Workaround:
	â€¢ Increased the resource limits for the affected containers.
	â€¢ Re-applied the pod configurations and monitored for stability.
Lessons Learned: Resource limits should be set based on actual workload requirements.
How to Avoid:
	â€¢ Use monitoring tools to track resource usage and adjust limits as needed.
	â€¢ Set up alerts for resource threshold breaches to avoid crashes.
```
