```
üìò Scenario #1: Zombie Pods Causing NodeDrain to Hang
Category: Cluster Management
Environment: K8s v1.23, On-prem bare metal, Systemd cgroups
Scenario Summary: Node drain stuck indefinitely due to unresponsive terminating pod.
What Happened: A pod with a custom finalizer never completed termination, blocking kubectl drain. Even after the pod was marked for deletion, the API server kept waiting because the finalizer wasn‚Äôt removed.
Diagnosis Steps:
	‚Ä¢ Checked kubectl get pods --all-namespaces -o wide to find lingering pods.
	‚Ä¢ Found pod stuck in Terminating state for over 20 minutes.
	‚Ä¢ Used kubectl describe pod <pod> to identify the presence of a custom finalizer.
	‚Ä¢ Investigated controller logs managing the finalizer ‚Äì the controller had crashed.
Root Cause: Finalizer logic was never executed because its controller was down, leaving the pod undeletable.
Fix/Workaround:
kubectl patch pod <pod-name> -p '{"metadata":{"finalizers":[]}}' --type=merge
Lessons Learned: Finalizers should have timeout or fail-safe logic.
How to Avoid:
	‚Ä¢ Avoid finalizers unless absolutely necessary.
	‚Ä¢ Add monitoring for stuck Terminating pods.
	‚Ä¢ Implement retry/timeout logic in finalizer controllers.

üìò Scenario #2: API Server Crash Due to Excessive CRD Writes
Category: Cluster Management
Environment: K8s v1.24, GKE, heavy use of custom controllers
Scenario Summary: API server crashed due to flooding by a malfunctioning controller creating too many custom resources.
What Happened: A bug in a controller created thousands of Custom Resources (CRs) in a tight reconciliation loop. Etcd was flooded, leading to slow writes, and the API server eventually became non-responsive.
Diagnosis Steps:
	‚Ä¢ API latency increased, leading to 504 Gateway Timeout errors in kubectl.
	‚Ä¢ Used kubectl get crds | wc -l to list all CRs.
	‚Ä¢ Analyzed controller logs ‚Äì found infinite reconcile on a specific CR type.
	‚Ä¢ etcd disk I/O was maxed.
Root Cause: Bad logic in reconcile loop: create was always called regardless of the state, creating resource floods.
Fix/Workaround:
	‚Ä¢ Scaled the controller to 0 replicas.
	‚Ä¢ Manually deleted thousands of stale CRs using batch deletion.
Lessons Learned: Always test reconcile logic in a sandboxed cluster.
How to Avoid:
	‚Ä¢ Implement create/update guards in reconciliation.
	‚Ä¢ Add Prometheus alert for high CR count.

üìò Scenario #3: Node Not Rejoining After Reboot
Category: Cluster Management
Environment: K8s v1.21, Self-managed cluster, Static nodes
Scenario Summary: A rebooted node failed to rejoin the cluster due to kubelet identity mismatch.
What Happened: After a kernel upgrade and reboot, a node didn‚Äôt appear in kubectl get nodes. The kubelet logs showed registration issues.
Diagnosis Steps:
	‚Ä¢ Checked system logs and kubelet logs.
	‚Ä¢ Noticed --hostname-override didn't match the node name registered earlier.
	‚Ä¢ kubectl get nodes -o wide showed old hostname; new one mismatched due to DHCP/hostname change.
Root Cause: Kubelet registered with a hostname that no longer matched its node identity in the cluster.
Fix/Workaround:
	‚Ä¢ Re-joined the node using correct --hostname-override.
	‚Ä¢ Cleaned up stale node entry from the cluster.
Lessons Learned: Node identity must remain consistent across reboots.
How to Avoid:
	‚Ä¢ Set static hostnames and IPs.
	‚Ä¢ Use consistent cloud-init or kubeadm configuration.

üìò Scenario #4: Etcd Disk Full Causing API Server Timeout
Category: Cluster Management
Environment: K8s v1.25, Bare-metal cluster
Scenario Summary: etcd ran out of disk space, making API server unresponsive.
What Happened: The cluster started failing API requests. Etcd logs showed disk space errors, and API server logs showed failed storage operations.
Diagnosis Steps:
	‚Ä¢ Used df -h on etcd nodes ‚Äî confirmed disk full.
	‚Ä¢ Reviewed /var/lib/etcd ‚Äì excessive WAL and snapshot files.
	‚Ä¢ Used etcdctl to assess DB size.
Root Cause: Lack of compaction and snapshotting caused disk to fill up with historical revisions and WALs.
Fix/Workaround:

bash
CopyEdit
etcdctl compact <rev>
etcdctl defrag
	‚Ä¢ Cleaned logs, snapshots, and increased disk space temporarily.
Lessons Learned: etcd requires periodic maintenance.
How to Avoid:
	‚Ä¢ Enable automatic compaction.
	‚Ä¢ Monitor disk space usage of etcd volumes.

üìò Scenario #5: Misconfigured Taints Blocking Pod Scheduling
Category: Cluster Management
Environment: K8s v1.26, Multi-tenant cluster
Scenario Summary: Critical workloads weren‚Äôt getting scheduled due to incorrect node taints.
What Happened: A user added taints (NoSchedule) to all nodes to isolate their app, but forgot to include tolerations in workloads. Other apps stopped working.
Diagnosis Steps:
	‚Ä¢ Pods stuck in Pending state.
	‚Ä¢ Used kubectl describe pod <pod> ‚Äì reason: no nodes match tolerations.
	‚Ä¢ Inspected node taints via kubectl describe node.
Root Cause: Lack of required tolerations on most workloads.
Fix/Workaround:
	‚Ä¢ Removed the inappropriate taints.
	‚Ä¢ Re-scheduled workloads.
Lessons Learned: Node taints must be reviewed cluster-wide.
How to Avoid:
	‚Ä¢ Educate teams on node taints and tolerations.
	‚Ä¢ Restrict RBAC for node mutation.

üìò Scenario #6: Kubelet DiskPressure Loop on Large Image Pulls
Category: Cluster Management
Environment: K8s v1.22, EKS
Scenario Summary: Continuous pod evictions caused by DiskPressure due to image bloating.
What Happened: A new container image with many layers was deployed. Node‚Äôs disk filled up, triggering kubelet‚Äôs DiskPressure condition. Evicted pods created a loop.
Diagnosis Steps:
	‚Ä¢ Checked node conditions: kubectl describe node showed DiskPressure: True.
	‚Ä¢ Monitored image cache with crictl images.
	‚Ä¢ Node /var/lib/containerd usage exceeded threshold.
Root Cause: Excessive layering in container image and high pull churn caused disk exhaustion.
Fix/Workaround:
	‚Ä¢ Rebuilt image using multistage builds and removed unused layers.
	‚Ä¢ Increased ephemeral disk space temporarily.
Lessons Learned: Container image size directly affects node stability.
How to Avoid:
	‚Ä¢ Set resource requests/limits appropriately.
	‚Ä¢ Use image scanning to reject bloated images.

üìò Scenario #7: Node Goes NotReady Due to Clock Skew
Category: Cluster Management
Environment: K8s v1.20, On-prem
Scenario Summary: One node dropped from the cluster due to TLS errors from time skew.
What Happened: TLS handshakes between the API server and a node started failing. Node became NotReady. Investigation showed NTP daemon was down.
Diagnosis Steps:
	‚Ä¢ Checked logs for TLS errors: ‚Äúcertificate expired or not yet valid‚Äù.
	‚Ä¢ Used timedatectl to check drift ‚Äì node was 45s behind.
	‚Ä¢ NTP service was inactive.
Root Cause: Large clock skew between node and control plane led to invalid TLS sessions.
Fix/Workaround:
	‚Ä¢ Restarted NTP sync.
	‚Ä¢ Restarted kubelet after sync.
Lessons Learned: Clock sync is critical in TLS-based distributed systems.
How to Avoid:
	‚Ä¢ Use chronyd or systemd-timesyncd.
	‚Ä¢ Monitor clock skew across nodes.

üìò Scenario #8: API Server High Latency Due to Event Flooding
Category: Cluster Management
Environment: K8s v1.23, Azure AKS
Scenario Summary: An app spamming Kubernetes events slowed down the entire API server.
What Happened: A custom controller logged frequent events (~50/second), causing the etcd event store to choke.
Diagnosis Steps:
	‚Ä¢ Prometheus showed spike in event count.
	‚Ä¢ kubectl get events --sort-by=.metadata.creationTimestamp showed massive spam.
	‚Ä¢ Found misbehaving controller repeating failure events.
Root Cause: No rate limiting on event creation in controller logic.
Fix/Workaround:
	‚Ä¢ Patched controller to rate-limit record.Eventf.
	‚Ä¢ Cleaned old events.
Lessons Learned: Events are not free ‚Äì they impact etcd/API server.
How to Avoid:
	‚Ä¢ Use deduplicated or summarized event logic.
	‚Ä¢ Set API server --event-ttl=1h and --eventRateLimit.

üìò Scenario #9: CoreDNS CrashLoop on Startup
Category: Cluster Management
Environment: K8s v1.24, DigitalOcean
Scenario Summary: CoreDNS pods kept crashing due to a misconfigured Corefile.
What Happened: A team added a custom rewrite rule in the Corefile which had invalid syntax. CoreDNS failed to start.
Diagnosis Steps:
	‚Ä¢ Checked logs: syntax error on startup.
	‚Ä¢ Used kubectl describe configmap coredns -n kube-system to inspect.
	‚Ä¢ Reproduced issue in test cluster.
Root Cause: Corefile misconfigured ‚Äì incorrect directive placement.
Fix/Workaround:
	‚Ä¢ Reverted to backup configmap.
	‚Ä¢ Restarted CoreDNS.
Lessons Learned: DNS misconfigurations can cascade quickly.
How to Avoid:
	‚Ä¢ Use a CoreDNS validator before applying config.
	‚Ä¢ Maintain versioned backups of Corefile.

üìò Scenario #10: Control Plane Unavailable After Flannel Misconfiguration
Category: Cluster Management
Environment: K8s v1.18, On-prem, Flannel CNI
Scenario Summary: Misaligned pod CIDRs caused overlay misrouting and API server failure.
What Happened: A new node was added with a different pod CIDR than what Flannel expected. This broke pod-to-pod and node-to-control-plane communication.
Diagnosis Steps:
	‚Ä¢ kubectl timed out from nodes.
	‚Ä¢ Logs showed dropped traffic in iptables.
	‚Ä¢ Compared --pod-cidr in kubelet and Flannel config.
Root Cause: Pod CIDRs weren‚Äôt consistent across node and Flannel.
Fix/Workaround:
	‚Ä¢ Reconfigured node with proper CIDR range.
	‚Ä¢ Flushed iptables and restarted Flannel.
Lessons Learned: CNI requires strict configuration consistency.
How to Avoid:
	‚Ä¢ Enforce CIDR policy via admission control.
	‚Ä¢ Validate podCIDR ranges before adding new nodes.

üìò Scenario #11: kube-proxy IPTables Rules Overlap Breaking Networking
Category: Cluster Management
Environment: K8s v1.22, On-prem with kube-proxy in IPTables mode
Scenario Summary: Services became unreachable due to overlapping custom IPTables rules with kube-proxy rules.
What Happened: A system admin added custom IPTables NAT rules for external routing, which inadvertently modified the same chains managed by kube-proxy.
Diagnosis Steps:
	‚Ä¢ DNS and service access failing intermittently.
	‚Ä¢ Ran iptables-save | grep KUBE- ‚Äì found modified chains.
	‚Ä¢ Checked kube-proxy logs: warnings about rule insert failures.
Root Cause: Manual IPTables rules conflicted with KUBE-SERVICES chains, causing rule precedence issues.
Fix/Workaround:
	‚Ä¢ Flushed custom rules and reloaded kube-proxy.

bash
CopyEdit
iptables -F; systemctl restart kube-proxy
Lessons Learned: Never mix manual IPTables rules with kube-proxy-managed chains.
How to Avoid:
	‚Ä¢ Use separate IPTables chains or policy routing.
	‚Ä¢ Document any node-level firewall rules clearly.

üìò Scenario #12: Stuck CSR Requests Blocking New Node Joins
Category: Cluster Management
Environment: K8s v1.20, kubeadm cluster
Scenario Summary: New nodes couldn‚Äôt join due to a backlog of unapproved CSRs.
What Happened: A spike in expired certificate renewals caused hundreds of CSRs to queue, none of which were being auto-approved. New nodes waited indefinitely.
Diagnosis Steps:
	‚Ä¢ Ran kubectl get csr ‚Äì saw >500 pending requests.
	‚Ä¢ New nodes stuck at kubelet: ‚Äúwaiting for server signing‚Äù.
	‚Ä¢ Approval controller was disabled due to misconfiguration.
Root Cause: Auto-approval for CSRs was turned off during a security patch, but not re-enabled.
Fix/Workaround:

bash
CopyEdit
kubectl certificate approve <csr-name>
	‚Ä¢ Re-enabled the CSR approver controller.
Lessons Learned: CSR management is critical for kubelet-node communication.
How to Avoid:
	‚Ä¢ Monitor pending CSRs.
	‚Ä¢ Don‚Äôt disable kube-controller-manager flags like --cluster-signing-cert-file.

üìò Scenario #13: Failed Cluster Upgrade Due to Unready Static Pods
Category: Cluster Management
Environment: K8s v1.21 ‚Üí v1.23 upgrade, kubeadm
Scenario Summary: Upgrade failed when static control plane pods weren‚Äôt ready due to invalid manifests.
What Happened: During upgrade, etcd didn‚Äôt come up because its pod manifest had a typo. Kubelet never started etcd, causing control plane install to hang.
Diagnosis Steps:
	‚Ä¢ Checked /etc/kubernetes/manifests/etcd.yaml for errors.
	‚Ä¢ Used journalctl -u kubelet to see static pod startup errors.
	‚Ä¢ Verified pod not running via crictl ps.
Root Cause: Human error editing the static pod manifest ‚Äì invalid volumeMount path.
Fix/Workaround:
	‚Ä¢ Fixed manifest.
	‚Ä¢ Restarted kubelet to load corrected pod.
Lessons Learned: Static pods need strict validation.
How to Avoid:
	‚Ä¢ Use YAML linter on static manifests.
	‚Ä¢ Backup manifests before upgrade.

üìò Scenario #14: Uncontrolled Logs Filled Disk on All Nodes
Category: Cluster Management
Environment: K8s v1.24, AWS EKS, containerd
Scenario Summary: Application pods generated excessive logs, filling up node /var/log.
What Happened: A debug flag was accidentally enabled in a backend pod, logging hundreds of lines/sec. The journald and container logs filled up all disk space.
Diagnosis Steps:
	‚Ä¢ df -h showed /var/log full.
	‚Ä¢ Checked /var/log/containers/ ‚Äì massive logs for one pod.
	‚Ä¢ Used kubectl logs to confirm excessive output.
Root Cause: A log level misconfiguration caused explosive growth in logs.
Fix/Workaround:
	‚Ä¢ Rotated and truncated logs.
	‚Ä¢ Restarted container runtime after cleanup.
	‚Ä¢ Disabled debug logging.
Lessons Learned: Logging should be controlled and bounded.
How to Avoid:
	‚Ä¢ Set log rotation policies for container runtimes.
	‚Ä¢ Enforce sane log levels via CI/CD validation.

üìò Scenario #15: Node Drain Fails Due to PodDisruptionBudget Deadlock
Category: Cluster Management
Environment: K8s v1.21, production cluster with HPA and PDB
Scenario Summary: kubectl drain never completed because PDBs blocked eviction.
What Happened: A deployment had minAvailable: 2 in PDB, but only 2 pods were running. Node drain couldn‚Äôt evict either pod without violating PDB.
Diagnosis Steps:
	‚Ä¢ Ran kubectl describe pdb <name> ‚Äì saw AllowedDisruptions: 0.
	‚Ä¢ Checked deployment and replica count.
	‚Ä¢ Tried drain ‚Äì stuck on pod eviction for 10+ minutes.
Root Cause: PDB guarantees clashed with under-scaled deployment.
Fix/Workaround:
	‚Ä¢ Temporarily edited PDB to reduce minAvailable.
	‚Ä¢ Scaled up replicas before drain.
Lessons Learned: PDBs require careful coordination with replica count.
How to Avoid:
	‚Ä¢ Validate PDBs during deployment scale-downs.
	‚Ä¢ Create alerts for PDB blocking evictions.

üìò Scenario #16: CrashLoop of Kube-Controller-Manager on Boot
Category: Cluster Management
Environment: K8s v1.23, self-hosted control plane
Scenario Summary: Controller-manager crashed on startup due to outdated admission controller configuration.
What Happened: After an upgrade, the --enable-admission-plugins flag included a deprecated plugin, causing crash.
Diagnosis Steps:
	‚Ä¢ Checked pod logs in /var/log/pods/.
	‚Ä¢ Saw panic error: ‚Äúunknown admission plugin‚Äù.
	‚Ä¢ Compared plugin list with K8s documentation.
Root Cause: Version mismatch between config and actual controller-manager binary.
Fix/Workaround:
	‚Ä¢ Removed the deprecated plugin from startup flags.
	‚Ä¢ Restarted pod.
Lessons Learned: Admission plugin deprecations are silent but fatal.
How to Avoid:
	‚Ä¢ Track deprecations in each Kubernetes version.
	‚Ä¢ Automate validation of startup flags.

üìò Scenario #17: Inconsistent Cluster State After Partial Backup Restore
Category: Cluster Management
Environment: K8s v1.24, Velero-based etcd backup
Scenario Summary: A partial etcd restore led to stale object references and broken dependencies.
What Happened: etcd snapshot was restored, but PVCs and secrets weren‚Äôt included. Many pods failed to mount or pull secrets.
Diagnosis Steps:
	‚Ä¢ Pods failed with ‚Äúvolume not found‚Äù and ‚Äúsecret missing‚Äù.
	‚Ä¢ kubectl get pvc --all-namespaces returned empty.
	‚Ä¢ Compared resource counts pre- and post-restore.
Root Cause: Restore did not include volume snapshots or Kubernetes secrets, leading to an incomplete object graph.
Fix/Workaround:
	‚Ä¢ Manually recreated PVCs and secrets using backups from another tool.
	‚Ä¢ Redeployed apps.
Lessons Learned: etcd backup is not enough alone.
How to Avoid:
	‚Ä¢ Use backup tools that support volume + etcd (e.g., Velero with restic).
	‚Ä¢ Periodically test full cluster restores.

üìò Scenario #18: kubelet Unable to Pull Images Due to Proxy Misconfig
Category: Cluster Management
Environment: K8s v1.25, Corporate proxy network
Scenario Summary: Nodes failed to pull images from DockerHub due to incorrect proxy environment configuration.
What Happened: New kubelet config missed NO_PROXY=10.0.0.0/8,kubernetes.default.svc, causing internal DNS failures and image pull errors.
Diagnosis Steps:
	‚Ä¢ kubectl describe pod showed ImagePullBackOff.
	‚Ä¢ Checked environment variables for kubelet via systemctl show kubelet.
	‚Ä¢ Verified lack of NO_PROXY.
Root Cause: Proxy config caused kubelet to route internal cluster DNS and registry traffic through the proxy.
Fix/Workaround:
	‚Ä¢ Updated kubelet service file to include proper NO_PROXY.
	‚Ä¢ Restarted kubelet.
Lessons Learned: Proxies in K8s require deep planning.
How to Avoid:
	‚Ä¢ Always set NO_PROXY with service CIDRs and cluster domains.
	‚Ä¢ Test image pulls with isolated nodes first.

üìò Scenario #19: Multiple Nodes Marked Unreachable Due to Flaky Network Interface
Category: Cluster Management
Environment: K8s v1.22, Bare-metal, bonded NICs
Scenario Summary: Flapping interface on switch caused nodes to be marked NotReady intermittently.
What Happened: A network switch port had flapping issues, leading to periodic loss of node heartbeats.
Diagnosis Steps:
	‚Ä¢ Node status flapped between Ready and NotReady.
	‚Ä¢ Checked NIC logs via dmesg and ethtool.
	‚Ä¢ Observed link flaps in switch logs.
Root Cause: Hardware or cable issue causing loss of connectivity.
Fix/Workaround:
	‚Ä¢ Replaced cable and switch port.
	‚Ä¢ Set up redundant bonding with failover.
Lessons Learned: Physical layer issues can appear as node flakiness.
How to Avoid:
	‚Ä¢ Monitor NIC link status and configure bonding.
	‚Ä¢ Proactively audit switch port health.

üìò Scenario #20: Node Labels Accidentally Overwritten by DaemonSet
Category: Cluster Management
Environment: K8s v1.24, DaemonSet-based node config
Scenario Summary: A DaemonSet used for node labeling overwrote existing labels used by schedulers.
What Happened: A platform team deployed a DaemonSet that set node labels like zone=us-east, but it overwrote custom labels like gpu=true.
Diagnosis Steps:
	‚Ä¢ Pods no longer scheduled to GPU nodes.
	‚Ä¢ kubectl get nodes --show-labels showed gpu label missing.
	‚Ä¢ Checked DaemonSet script ‚Äì labels were overwritten, not merged.
Root Cause: Label management script used kubectl label node <node> key=value --overwrite, removing other labels.
Fix/Workaround:
	‚Ä¢ Restored original labels from backup.
	‚Ä¢ Updated script to merge labels.
Lessons Learned: Node labels are critical for scheduling decisions.
How to Avoid:
	‚Ä¢ Use label merging logic (e.g., fetch current labels, then patch).
	‚Ä¢ Protect key node labels via admission controllers.

üìò Scenario #21: Cluster Autoscaler Continuously Spawning and Deleting Nodes
Category: Cluster Management
Environment: K8s v1.24, AWS EKS with Cluster Autoscaler
Scenario Summary: The cluster was rapidly scaling up and down, creating instability in workloads.
What Happened: A misconfigured deployment had a readiness probe that failed intermittently, making pods seem unready. Cluster Autoscaler detected these as unschedulable, triggering new node provisioning. Once the pod appeared healthy again, Autoscaler would scale down.
Diagnosis Steps:
	‚Ä¢ Monitored Cluster Autoscaler logs (kubectl -n kube-system logs -l app=cluster-autoscaler).
	‚Ä¢ Identified repeated scale-up and scale-down messages.
	‚Ä¢ Traced back to a specific deployment‚Äôs readiness probe.
Root Cause: Flaky readiness probe created false unschedulable pods.
Fix/Workaround:
	‚Ä¢ Fixed the readiness probe to accurately reflect pod health.
	‚Ä¢ Tuned scale-down-delay-after-add and scale-down-unneeded-time settings.
Lessons Learned: Readiness probes directly impact Autoscaler decisions.
How to Avoid:
	‚Ä¢ Validate all probes before production deployments.
	‚Ä¢ Use Autoscaler logging to audit scaling activity.

üìò Scenario #22: Stale Finalizers Preventing Namespace Deletion
Category: Cluster Management
Environment: K8s v1.21, self-managed
Scenario Summary: A namespace remained in ‚ÄúTerminating‚Äù state indefinitely.
What Happened: The namespace contained resources with finalizers pointing to a deleted controller. Kubernetes waited forever for the finalizer to complete cleanup.
Diagnosis Steps:
	‚Ä¢ Ran kubectl get ns <name> -o json ‚Äì saw dangling finalizers.
	‚Ä¢ Checked for the corresponding CRD/controller ‚Äì it was uninstalled.
Root Cause: Finalizers without owning controller cause resource lifecycle deadlocks.
Fix/Workaround:
	‚Ä¢ Manually removed finalizers using a patched JSON:

bash
CopyEdit
kubectl patch ns <name> -p '{"spec":{"finalizers":[]}}' --type=merge
Lessons Learned: Always delete CRs before removing the CRD or controller.
How to Avoid:
	‚Ä¢ Implement controller cleanup logic.
	‚Ä¢ Audit finalizers periodically.

üìò Scenario #23: CoreDNS CrashLoop Due to Invalid ConfigMap Update
Category: Cluster Management
Environment: K8s v1.23, managed GKE
Scenario Summary: CoreDNS stopped resolving names cluster-wide after a config update.
What Happened: A platform engineer edited the CoreDNS ConfigMap to add a rewrite rule, but introduced a syntax error. The new pods started crashing, and DNS resolution stopped working across the cluster.
Diagnosis Steps:
	‚Ä¢ Ran kubectl logs -n kube-system -l k8s-app=kube-dns ‚Äì saw config parse errors.
	‚Ä¢ Used kubectl describe pod to confirm CrashLoopBackOff.
	‚Ä¢ Validated config against CoreDNS docs.
Root Cause: Invalid configuration line in CoreDNS ConfigMap.
Fix/Workaround:
	‚Ä¢ Rolled back to previous working ConfigMap.
	‚Ä¢ Restarted CoreDNS pods to pick up change.
Lessons Learned: ConfigMap changes can instantly affect cluster-wide services.
How to Avoid:
	‚Ä¢ Use coredns -conf <file> locally to validate changes.
	‚Ä¢ Test changes in a non-prod namespace before rollout.

üìò Scenario #24: Pod Eviction Storm Due to DiskPressure
Category: Cluster Management
Environment: K8s v1.25, self-managed, containerd
Scenario Summary: A sudden spike in image pulls caused all nodes to hit disk pressure, leading to massive pod evictions.
What Happened: A nightly batch job triggered a container image update across thousands of pods. Pulling these images used all available space in /var/lib/containerd, which led to node condition DiskPressure, forcing eviction of critical workloads.
Diagnosis Steps:
	‚Ä¢ Used kubectl describe node ‚Äì found DiskPressure=True.
	‚Ä¢ Inspected /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/.
	‚Ä¢ Checked image pull logs.
Root Cause: No image GC and too many simultaneous pulls filled up disk space.
Fix/Workaround:
	‚Ä¢ Pruned unused images.
	‚Ä¢ Enabled container runtime garbage collection.
Lessons Learned: DiskPressure can take down entire nodes without warning.
How to Avoid:
	‚Ä¢ Set eviction thresholds properly in kubelet.
	‚Ä¢ Enforce rolling update limits (maxUnavailable).

üìò Scenario #25: Orphaned PVs Causing Unscheduled Pods
Category: Cluster Management
Environment: K8s v1.20, CSI storage on vSphere
Scenario Summary: PVCs were stuck in Pending state due to existing orphaned PVs in Released state.
What Happened: After pod deletion, PVs went into Released state but were never cleaned up due to missing ReclaimPolicy logic. When new PVCs requested the same storage class, provisioning failed.
Diagnosis Steps:
	‚Ä¢ Ran kubectl get pvc ‚Äì saw Pending PVCs.
	‚Ä¢ kubectl get pv ‚Äì old PVs stuck in Released.
	‚Ä¢ CSI driver logs showed volume claim conflicts.
Root Cause: ReclaimPolicy set to Retain and no manual cleanup.
Fix/Workaround:
	‚Ä¢ Manually deleted orphaned PVs.
	‚Ä¢ Changed ReclaimPolicy to Delete for similar volumes.
Lessons Learned: PV lifecycle must be actively monitored.
How to Avoid:
	‚Ä¢ Add cleanup logic in storage lifecycle.
	‚Ä¢ Implement PV alerts based on state.
```
