ðŸ“˜ Scenario #51: High Latency Due to Inefficient Ingress Controller Configuration
Category: Cluster Management
Environment: K8s v1.20, AWS EKS
Scenario Summary: Ingress controller configuration caused high network latency due to inefficient routing rules.
What Happened: Ingress controller was handling a large number of routes inefficiently, resulting in significant network latency and slow response times for external traffic.
Diagnosis Steps:
	â€¢ Analyzed ingress controller logs for routing delays.
	â€¢ Checked ingress resources and discovered unnecessary complex path-based routing rules.
Root Cause: Inefficient ingress routing rules and too many path-based routes led to slower packet processing.
Fix/Workaround:
	â€¢ Simplified ingress resource definitions and optimized routing rules.
	â€¢ Restarted ingress controller to apply changes.
Lessons Learned: Optimizing ingress routing rules is critical for performance, especially in high-traffic environments.
How to Avoid:
	â€¢ Regularly review and optimize ingress resources.
	â€¢ Use a more efficient ingress controller (e.g., NGINX Ingress Controller) for high-volume environments.

ðŸ“˜ Scenario #52: Node Draining Delay During Maintenance
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: Node draining took an unusually long time during maintenance due to unscheduled pod disruption.
What Happened: During a scheduled node maintenance, draining took longer than expected because pods were not respecting PodDisruptionBudgets.
Diagnosis Steps:
	â€¢ Checked kubectl describe for affected pods and identified PodDisruptionBudget violations.
	â€¢ Observed that some pods had hard constraints on disruption due to storage.
Root Cause: PodDisruptionBudget was too strict, preventing pods from being evicted quickly.
Fix/Workaround:
	â€¢ Adjusted PodDisruptionBudget to allow more flexibility for pod evictions.
	â€¢ Manually evicted the pods to speed up the node draining process.
Lessons Learned: PodDisruptionBudgets should be set based on actual disruption tolerance.
How to Avoid:
	â€¢ Set reasonable disruption budgets for critical applications.
	â€¢ Test disruption scenarios during maintenance windows to identify issues.

ðŸ“˜ Scenario #53: Unresponsive Cluster After Large-Scale Deployment
Category: Cluster Management
Environment: K8s v1.19, Azure AKS
Scenario Summary: Cluster became unresponsive after deploying a large number of pods in a single batch.
What Happened: The cluster became unresponsive after deploying a batch of 500 pods in a single operation, causing resource exhaustion.
Diagnosis Steps:
	â€¢ Checked cluster logs and found that the control plane was overwhelmed with API requests.
	â€¢ Observed resource limits on the nodes, which were maxed out.
Root Cause: The large-scale deployment exhausted the clusterâ€™s available resources, causing a spike in API server load.
Fix/Workaround:
	â€¢ Implemented gradual pod deployment using rolling updates instead of a batch deployment.
	â€¢ Increased the node resource capacity to handle larger loads.
Lessons Learned: Gradual deployments and resource planning are necessary when deploying large numbers of pods.
How to Avoid:
	â€¢ Use rolling updates or deploy in smaller batches.
	â€¢ Monitor cluster resources and scale nodes accordingly.

ðŸ“˜ Scenario #54: Failed Node Recovery Due to Corrupt Kubelet Configuration
Category: Cluster Management
Environment: K8s v1.23, Bare Metal
Scenario Summary: Node failed to recover after being drained due to a corrupt kubelet configuration.
What Happened: After a node was drained for maintenance, it failed to rejoin the cluster due to a corrupted kubelet configuration file.
Diagnosis Steps:
	â€¢ Checked kubelet logs and identified errors related to configuration loading.
	â€¢ Verified kubelet configuration file on the affected node and found corruption.
Root Cause: A corrupted kubelet configuration prevented the node from starting properly.
Fix/Workaround:
	â€¢ Replaced the corrupted kubelet configuration file with a backup.
	â€¢ Restarted the kubelet service and the node successfully rejoined the cluster.
Lessons Learned: Regular backups of critical configuration files like kubelet configs can save time during node recovery.
How to Avoid:
	â€¢ Automate backups of critical configurations.
	â€¢ Implement configuration management tools for easier recovery.

ðŸ“˜ Scenario #55: Resource Exhaustion Due to Misconfigured Horizontal Pod Autoscaler
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: Cluster resources were exhausted due to misconfiguration in the Horizontal Pod Autoscaler (HPA), resulting in excessive pod scaling.
What Happened: HPA was configured to scale pods based on CPU utilization but had an overly sensitive threshold, causing the application to scale out rapidly and exhaust resources.
Diagnosis Steps:
	â€¢ Analyzed HPA metrics and found excessive scaling actions.
	â€¢ Verified CPU utilization metrics and observed that they were consistently above the threshold due to a sudden workload spike.
Root Cause: HPA was too aggressive in scaling up based on CPU utilization, without considering other metrics like memory usage or custom metrics.
Fix/Workaround:
	â€¢ Adjusted HPA configuration to scale based on a combination of CPU and memory usage.
	â€¢ Set more appropriate scaling thresholds.
Lessons Learned: Scaling based on a single metric (e.g., CPU) can lead to inefficiency, especially during workload spikes.
How to Avoid:
	â€¢ Use multiple metrics for autoscaling (e.g., CPU, memory, and custom metrics).
	â€¢ Set more conservative scaling thresholds to prevent resource exhaustion.

ðŸ“˜ Scenario #56: Inconsistent Application Behavior After Pod Restart
Category: Cluster Management
Environment: K8s v1.20, GKE
Scenario Summary: Application behavior became inconsistent after pod restarts due to improper state handling.
What Happened: After a pod was restarted, the application started behaving unpredictably, with some users experiencing different results from others due to lack of state synchronization.
Diagnosis Steps:
	â€¢ Checked pod logs and noticed that state data was being stored in the podâ€™s ephemeral storage.
	â€¢ Verified that application code did not handle state persistence properly.
Root Cause: The application was not designed to persist state beyond the pod lifecycle, leading to inconsistent behavior after restarts.
Fix/Workaround:
	â€¢ Moved application state to persistent volumes or external databases.
	â€¢ Adjusted the application logic to handle state recovery properly after restarts.
Lessons Learned: State should be managed outside of ephemeral storage for applications that require consistency.
How to Avoid:
	â€¢ Use persistent volumes for stateful applications.
	â€¢ Implement state synchronization mechanisms where necessary.

ðŸ“˜ Scenario #57: Cluster-wide Service Outage Due to Missing ClusterRoleBinding
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: Cluster-wide service outage occurred after an automated change removed a critical ClusterRoleBinding.
What Happened: A misconfigured automation pipeline accidentally removed a ClusterRoleBinding, which was required for certain critical services to function.
Diagnosis Steps:
	â€¢ Analyzed service logs and found permission-related errors.
	â€¢ Checked the RBAC configuration and found the missing ClusterRoleBinding.
Root Cause: Automated pipeline incorrectly removed the ClusterRoleBinding, causing service permissions to be revoked.
Fix/Workaround:
	â€¢ Restored the missing ClusterRoleBinding.
	â€¢ Manually verified that affected services were functioning correctly.
Lessons Learned: Automation changes must be reviewed and tested to prevent accidental permission misconfigurations.
How to Avoid:
	â€¢ Use automated tests and checks for RBAC changes.
	â€¢ Implement safeguards and approval workflows for automated configuration changes.

ðŸ“˜ Scenario #58: Node Overcommitment Leading to Pod Evictions
Category: Cluster Management
Environment: K8s v1.19, Bare Metal
Scenario Summary: Node overcommitment led to pod evictions, causing application downtime.
What Happened: Due to improper resource requests and limits, the node was overcommitted, which led to the eviction of critical pods.
Diagnosis Steps:
	â€¢ Checked the nodeâ€™s resource utilization and found it was maxed out.
	â€¢ Analyzed pod logs to see eviction messages related to resource limits.
Root Cause: Pods did not have properly set resource requests and limits, leading to resource overcommitment on the node.
Fix/Workaround:
	â€¢ Added appropriate resource requests and limits to the affected pods.
	â€¢ Rescheduled the pods to other nodes with available resources.
Lessons Learned: Properly setting resource requests and limits prevents overcommitment and avoids pod evictions.
How to Avoid:
	â€¢ Always set appropriate resource requests and limits for all pods.
	â€¢ Use resource quotas and limit ranges to prevent overcommitment.

ðŸ“˜ Scenario #59: Failed Pod Startup Due to Image Pull Policy Misconfiguration
Category: Cluster Management
Environment: K8s v1.23, Azure AKS
Scenario Summary: Pods failed to start because the image pull policy was misconfigured.
What Happened: The image pull policy was set to Never, preventing Kubernetes from pulling the required container images from the registry.
Diagnosis Steps:
	â€¢ Checked pod events and found image pull errors.
	â€¢ Verified the image pull policy in the pod specification.
Root Cause: Image pull policy was set to Never, which prevents Kubernetes from pulling images from the registry.
Fix/Workaround:
	â€¢ Changed the image pull policy to IfNotPresent or Always in the pod configuration.
	â€¢ Re-deployed the pods.
Lessons Learned: The correct image pull policy is necessary to ensure Kubernetes can pull container images from a registry.
How to Avoid:
	â€¢ Double-check the image pull policy in pod specifications before deployment.
	â€¢ Use Always for images stored in remote registries.

ðŸ“˜ Scenario #60: Excessive Control Plane Resource Usage During Pod Scheduling
Category: Cluster Management
Environment: K8s v1.24, AWS EKS
Scenario Summary: Control plane resources were excessively utilized during pod scheduling, leading to slow deployments.
What Happened: Pod scheduling took significantly longer than expected due to excessive resource consumption in the control plane.
Diagnosis Steps:
	â€¢ Monitored control plane metrics and found high CPU and memory usage.
	â€¢ Analyzed scheduler logs to identify resource bottlenecks.
Root Cause: The default scheduler was not optimized for high resource consumption, causing delays in pod scheduling.
Fix/Workaround:
	â€¢ Optimized the scheduler configuration to reduce resource usage.
	â€¢ Split large workloads into smaller ones to improve scheduling efficiency.
Lessons Learned: Efficient scheduler configuration is essential for handling large-scale deployments.
How to Avoid:
	â€¢ Optimize scheduler settings for large clusters.
	â€¢ Use scheduler features like affinity and anti-affinity to control pod placement.

ðŸ“˜ Scenario #61: Persistent Volume Claim Failure Due to Resource Quota Exceedance
Category: Cluster Management
Environment: K8s v1.22, GKE
Scenario Summary: Persistent Volume Claims (PVCs) failed due to exceeding the resource quota for storage in the namespace.
What Happened: A user attempted to create PVCs that exceeded the available storage quota, leading to failed PVC creation.
Diagnosis Steps:
	â€¢ Checked the namespace resource quotas using kubectl get resourcequotas.
	â€¢ Observed that the storage limit had been reached.
Root Cause: PVCs exceeded the configured storage resource quota in the namespace.
Fix/Workaround:
	â€¢ Increased the storage quota in the namespace.
	â€¢ Cleaned up unused PVCs to free up space.
Lessons Learned: Proper resource quota management is critical for ensuring that users cannot overuse resources.
How to Avoid:
	â€¢ Regularly review and adjust resource quotas based on usage patterns.
	â€¢ Implement automated alerts for resource quota breaches.

ðŸ“˜ Scenario #62: Failed Pod Rescheduling Due to Node Affinity Misconfiguration
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: Pods failed to reschedule after a node failure due to improper node affinity rules.
What Happened: When a node was taken down for maintenance, the pod failed to reschedule due to restrictive node affinity settings.
Diagnosis Steps:
	â€¢ Checked pod events and noticed affinity rule errors preventing the pod from scheduling on other nodes.
	â€¢ Analyzed the node affinity configuration in the pod spec.
Root Cause: Node affinity rules were set too restrictively, preventing the pod from being scheduled on other nodes.
Fix/Workaround:
	â€¢ Adjusted the node affinity rules to be less restrictive.
	â€¢ Re-scheduled the pods to available nodes.
Lessons Learned: Affinity rules should be configured to provide sufficient flexibility for pod rescheduling.
How to Avoid:
	â€¢ Set node affinity rules based on availability and workloads.
	â€¢ Regularly test affinity and anti-affinity rules during node maintenance windows.

ðŸ“˜ Scenario #63: Intermittent Network Latency Due to Misconfigured CNI Plugin
Category: Cluster Management
Environment: K8s v1.24, Azure AKS
Scenario Summary: Network latency issues occurred intermittently due to misconfiguration in the CNI (Container Network Interface) plugin.
What Happened: Network latency was sporadically high between pods due to improper settings in the CNI plugin.
Diagnosis Steps:
	â€¢ Analyzed network metrics and noticed high latency between pods in different nodes.
	â€¢ Checked CNI plugin logs and configuration and found incorrect MTU (Maximum Transmission Unit) settings.
Root Cause: MTU misconfiguration in the CNI plugin caused packet fragmentation, resulting in network latency.
Fix/Workaround:
	â€¢ Corrected the MTU setting in the CNI configuration to match the network infrastructure.
	â€¢ Restarted the CNI plugin and verified network performance.
Lessons Learned: Proper CNI configuration is essential to avoid network latency and connectivity issues.
How to Avoid:
	â€¢ Ensure CNI plugin configurations match the underlying network settings.
	â€¢ Test network performance after changes to the CNI configuration.

ðŸ“˜ Scenario #64: Excessive Pod Restarts Due to Resource Limits
Category: Cluster Management
Environment: K8s v1.19, GKE
Scenario Summary: A pod was restarting frequently due to resource limits being too low, causing the container to be killed.
What Happened: Pods were being killed and restarted due to the containerâ€™s resource requests and limits being set too low, causing OOM (Out of Memory) kills.
Diagnosis Steps:
	â€¢ Checked pod logs and identified frequent OOM kills.
	â€¢ Reviewed resource requests and limits in the pod spec.
Root Cause: Resource limits were too low, leading to the container being killed when it exceeded available memory.
Fix/Workaround:
	â€¢ Increased the memory limits and requests for the affected pods.
	â€¢ Re-deployed the updated pods and monitored for stability.
Lessons Learned: Proper resource requests and limits should be set to avoid OOM kills and pod restarts.
How to Avoid:
	â€¢ Regularly review resource requests and limits based on workload requirements.
	â€¢ Use resource usage metrics to set more accurate resource limits.

ðŸ“˜ Scenario #65: Cluster Performance Degradation Due to Excessive Logs
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: Cluster performance degraded because of excessive logs being generated by applications, leading to high disk usage.
What Happened: Excessive log output from applications filled up the disk, slowing down the entire cluster.
Diagnosis Steps:
	â€¢ Monitored disk usage and found that logs were consuming most of the disk space.
	â€¢ Identified the affected applications by reviewing the logging configuration.
Root Cause: Applications were logging excessively, and log rotation was not properly configured.
Fix/Workaround:
	â€¢ Configured log rotation for the affected applications.
	â€¢ Reduced the verbosity of the logs in application settings.
Lessons Learned: Proper log management and rotation are essential to avoid filling up disk space and impacting cluster performance.
How to Avoid:
	â€¢ Configure log rotation and retention policies for all applications.
	â€¢ Monitor disk usage and set up alerts for high usage.

ðŸ“˜ Scenario #66: Insufficient Cluster Capacity Due to Unchecked CronJobs
Category: Cluster Management
Environment: K8s v1.21, GKE
Scenario Summary: The cluster experienced resource exhaustion because CronJobs were running in parallel without proper capacity checks.
What Happened: Several CronJobs were triggered simultaneously, causing the cluster to run out of CPU and memory resources.
Diagnosis Steps:
	â€¢ Checked CronJob schedules and found multiple jobs running at the same time.
	â€¢ Monitored resource usage and identified high CPU and memory consumption from the CronJobs.
Root Cause: Lack of resource limits and concurrent job checks in CronJobs.
Fix/Workaround:
	â€¢ Added resource requests and limits for CronJobs.
	â€¢ Configured CronJobs to stagger their execution times to avoid simultaneous execution.
Lessons Learned: Always add resource limits and configure CronJobs to prevent them from running in parallel and exhausting cluster resources.
How to Avoid:
	â€¢ Set appropriate resource requests and limits for CronJobs.
	â€¢ Use concurrencyPolicy to control parallel executions of CronJobs.

ðŸ“˜ Scenario #67: Unsuccessful Pod Scaling Due to Affinity/Anti-Affinity Conflict
Category: Cluster Management
Environment: K8s v1.23, Azure AKS
Scenario Summary: Pod scaling failed due to conflicting affinity/anti-affinity rules that prevented pods from being scheduled.
What Happened: A deploymentâ€™s pod scaling was unsuccessful due to the anti-affinity rules that conflicted with available nodes.
Diagnosis Steps:
	â€¢ Checked pod scaling logs and identified unschedulable errors related to affinity rules.
	â€¢ Reviewed affinity/anti-affinity settings in the pod deployment configuration.
Root Cause: The anti-affinity rule required pods to be scheduled on specific nodes, but there were not enough available nodes.
Fix/Workaround:
	â€¢ Relaxed the anti-affinity rule to allow pods to be scheduled on any available node.
	â€¢ Increased the number of nodes to ensure sufficient capacity.
Lessons Learned: Affinity and anti-affinity rules should be configured carefully, especially in dynamic environments with changing node capacity.
How to Avoid:
	â€¢ Test affinity and anti-affinity configurations thoroughly.
	â€¢ Use flexible affinity rules to allow for dynamic scaling and node availability.

ðŸ“˜ Scenario #68: Cluster Inaccessibility Due to API Server Throttling
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: Cluster became inaccessible due to excessive API server throttling caused by too many concurrent requests.
What Happened: The API server started throttling requests because the number of concurrent API calls exceeded the available limit.
Diagnosis Steps:
	â€¢ Monitored API server metrics and identified a high rate of incoming requests.
	â€¢ Checked client application logs and observed excessive API calls.
Root Cause: Clients were making too many API requests in a short period, exceeding the rate limits of the API server.
Fix/Workaround:
	â€¢ Throttled client requests to reduce API server load.
	â€¢ Implemented exponential backoff for retries in client applications.
Lessons Learned: Avoid overwhelming the API server with excessive requests and implement rate-limiting mechanisms.
How to Avoid:
	â€¢ Implement API request throttling and retries in client applications.
	â€¢ Use rate-limiting tools like kubectl to monitor API usage.

ðŸ“˜ Scenario #69: Persistent Volume Expansion Failure
Category: Cluster Management
Environment: K8s v1.20, GKE
Scenario Summary: Expansion of a Persistent Volume (PV) failed due to improper storage class settings.
What Happened: The request to expand a persistent volume failed because the storage class was not configured to support volume expansion.
Diagnosis Steps:
	â€¢ Verified the PV and PVC configurations.
	â€¢ Checked the storage class settings and identified that volume expansion was not enabled.
Root Cause: The storage class did not have the allowVolumeExpansion flag set to true.
Fix/Workaround:
	â€¢ Updated the storage class to allow volume expansion.
	â€¢ Expanded the persistent volume and verified the PVC reflected the changes.
Lessons Learned: Ensure that storage classes are configured to allow volume expansion when using dynamic provisioning.
How to Avoid:
	â€¢ Check storage class configurations before creating PVs.
	â€¢ Enable allowVolumeExpansion for dynamic storage provisioning.

ðŸ“˜ Scenario #70: Unauthorized Access to Cluster Resources Due to RBAC Misconfiguration
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: Unauthorized users gained access to sensitive resources due to misconfigured RBAC roles and bindings.
What Happened: An RBAC misconfiguration allowed unauthorized users to access cluster resources, including secrets.
Diagnosis Steps:
	â€¢ Checked RBAC policies and found overly permissive role bindings.
	â€¢ Analyzed user access logs and identified unauthorized access to sensitive resources.
Root Cause: Over-permissive RBAC role bindings granted excessive access to unauthorized users.
Fix/Workaround:
	â€¢ Corrected RBAC policies to restrict access.
	â€¢ Audited user access and removed unauthorized permissions.
Lessons Learned: Proper RBAC configuration is crucial for securing cluster resources.
How to Avoid:
	â€¢ Implement the principle of least privilege for RBAC roles.
	â€¢ Regularly audit RBAC policies and bindings.

ðŸ“˜ Scenario #71: Inconsistent Pod State Due to Image Pull Failures
Category: Cluster Management
Environment: K8s v1.20, GKE
Scenario Summary: Pods entered an inconsistent state because the container image failed to pull due to incorrect image tag.
What Happened: Pods started with an image tag that did not exist in the container registry, causing the pods to enter a CrashLoopBackOff state.
Diagnosis Steps:
	â€¢ Checked the pod events and found image pull errors with "Tag not found" messages.
	â€¢ Verified the image tag in the deployment configuration.
Root Cause: The container image tag specified in the deployment was incorrect or missing from the container registry.
Fix/Workaround:
	â€¢ Corrected the image tag in the deployment configuration to point to an existing image.
	â€¢ Redeployed the application.
Lessons Learned: Always verify image tags before deploying and ensure the image is available in the registry.
How to Avoid:
	â€¢ Use CI/CD pipelines to automatically verify image availability before deployment.
	â€¢ Enable image pull retries for transient network issues.

ðŸ“˜ Scenario #72: Pod Disruption Due to Insufficient Node Resources
Category: Cluster Management
Environment: K8s v1.22, Azure AKS
Scenario Summary: Pods experienced disruptions as nodes ran out of CPU and memory, causing evictions.
What Happened: During a high workload period, nodes ran out of resources, causing the scheduler to evict pods and causing disruptions.
Diagnosis Steps:
	â€¢ Monitored node resource usage and identified CPU and memory exhaustion.
	â€¢ Reviewed pod events and noticed pod evictions due to resource pressure.
Root Cause: Insufficient node resources for the workload being run, causing resource contention and pod evictions.
Fix/Workaround:
	â€¢ Added more nodes to the cluster to meet resource requirements.
	â€¢ Adjusted pod resource requests/limits to be more aligned with node resources.
Lessons Learned: Regularly monitor and scale nodes to ensure sufficient resources during peak workloads.
How to Avoid:
	â€¢ Use cluster autoscaling to add nodes automatically when resource pressure increases.
	â€¢ Set appropriate resource requests and limits for pods.

ðŸ“˜ Scenario #73: Service Discovery Issues Due to DNS Resolution Failures
Category: Cluster Management
Environment: K8s v1.21, AWS EKS
Scenario Summary: Services could not discover each other due to DNS resolution failures, affecting internal communication.
What Happened: Pods were unable to resolve internal service names due to DNS failures, leading to broken inter-service communication.
Diagnosis Steps:
	â€¢ Checked DNS logs and found dnsmasq errors.
	â€¢ Investigated CoreDNS logs and found insufficient resources allocated to the DNS pods.
Root Cause: CoreDNS pods were running out of resources (CPU/memory), causing DNS resolution failures.
Fix/Workaround:
	â€¢ Increased resource limits for the CoreDNS pods.
	â€¢ Restarted CoreDNS pods to apply the new resource settings.
Lessons Learned: Ensure that CoreDNS has enough resources to handle DNS requests efficiently.
How to Avoid:
	â€¢ Monitor CoreDNS pod resource usage.
	â€¢ Allocate adequate resources based on cluster size and workload.

ðŸ“˜ Scenario #74: Persistent Volume Provisioning Delays
Category: Cluster Management
Environment: K8s v1.22, GKE
Scenario Summary: Persistent volume provisioning was delayed due to an issue with the dynamic provisioner.
What Happened: PVCs were stuck in the Pending state because the dynamic provisioner could not create the required PVs.
Diagnosis Steps:
	â€¢ Checked PVC status using kubectl get pvc and saw that they were stuck in Pending.
	â€¢ Investigated storage class settings and found an issue with the provisioner configuration.
Root Cause: Misconfigured storage class settings were preventing the dynamic provisioner from provisioning volumes.
Fix/Workaround:
	â€¢ Corrected the storage class settings, ensuring the correct provisioner was specified.
	â€¢ Recreated the PVCs, and provisioning completed successfully.
Lessons Learned: Validate storage class settings and provisioner configurations during cluster setup.
How to Avoid:
	â€¢ Test storage classes and volume provisioning in staging environments before production use.
	â€¢ Monitor PV provisioning and automate alerts for failures.

ðŸ“˜ Scenario #75: Deployment Rollback Failure Due to Missing Image
Category: Cluster Management
Environment: K8s v1.21, Azure AKS
Scenario Summary: A deployment rollback failed due to the rollback image version no longer being available in the container registry.
What Happened: After an update, the deployment was rolled back to a previous image version that was no longer present in the container registry, causing the rollback to fail.
Diagnosis Steps:
	â€¢ Checked the deployment history and found that the previous image was no longer available.
	â€¢ Examined the container registry and confirmed the image version had been deleted.
Root Cause: The image version intended for rollback was deleted from the registry before the rollback occurred.
Fix/Workaround:
	â€¢ Rebuilt the previous image version and pushed it to the registry.
	â€¢ Triggered a successful rollback after the image was available.
Lessons Learned: Always retain previous image versions for safe rollbacks.
How to Avoid:
	â€¢ Implement retention policies for container images.
	â€¢ Use CI/CD pipelines to tag and store images for future rollbacks.