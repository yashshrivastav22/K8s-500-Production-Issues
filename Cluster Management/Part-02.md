```
üìò Scenario #26: Taints and Tolerations Mismatch Prevented Workload Scheduling
Category: Cluster Management
Environment: K8s v1.22, managed AKS
Scenario Summary: Workloads failed to schedule on new nodes that had a taint the workloads didn‚Äôt tolerate.
What Happened: Platform team added a new node pool with node-role.kubernetes.io/gpu:NoSchedule, but forgot to add tolerations to GPU workloads.
Diagnosis Steps:
	‚Ä¢ kubectl describe pod ‚Äì showed reason: ‚Äú0/3 nodes are available: node(s) had taints‚Äù.
	‚Ä¢ Checked node taints via kubectl get nodes -o json.
Root Cause: Taints on new node pool weren‚Äôt matched by tolerations in pods.
Fix/Workaround:
	‚Ä¢ Added proper tolerations to workloads:

yaml
CopyEdit
tolerations:
- key: "node-role.kubernetes.io/gpu"
  operator: "Exists"
  effect: "NoSchedule"
Lessons Learned: Node taints should be coordinated with scheduling policies.
How to Avoid:
	‚Ä¢ Use preset toleration templates in CI/CD pipelines.
	‚Ä¢ Test new node pools with dummy workloads.

üìò Scenario #27: Node Bootstrap Failure Due to Unavailable Container Registry
Category: Cluster Management
Environment: K8s v1.21, on-prem, private registry
Scenario Summary: New nodes failed to join the cluster due to container runtime timeout when pulling base images.
What Happened: The internal Docker registry was down during node provisioning, so containerd couldn't pull pauseand CNI images. Nodes stayed in NotReady state.
Diagnosis Steps:
	‚Ä¢ journalctl -u containerd ‚Äì repeated image pull failures.
	‚Ä¢ Node conditions showed ContainerRuntimeNotReady.
Root Cause: Bootstrap process relies on image pulls from unavailable registry.
Fix/Workaround:
	‚Ä¢ Brought internal registry back online.
	‚Ä¢ Pre-pulled pause/CNI images to node image templates.
Lessons Learned: Registry availability is a bootstrap dependency.
How to Avoid:
	‚Ä¢ Preload all essential images into AMI/base image.
	‚Ä¢ Monitor registry uptime independently.

üìò Scenario #28: kubelet Fails to Start Due to Expired TLS Certs
Category: Cluster Management
Environment: K8s v1.19, kubeadm cluster
Scenario Summary: Several nodes went NotReady after reboot due to kubelet failing to start with expired client certs.
What Happened: Kubelet uses a client certificate for authentication with the API server. These are typically auto-rotated, but the nodes were offline when the rotation was due.
Diagnosis Steps:
	‚Ä¢ journalctl -u kubelet ‚Äì cert expired error.
	‚Ä¢ /var/lib/kubelet/pki/kubelet-client-current.pem ‚Äì expired date.
Root Cause: Kubelet cert rotation missed due to node downtime.
Fix/Workaround:
	‚Ä¢ Regenerated kubelet certs using kubeadm.

bash
CopyEdit
kubeadm certs renew all
Lessons Learned: Cert rotation has a dependency on uptime.
How to Avoid:
	‚Ä¢ Monitor cert expiry proactively.
	‚Ä¢ Rotate certs manually before planned outages.

üìò Scenario #29: kube-scheduler Crash Due to Invalid Leader Election Config
Category: Cluster Management
Environment: K8s v1.24, custom scheduler deployment
Scenario Summary: kube-scheduler pod failed with panic due to misconfigured leader election flags.
What Happened: An override in the Helm chart introduced an invalid leader election namespace, causing the scheduler to panic and crash on startup.
Diagnosis Steps:
	‚Ä¢ Pod logs showed panic: cannot create leader election record.
	‚Ä¢ Checked Helm values ‚Äì found wrong namespace name.
Root Cause: Namespace specified for leader election did not exist.
Fix/Workaround:
	‚Ä¢ Created the missing namespace.
	‚Ä¢ Restarted the scheduler pod.
Lessons Learned: Leader election is sensitive to namespace scoping.
How to Avoid:
	‚Ä¢ Use default kube-system unless explicitly scoped.
	‚Ä¢ Validate all scheduler configs with CI linting.

üìò Scenario #30: Cluster DNS Resolution Broken After Calico CNI Update
Category: Cluster Management
Environment: K8s v1.23, self-hosted Calico
Scenario Summary: DNS resolution broke after Calico CNI update due to iptables policy drop changes.
What Happened: New version of Calico enforced stricter iptables drop policies, blocking traffic from CoreDNS to pods.
Diagnosis Steps:
	‚Ä¢ DNS requests timed out.
	‚Ä¢ Packet capture showed ICMP unreachable from pods to CoreDNS.
	‚Ä¢ Checked Calico policy and iptables rules.
Root Cause: Calico‚Äôs default deny policy applied to kube-dns traffic.
Fix/Workaround:
	‚Ä¢ Added explicit Calico policy allowing kube-dns to pod traffic.

yaml:
egress:
- action: Allow
  destination:
    selector: "k8s-app == 'kube-dns'"

Lessons Learned: CNI policy changes can impact DNS without warning.
How to Avoid:
	‚Ä¢ Review and test all network policy upgrades in staging.
	‚Ä¢ Use canary upgrade strategy for CNI.

üìò Scenario #31: Node Clock Drift Causing Authentication Failures
Category: Cluster Management
Environment: K8s v1.22, on-prem, kubeadm
Scenario Summary: Authentication tokens failed across the cluster due to node clock skew.
What Happened: Token-based authentication failed for all workloads and kubectl access due to time drift between worker nodes and the API server.
Diagnosis Steps:
	‚Ä¢ Ran kubectl logs and found expired token errors.
	‚Ä¢ Checked node time using date on each node ‚Äì found significant drift.
	‚Ä¢ Verified NTP daemon status ‚Äì not running.
Root Cause: NTP daemon disabled on worker nodes.
Fix/Workaround:
	‚Ä¢ Re-enabled and restarted NTP on all nodes.
	‚Ä¢ Synchronized system clocks manually.
Lessons Learned: Time synchronization is critical for certificate and token-based auth.
How to Avoid:
	‚Ä¢ Ensure NTP or chrony is enabled via bootstrap configuration.
	‚Ä¢ Monitor time drift via node-exporter.

üìò Scenario #32: Inconsistent Node Labels Causing Scheduling Bugs
Category: Cluster Management
Environment: K8s v1.24, multi-zone GKE
Scenario Summary: Zone-aware workloads failed to schedule due to missing zone labels on some nodes.
What Happened: Pods using topologySpreadConstraints for zone balancing failed to find valid nodes because some nodes lacked the topology.kubernetes.io/zone label.
Diagnosis Steps:
	‚Ä¢ Pod events showed no matching topology key errors.
	‚Ä¢ Compared node labels across zones ‚Äì found inconsistency.
Root Cause: A few nodes were manually added without required zone labels.
Fix/Workaround:
	‚Ä¢ Manually patched node labels to restore zone metadata.
Lessons Learned: Label uniformity is essential for topology constraints.
How to Avoid:
	‚Ä¢ Automate label injection using cloud-init or DaemonSet.
	‚Ä¢ Add CI checks for required labels on node join.

üìò Scenario #33: API Server Slowdowns from High Watch Connection Count
Category: Cluster Management
Environment: K8s v1.23, OpenShift
Scenario Summary: API latency rose sharply due to thousands of watch connections from misbehaving clients.
What Happened: Multiple pods opened persistent watch connections and never closed them, overloading the API server.
Diagnosis Steps:
	‚Ä¢ Monitored API metrics /metrics for apiserver_registered_watchers.
	‚Ä¢ Identified top offenders using connection source IPs.
Root Cause: Custom controller with poor watch logic never closed connections.
Fix/Workaround:
	‚Ä¢ Restarted offending pods.
	‚Ä¢ Updated controller to reuse watches.
Lessons Learned: Unbounded watches can exhaust server resources.
How to Avoid:
	‚Ä¢ Use client-go with resync periods and connection limits.
	‚Ä¢ Enable metrics to detect watch leaks early.

üìò Scenario #34: Etcd Disk Full Crashing the Cluster
Category: Cluster Management
Environment: K8s v1.21, self-managed with local etcd
Scenario Summary: Entire control plane crashed due to etcd disk running out of space.
What Happened: Continuous writes from custom resources filled the disk where etcd data was stored.
Diagnosis Steps:
	‚Ä¢ Observed etcdserver: mvcc: database space exceeded errors.
	‚Ä¢ Checked disk usage: df -h showed 100% full.
Root Cause: No compaction or defragmentation done on etcd for weeks.
Fix/Workaround:
	‚Ä¢ Performed etcd compaction and defragmentation.
	‚Ä¢ Added disk space temporarily.
Lessons Learned: Etcd needs regular maintenance.
How to Avoid:
	‚Ä¢ Set up cron jobs or alerts for etcd health.
	‚Ä¢ Monitor disk usage and trigger auto-compaction.

üìò Scenario #35: ClusterConfigMap Deleted by Accident Bringing Down Addons
Category: Cluster Management
Environment: K8s v1.24, Rancher
Scenario Summary: A user accidentally deleted the kube-root-ca.crt ConfigMap, which many workloads relied on.
What Happened: Pods mounting the kube-root-ca.crt ConfigMap failed to start after deletion. DNS, metrics-server, and other system components failed.
Diagnosis Steps:
	‚Ä¢ Pod events showed missing ConfigMap errors.
	‚Ä¢ Attempted to remount volumes manually.
Root Cause: System-critical ConfigMap was deleted without RBAC protections.
Fix/Workaround:
	‚Ä¢ Recreated ConfigMap from backup.
	‚Ä¢ Re-deployed affected system workloads.
Lessons Learned: Some ConfigMaps are essential and must be protected.
How to Avoid:
	‚Ä¢ Add RBAC restrictions to system namespaces.
	‚Ä¢ Use OPA/Gatekeeper to prevent deletions of protected resources.

üìò Scenario #36: Misconfigured NodeAffinity Excluding All Nodes
Category: Cluster Management
Environment: K8s v1.22, Azure AKS
Scenario Summary: A critical deployment was unschedulable due to strict nodeAffinity rules.
What Happened: nodeAffinity required a zone that did not exist in the cluster, making all nodes invalid.
Diagnosis Steps:
	‚Ä¢ Pod events showed 0/10 nodes available errors.
	‚Ä¢ Checked spec.affinity section in deployment YAML.
Root Cause: Invalid or overly strict requiredDuringScheduling nodeAffinity.
Fix/Workaround:
	‚Ä¢ Updated deployment YAML to reflect actual zones.
	‚Ä¢ Re-deployed workloads.
Lessons Learned: nodeAffinity is strict and should be used carefully.
How to Avoid:
	‚Ä¢ Validate node labels before setting affinity.
	‚Ä¢ Use preferredDuringScheduling for soft constraints.

üìò Scenario #37: Outdated Admission Webhook Blocking All Deployments
Category: Cluster Management
Environment: K8s v1.25, self-hosted
Scenario Summary: A stale mutating webhook caused all deployments to fail due to TLS certificate errors.
What Happened: The admission webhook had expired TLS certs, causing validation errors on all resource creation attempts.
Diagnosis Steps:
	‚Ä¢ Created a dummy pod and observed webhook errors.
	‚Ä¢ Checked logs of the webhook pod ‚Äì found TLS handshake failures.
Root Cause: Webhook server was down due to expired TLS cert.
Fix/Workaround:
	‚Ä¢ Renewed cert and redeployed webhook.
	‚Ä¢ Disabled webhook temporarily for emergency deployments.
Lessons Learned: Webhooks are gatekeepers ‚Äì they must be monitored.
How to Avoid:
	‚Ä¢ Rotate webhook certs using cert-manager.
	‚Ä¢ Alert on webhook downtime or errors.

üìò Scenario #38: API Server Certificate Expiry Blocking Cluster Access
Category: Cluster Management
Environment: K8s v1.19, kubeadm
Scenario Summary: After 1 year of uptime, API server certificate expired, blocking access to all components.
What Happened: Default kubeadm cert rotation didn‚Äôt occur, leading to expiry of API server and etcd peer certs.
Diagnosis Steps:
	‚Ä¢ kubectl failed with x509: certificate has expired.
	‚Ä¢ Checked /etc/kubernetes/pki/apiserver.crt expiry date.
Root Cause: kubeadm certificates were never rotated or renewed.
Fix/Workaround:
	‚Ä¢ Used kubeadm certs renew all.
	‚Ä¢ Restarted control plane components.
Lessons Learned: Certificates expire silently unless monitored.
How to Avoid:
	‚Ä¢ Rotate certs before expiry.
	‚Ä¢ Monitor /metrics for cert validity.

üìò Scenario #39: CRI Socket Mismatch Preventing kubelet Startup
Category: Cluster Management
Environment: K8s v1.22, containerd switch
Scenario Summary: kubelet failed to start after switching from Docker to containerd due to incorrect CRI socket path.
What Happened: The node image had containerd installed, but the kubelet still pointed to the Docker socket.
Diagnosis Steps:
	‚Ä¢ Checked kubelet logs for failed to connect to CRI socket.
	‚Ä¢ Verified config file at /var/lib/kubelet/kubeadm-flags.env.
Root Cause: Wrong --container-runtime-endpoint specified.
Fix/Workaround:
	‚Ä¢ Updated kubelet flags to point to /run/containerd/containerd.sock.
	‚Ä¢ Restarted kubelet.
Lessons Learned: CRI migration requires explicit config updates.
How to Avoid:
	‚Ä¢ Use migration scripts or kubeadm migration guides.
	‚Ä¢ Validate container runtime on node bootstrap.

üìò Scenario #40: Cluster-Wide Crash Due to Misconfigured Resource Quotas
Category: Cluster Management
Environment: K8s v1.24, multi-tenant namespace setup
Scenario Summary: Cluster workloads failed after applying overly strict resource quotas that denied new pod creation.
What Happened: A new quota was applied with very low CPU/memory limits. All new pods across namespaces failed scheduling.
Diagnosis Steps:
	‚Ä¢ Pod events showed failed quota check errors.
	‚Ä¢ Checked quota via kubectl describe quota in all namespaces.
Root Cause: Misconfigured CPU/memory limits set globally.
Fix/Workaround:
	‚Ä¢ Rolled back the quota to previous values.
	‚Ä¢ Unblocked critical namespaces manually.
Lessons Learned: Quota changes should be staged and validated.
How to Avoid:
	‚Ä¢ Test new quotas in shadow or dry-run mode.
	‚Ä¢ Use automated checks before applying quotas.

üìò Scenario #41: Cluster Upgrade Failing Due to CNI Compatibility
Category: Cluster Management
Environment: K8s v1.21 to v1.22, custom CNI plugin
Scenario Summary: Cluster upgrade failed due to an incompatible version of the CNI plugin.
What Happened: After upgrading the control plane, CNI plugins failed to work, resulting in no network connectivity between pods.
Diagnosis Steps:
	‚Ä¢ Checked kubelet and container runtime logs ‚Äì observed CNI errors.
	‚Ä¢ Verified CNI plugin version ‚Äì it was incompatible with K8s v1.22.
Root Cause: CNI plugin was not upgraded alongside the Kubernetes control plane.
Fix/Workaround:
	‚Ä¢ Upgraded the CNI plugin to the version compatible with K8s v1.22.
	‚Ä¢ Restarted affected pods and nodes.
Lessons Learned: Always ensure compatibility between the Kubernetes version and CNI plugin.
How to Avoid:
	‚Ä¢ Follow Kubernetes upgrade documentation and ensure CNI plugins are upgraded.
	‚Ä¢ Test in a staging environment before performing production upgrades.

üìò Scenario #42: Failed Pod Security Policy Enforcement Causing Privileged Container Launch
Category: Cluster Management
Environment: K8s v1.22, AWS EKS
Scenario Summary: Privileged containers were able to run despite Pod Security Policy enforcement.
What Happened: A container was able to run as privileged despite a restrictive PodSecurityPolicy being in place.
Diagnosis Steps:
	‚Ä¢ Checked pod events and logs, found no violations of PodSecurityPolicy.
	‚Ä¢ Verified PodSecurityPolicy settings and namespace annotations.
Root Cause: PodSecurityPolicy was not enforced due to missing podsecuritypolicy admission controller.
Fix/Workaround:
	‚Ä¢ Enabled the podsecuritypolicy admission controller.
	‚Ä¢ Updated the PodSecurityPolicy to restrict privileged containers.
Lessons Learned: Admission controllers must be properly configured for security policies to be enforced.
How to Avoid:
	‚Ä¢ Double-check admission controller configurations during initial cluster setup.
	‚Ä¢ Regularly audit security policies and admission controllers.

üìò Scenario #43: Node Pool Scaling Impacting StatefulSets
Category: Cluster Management
Environment: K8s v1.24, GKE
Scenario Summary: StatefulSet pods were rescheduled across different nodes, breaking persistent volume bindings.
What Happened: Node pool scaling in GKE triggered a rescheduling of StatefulSet pods, breaking persistent volume claims that were tied to specific nodes.
Diagnosis Steps:
	‚Ä¢ Observed failed to bind volume errors.
	‚Ä¢ Checked StatefulSet configuration for node affinity and volume binding policies.
Root Cause: Lack of proper node affinity or persistent volume binding policies in StatefulSet configuration.
Fix/Workaround:
	‚Ä¢ Added proper node affinity rules and volume binding policies to StatefulSet.
	‚Ä¢ Rescheduled the pods successfully.
Lessons Learned: StatefulSets require careful management of node affinity and persistent volume binding policies.
How to Avoid:
	‚Ä¢ Use pod affinity rules for StatefulSets to ensure proper scheduling and volume binding.
	‚Ä¢ Monitor volume binding status when scaling node pools.

üìò Scenario #44: Kubelet Crash Due to Out of Memory (OOM) Errors
Category: Cluster Management
Environment: K8s v1.20, bare metal
Scenario Summary: Kubelet crashed after running out of memory due to excessive pod resource usage.
What Happened: The kubelet on a node crashed after the available memory was exhausted due to pods consuming more memory than allocated.
Diagnosis Steps:
	‚Ä¢ Checked kubelet logs for OOM errors.
	‚Ä¢ Used kubectl describe node to check resource utilization.
Root Cause: Pod resource requests and limits were not set properly, leading to excessive memory consumption.
Fix/Workaround:
	‚Ä¢ Set proper resource requests and limits on pods to prevent memory over-consumption.
	‚Ä¢ Restarted the kubelet on the affected node.
Lessons Learned: Pod resource limits and requests are essential for proper node resource utilization.
How to Avoid:
	‚Ä¢ Set reasonable resource requests and limits for all pods.
	‚Ä¢ Monitor node resource usage to catch resource overuse before it causes crashes.

üìò Scenario #45: DNS Resolution Failure in Multi-Cluster Setup
Category: Cluster Management
Environment: K8s v1.23, multi-cluster federation
Scenario Summary: DNS resolution failed between two federated clusters due to missing DNS records.
What Happened: DNS queries failed between two federated clusters, preventing services from accessing each other across clusters.
Diagnosis Steps:
	‚Ä¢ Used kubectl get svc to check DNS records.
	‚Ä¢ Identified missing service entries in the DNS server configuration.
Root Cause: DNS configuration was incomplete, missing records for federated services.
Fix/Workaround:
	‚Ä¢ Added missing DNS records manually.
	‚Ä¢ Updated DNS configurations to include service records for all federated clusters.
Lessons Learned: In multi-cluster setups, DNS configuration is critical to service discovery.
How to Avoid:
	‚Ä¢ Automate DNS record creation during multi-cluster federation setup.
	‚Ä¢ Regularly audit DNS configurations in multi-cluster environments.

üìò Scenario #46: Insufficient Resource Limits in Autoscaling Setup
Category: Cluster Management
Environment: K8s v1.21, GKE with Horizontal Pod Autoscaler (HPA)
Scenario Summary: Horizontal Pod Autoscaler did not scale pods up as expected due to insufficient resource limits.
What Happened: The Horizontal Pod Autoscaler failed to scale the application pods up, even under load, due to insufficient resource limits set on the pods.
Diagnosis Steps:
	‚Ä¢ Observed HPA metrics showing no scaling action.
	‚Ä¢ Checked pod resource requests and limits.
Root Cause: Resource limits were too low for HPA to trigger scaling actions.
Fix/Workaround:
	‚Ä¢ Increased resource requests and limits for the affected pods.
	‚Ä¢ Manually scaled the pods and monitored the autoscaling behavior.
Lessons Learned: Proper resource limits are essential for autoscaling to function correctly.
How to Avoid:
	‚Ä¢ Set adequate resource requests and limits for workloads managed by HPA.
	‚Ä¢ Monitor autoscaling events to identify under-scaling issues.

üìò Scenario #47: Control Plane Overload Due to High Audit Log Volume
Category: Cluster Management
Environment: K8s v1.22, Azure AKS
Scenario Summary: The control plane became overloaded and slow due to excessive audit log volume.
What Happened: A misconfigured audit policy led to high volumes of audit logs being generated, overwhelming the control plane.
Diagnosis Steps:
	‚Ä¢ Monitored control plane metrics and found high CPU usage due to audit logs.
	‚Ä¢ Reviewed audit policy and found it was logging excessive data.
Root Cause: Overly broad audit log configuration captured too many events.
Fix/Workaround:
	‚Ä¢ Refined audit policy to log only critical events.
	‚Ä¢ Restarted the API server.
Lessons Learned: Audit logging needs to be fine-tuned to prevent overload.
How to Avoid:
	‚Ä¢ Regularly review and refine audit logging policies.
	‚Ä¢ Set alerts for high audit log volumes.

üìò Scenario #48: Resource Fragmentation Causing Cluster Instability
Category: Cluster Management
Environment: K8s v1.23, bare metal
Scenario Summary: Resource fragmentation due to unbalanced pod distribution led to cluster instability.
What Happened: Over time, pod distribution became uneven, with some nodes over-committed while others remained underutilized. This caused resource fragmentation, leading to cluster instability.
Diagnosis Steps:
	‚Ä¢ Checked node resource utilization and found over-committed nodes with high pod density.
	‚Ä¢ Examined pod distribution and noticed skewed placement.
Root Cause: Lack of proper pod scheduling and resource management.
Fix/Workaround:
	‚Ä¢ Applied pod affinity and anti-affinity rules to achieve balanced scheduling.
	‚Ä¢ Rescheduled pods manually to redistribute workload.
Lessons Learned: Resource management and scheduling rules are crucial for maintaining cluster stability.
How to Avoid:
	‚Ä¢ Use affinity and anti-affinity rules to control pod placement.
	‚Ä¢ Regularly monitor resource utilization and adjust pod placement strategies.

üìò Scenario #49: Failed Cluster Backup Due to Misconfigured Volume Snapshots
Category: Cluster Management
Environment: K8s v1.21, AWS EBS
Scenario Summary: Cluster backup failed due to a misconfigured volume snapshot driver.
What Happened: The backup process failed because the EBS volume snapshot driver was misconfigured, resulting in incomplete backups.
Diagnosis Steps:
	‚Ä¢ Checked backup logs for error messages related to volume snapshot failures.
	‚Ä¢ Verified snapshot driver configuration in storage class.
Root Cause: Misconfigured volume snapshot driver prevented proper backups.
Fix/Workaround:
	‚Ä¢ Corrected snapshot driver configuration in storage class.
	‚Ä¢ Ran the backup process again, which completed successfully.
Lessons Learned: Backup configuration must be thoroughly checked and tested.
How to Avoid:
	‚Ä¢ Automate backup testing and validation in staging environments.
	‚Ä¢ Regularly verify backup configurations.

üìò Scenario #50: Failed Deployment Due to Image Pulling Issues
Category: Cluster Management
Environment: K8s v1.22, custom Docker registry
Scenario Summary: Deployment failed due to image pulling issues from a custom Docker registry.
What Happened: A deployment failed because Kubernetes could not pull images from a custom Docker registry due to misconfigured image pull secrets.
Diagnosis Steps:
	‚Ä¢ Observed ImagePullBackOff errors for the failing pods.
	‚Ä¢ Checked image pull secrets and registry configuration.
Root Cause: Incorrect or missing image pull secrets for accessing the custom registry.
Fix/Workaround:
	‚Ä¢ Corrected the image pull secrets in the deployment YAML.
	‚Ä¢ Re-deployed the application.
Lessons Learned: Image pull secrets must be configured properly for private registries.
How to Avoid:
	‚Ä¢ Always verify image pull secrets for private registries.
	‚Ä¢ Use Kubernetes secrets management tools for image pull credentials.
```
