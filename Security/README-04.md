üìò Scenario #276: JWT Token Replay Attack in Webhook Auth
Category: Security
Environment: Kubernetes v1.21, AKS
Summary: Reused JWT tokens from intercepted API requests were used to impersonate authorized users.
What Happened: A webhook-based authentication system accepted JWTs without checking their freshness. Tokens were reused in replay attacks.
Diagnosis Steps:
	‚Ä¢ Inspected API server logs for duplicate token use.
	‚Ä¢ Found repeated requests with same JWT from different IPs.
	‚Ä¢ Correlated with the webhook server not validating expiry/nonce.
Root Cause: Webhook did not validate tokens properly.
Fix/Workaround:
	‚Ä¢ Updated webhook to validate expiry and nonce in tokens.
	‚Ä¢ Rotated keys and invalidated sessions.
Lessons Learned: Token reuse must be considered in authentication systems.
How to Avoid:
	‚Ä¢ Use time-limited tokens.
	‚Ä¢ Implement replay protection with nonces or one-time tokens.

üìò Scenario #277: Container With Hardcoded SSH Keys
Category: Security
Environment: Kubernetes v1.20, On-Prem
Summary: A base image included hardcoded SSH keys which allowed attackers lateral access between environments.
What Happened: A developer reused a base image with an embedded SSH private key. This key was used across environments and eventually leaked.
Diagnosis Steps:
	‚Ä¢ Analyzed image layers with Trivy.
	‚Ä¢ Found hardcoded private key in /root/.ssh/id_rsa.
	‚Ä¢ Tested and confirmed it allowed access to multiple systems.
Root Cause: Insecure base image with sensitive files included.
Fix/Workaround:
	‚Ä¢ Rebuilt images without sensitive content.
	‚Ä¢ Rotated all affected SSH keys.
Lessons Learned: Never embed sensitive credentials in container images.
How to Avoid:
	‚Ä¢ Scan images before use.
	‚Ä¢ Use multistage builds to exclude dev artifacts.

üìò Scenario #278: Insecure Helm Chart Defaults
Category: Security
Environment: Kubernetes v1.24, GKE
Summary: A popular Helm chart had insecure defaults, like exposing dashboards or running as root.
What Happened: A team installed a chart from a public Helm repo and unknowingly exposed a dashboard on the internet.
Diagnosis Steps:
	‚Ä¢ Discovered open dashboards in a routine scan.
	‚Ä¢ Reviewed Helm chart‚Äôs default values.
	‚Ä¢ Found insecure values.yaml configurations.
Root Cause: Use of Helm chart without overriding insecure defaults.
Fix/Workaround:
	‚Ä¢ Overrode defaults in values.yaml.
	‚Ä¢ Audited Helm charts for misconfigurations.
Lessons Learned: Don‚Äôt trust defaults‚Äîvalidate every Helm deployment.
How to Avoid:
	‚Ä¢ Read charts carefully before applying.
	‚Ä¢ Maintain internal forks of public charts with hardened defaults.

üìò Scenario #279: Shared Cluster with Overlapping Namespaces
Category: Security
Environment: Kubernetes v1.22, Shared Dev Cluster
Summary: Multiple teams used the same namespace naming conventions, causing RBAC overlaps and security concerns.
What Happened: Two teams created namespaces with the same name across dev environments. RBAC rules overlapped and one team accessed another‚Äôs workloads.
Diagnosis Steps:
	‚Ä¢ Reviewed RBAC bindings across namespaces.
	‚Ä¢ Found conflicting roles due to reused namespace names.
	‚Ä¢ Inspected access logs and verified misuse.
Root Cause: Lack of namespace naming policies in a shared cluster.
Fix/Workaround:
	‚Ä¢ Introduced prefix-based namespace naming (e.g., team1-dev).
	‚Ä¢ Scoped RBAC permissions tightly.
Lessons Learned: Namespace naming is security-sensitive in shared clusters.
How to Avoid:
	‚Ä¢ Enforce naming policies.
	‚Ä¢ Use automated namespace creation with templates.

üìò Scenario #280: CVE Ignored in Base Image for Months
Category: Security
Environment: Kubernetes v1.23, AWS
Summary: A known CVE affecting the base image used by multiple services remained unpatched due to no alerting.
What Happened: A vulnerability in glibc went unnoticed for months because there was no automated CVE scan or alerting. Security only discovered it during a quarterly audit.
Diagnosis Steps:
	‚Ä¢ Scanned container image layers manually.
	‚Ä¢ Confirmed multiple CVEs, including critical ones.
	‚Ä¢ Traced image origin to a legacy Dockerfile.
Root Cause: No vulnerability scanning in CI/CD.
Fix/Workaround:
	‚Ä¢ Integrated Clair + Trivy scans into CI/CD pipelines.
	‚Ä¢ Setup Slack alerts for critical CVEs.
Lessons Learned: Continuous scanning is vital to security hygiene.
How to Avoid:
	‚Ä¢ Integrate image scanning into build pipelines.
	‚Ä¢ Monitor CVE databases for base images regularly.

üìò Scenario #281: Misconfigured PodSecurityPolicy Allowed Privileged Containers
Category: Security
Environment: Kubernetes v1.21, On-Prem Cluster
Summary: Pods were running with privileged: true due to a permissive PodSecurityPolicy (PSP) left enabled during testing.
What Happened: Developers accidentally left a wide-open PSP in place that allowed privileged containers, host networking, and host path mounts. This allowed a compromised container to access host files.
Diagnosis Steps:
	‚Ä¢ Audited active PSPs.
	‚Ä¢ Identified a PSP with overly permissive rules.
	‚Ä¢ Found pods using privileged: true.
Root Cause: Lack of PSP review before production deployment.
Fix/Workaround:
	‚Ä¢ Removed the insecure PSP.
	‚Ä¢ Implemented a restrictive default PSP.
	‚Ä¢ Migrated to PodSecurityAdmission after PSP deprecation.
Lessons Learned: Security defaults should be restrictive, not permissive.
How to Avoid:
	‚Ä¢ Review PSP or PodSecurity configurations regularly.
	‚Ä¢ Implement strict admission control policies.

üìò Scenario #282: GitLab Runners Spawning Privileged Containers
Category: Security
Environment: Kubernetes v1.23, GitLab CI on EKS
Summary: GitLab runners were configured to run privileged containers to support Docker-in-Docker (DinD), leading to a high-risk setup.
What Happened: A developer pipeline was hijacked and used to build malicious images, which had access to the underlying node due to privileged mode.
Diagnosis Steps:
	‚Ä¢ Detected unusual image pushes to private registry.
	‚Ä¢ Reviewed runner configuration ‚Äì found privileged: true enabled.
	‚Ä¢ Audited node access logs.
Root Cause: Runners configured with elevated privileges for convenience.
Fix/Workaround:
	‚Ä¢ Disabled DinD and used Kaniko for builds.
	‚Ä¢ Set runner securityContext to avoid privilege escalation.
Lessons Learned: Privileged mode should be a last resort.
How to Avoid:
	‚Ä¢ Avoid using DinD where possible.
	‚Ä¢ Use rootless build tools like Kaniko or Buildah.

üìò Scenario #283: Kubernetes Secrets Mounted in World-Readable Volumes
Category: Security
Environment: Kubernetes v1.24, GKE
Summary: Secret volumes were mounted with 0644 permissions, allowing any user process inside the container to read them.
What Happened: A poorly configured application image had other processes running that could access mounted secrets (e.g., service credentials).
Diagnosis Steps:
	‚Ä¢ Reviewed mounted secret volumes and permissions.
	‚Ä¢ Identified 0644 file mode on mounted files.
	‚Ä¢ Verified multiple processes in the pod could access the secrets.
Root Cause: Secret volume default mode wasn't overridden.
Fix/Workaround:
	‚Ä¢ Set defaultMode: 0400 on all secret volumes.
	‚Ä¢ Isolated processes via containers.
Lessons Learned: Least privilege applies to file access too.
How to Avoid:
	‚Ä¢ Set correct permissions on secret mounts.
	‚Ä¢ Use multi-container pods to isolate secrets access.

üìò Scenario #284: Kubelet Port Exposed on Public Interface
Category: Security
Environment: Kubernetes v1.20, Bare Metal
Summary: Kubelet was accidentally exposed on port 10250 to the public internet, allowing unauthenticated metrics and logs access.
What Happened: Network misconfiguration led to open Kubelet ports without authentication. Attackers scraped pod logs and exploited the /exec endpoint.
Diagnosis Steps:
	‚Ä¢ Scanned node ports using nmap.
	‚Ä¢ Discovered open port 10250 without TLS.
	‚Ä¢ Verified logs and metrics access externally.
Root Cause: Kubelet served insecure API without proper firewall rules.
Fix/Workaround:
	‚Ä¢ Enabled Kubelet authentication and authorization.
	‚Ä¢ Restricted access via firewall and node security groups.
Lessons Learned: Never expose internal components publicly.
How to Avoid:
	‚Ä¢ Audit node ports regularly.
	‚Ä¢ Harden Kubelet with authN/authZ and TLS.

üìò Scenario #285: Cluster Admin Bound to All Authenticated Users
Category: Security
Environment: Kubernetes v1.21, AKS
Summary: A ClusterRoleBinding accidentally granted cluster-admin to all authenticated users due to system:authenticated group.
What Happened: A misconfigured YAML granted admin access broadly, bypassing intended RBAC restrictions.
Diagnosis Steps:
	‚Ä¢ Audited ClusterRoleBindings.
	‚Ä¢ Found binding: subjects: kind: Group, name: system:authenticated.
	‚Ä¢ Verified users could create/delete resources cluster-wide.
Root Cause: RBAC misconfiguration during onboarding automation.
Fix/Workaround:
	‚Ä¢ Deleted the binding immediately.
	‚Ä¢ Implemented an RBAC policy validation webhook.
Lessons Learned: Misuse of built-in groups can be catastrophic.
How to Avoid:
	‚Ä¢ Avoid using broad group bindings.
	‚Ä¢ Implement pre-commit checks for RBAC files.

üìò Scenario #286: Webhook Authentication Timing Out, Causing Denial of Service
Category: Security
Environment: Kubernetes v1.22, EKS
Summary: Authentication webhook for custom RBAC timed out under load, rejecting valid users and causing cluster-wide issues.
What Happened: Spike in API requests caused the external webhook server to time out. This led to mass access denials and degraded API server performance.
Diagnosis Steps:
	‚Ä¢ Checked API server logs for webhook timeout messages.
	‚Ä¢ Monitored external auth service ‚Äì saw 5xx errors.
	‚Ä¢ Replayed request load to replicate.
Root Cause: Auth webhook couldn't scale with API server traffic.
Fix/Workaround:
	‚Ä¢ Increased webhook timeouts and horizontal scaling.
	‚Ä¢ Added local caching for frequent identities.
Lessons Learned: External dependencies can introduce denial of service risks.
How to Avoid:
	‚Ä¢ Stress-test webhooks.
	‚Ä¢ Use token-based or in-cluster auth where possible.

üìò Scenario #287: CSI Driver Exposing Node Secrets
Category: Security
Environment: Kubernetes v1.24, CSI Plugin (AWS Secrets Store)
Summary: Misconfigured CSI driver exposed secrets on hostPath mount accessible to privileged pods.
What Happened: Secrets mounted via the CSI driver were not isolated properly, allowing another pod with hostPath access to read them.
Diagnosis Steps:
	‚Ä¢ Reviewed CSI driver logs and configurations.
	‚Ä¢ Found secrets mounted in shared path (/var/lib/...).
	‚Ä¢ Identified privilege escalation path via hostPath.
Root Cause: CSI driver exposed secrets globally on node filesystem.
Fix/Workaround:
	‚Ä¢ Scoped CSI mounts with per-pod directories.
	‚Ä¢ Disabled hostPath access for workloads.
Lessons Learned: CSI drivers must be hardened like apps.
How to Avoid:
	‚Ä¢ Test CSI secrets exposure under threat models.
	‚Ä¢ Restrict node-level file access via policies.

üìò Scenario #288: EphemeralContainers Used for Reconnaissance
Category: Security
Environment: Kubernetes v1.25, GKE
Summary: A compromised user deployed ephemeral containers to inspect and copy secrets from running pods.
What Happened: A user with access to ephemeralcontainers feature spun up containers in critical pods and read mounted secrets and env vars.
Diagnosis Steps:
	‚Ä¢ Audited API server calls to ephemeralcontainers API.
	‚Ä¢ Found suspicious container launches.
	‚Ä¢ Inspected shell history and accessed secrets.
Root Cause: Overprivileged user with ephemeralcontainers access.
Fix/Workaround:
	‚Ä¢ Removed permissions to ephemeral containers for all roles.
	‚Ä¢ Set audit policies for their use.
Lessons Learned: New features introduce new attack vectors.
How to Avoid:
	‚Ä¢ Lock down access to new APIs.
	‚Ä¢ Monitor audit logs for container injection attempts.

üìò Scenario #289: hostAliases Used for Spoofing Internal Services
Category: Security
Environment: Kubernetes v1.22, On-Prem
Summary: Malicious pod used hostAliases to spoof internal service hostnames and intercept requests.
What Happened: An insider attack modified /etc/hosts in a pod using hostAliases to redirect requests to attacker-controlled services.
Diagnosis Steps:
	‚Ä¢ Reviewed pod manifests with hostAliases.
	‚Ä¢ Captured outbound DNS traffic and traced redirections.
	‚Ä¢ Detected communication with rogue internal services.
Root Cause: Abuse of hostAliases field in PodSpec.
Fix/Workaround:
	‚Ä¢ Disabled use of hostAliases via OPA policies.
	‚Ä¢ Logged all pod specs with custom host entries.
Lessons Learned: Host file spoofing can bypass DNS-based security.
How to Avoid:
	‚Ä¢ Restrict or disallow use of hostAliases.
	‚Ä¢ Rely on service discovery via DNS only.

üìò Scenario #290: Privilege Escalation via Unchecked securityContext in Helm Chart
Category: Security
Environment: Kubernetes v1.21, Helm v3.8
Summary: A third-party Helm chart allowed setting arbitrary securityContext, letting users run pods as root in production.
What Happened: A chart exposed securityContext overrides without constraints. A developer added runAsUser: 0during deployment, leading to root-level containers.
Diagnosis Steps:
	‚Ä¢ Inspected Helm chart values and rendered manifests.
	‚Ä¢ Detected containers with runAsUser: 0.
	‚Ä¢ Reviewed change logs in GitOps pipeline.
Root Cause: Chart did not validate or restrict securityContext fields.
Fix/Workaround:
	‚Ä¢ Forked chart and restricted overrides via schema.
	‚Ä¢ Implemented OPA Gatekeeper to block root containers.
Lessons Learned: Helm charts can be as dangerous as code.
How to Avoid:
	‚Ä¢ Validate all chart values.
	‚Ä¢ Use policy engines to restrict risky configurations.

üìò Scenario #291: Service Account Token Leakage via Logs
Category: Security
Environment: Kubernetes v1.23, AKS
Summary: Application inadvertently logged its mounted service account token, exposing it to log aggregation systems.
What Happened: A misconfigured logging library dumped all environment variables and mounted file contents at startup, including the token from /var/run/secrets/kubernetes.io/serviceaccount/token.
Diagnosis Steps:
	‚Ä¢ Searched central logs for token patterns.
	‚Ä¢ Confirmed multiple logs contained valid JWTs.
	‚Ä¢ Validated token usage in audit logs.
Root Cause: Poor logging hygiene in application code.
Fix/Workaround:
	‚Ä¢ Rotated all impacted service account tokens.
	‚Ä¢ Added environment and file sanitization to logging library.
Lessons Learned: Tokens are sensitive credentials and should never be logged.
How to Avoid:
	‚Ä¢ Add a startup check to prevent token exposure.
	‚Ä¢ Use static analysis or OPA to block risky mounts/logs.

üìò Scenario #292: Escalation via Editable Validating WebhookConfiguration
Category: Security
Environment: Kubernetes v1.24, EKS
Summary: User with edit rights on a validating webhook modified it to bypass critical security policies.
What Happened: An internal user reconfigured the webhook to always return allow, disabling cluster-wide security checks.
Diagnosis Steps:
	‚Ä¢ Detected anomaly: privileged pods getting deployed.
	‚Ä¢ Checked webhook configuration history in GitOps.
	‚Ä¢ Verified that failurePolicy: Ignore and static allow logic were added.
Root Cause: Lack of control over webhook configuration permissions.
Fix/Workaround:
	‚Ä¢ Restricted access to ValidatingWebhookConfiguration objects.
	‚Ä¢ Added checksums to webhook definitions in GitOps.
Lessons Learned: Webhooks must be tightly controlled to preserve cluster security.
How to Avoid:
	‚Ä¢ Lock down RBAC access to webhook configurations.
	‚Ä¢ Monitor changes with alerts and diff checks.

üìò Scenario #293: Stale Node Certificates After Rejoining Cluster
Category: Security
Environment: Kubernetes v1.21, Kubeadm-based cluster
Summary: A node was rejoined to the cluster using a stale certificate, giving it access it shouldn't have.
What Happened: A node that was previously removed was added back using an old /var/lib/kubelet/pki/kubelet-client.crt, which was still valid.
Diagnosis Steps:
	‚Ä¢ Compared certificate expiry and usage.
	‚Ä¢ Found stale kubelet cert on rejoined node.
	‚Ä¢ Verified node had been deleted previously.
Root Cause: Old credentials not purged before node rejoin.
Fix/Workaround:
	‚Ä¢ Manually deleted old certificates from the node.
	‚Ä¢ Set short TTLs for client certificates.
Lessons Learned: Node certs should be one-time-use and short-lived.
How to Avoid:
	‚Ä¢ Rotate node credentials regularly.
	‚Ä¢ Use automation to purge sensitive files before rejoining.

üìò Scenario #294: ArgoCD Exploit via Unverified Helm Charts
Category: Security
Environment: Kubernetes v1.24, ArgoCD
Summary: ArgoCD deployed a malicious Helm chart that added privileged pods and container escape backdoors.
What Happened: A team added a new Helm repo that wasn‚Äôt verified. The chart had post-install hooks that ran containers with host access.
Diagnosis Steps:
	‚Ä¢ Found unusual pods using hostNetwork and hostPID.
	‚Ä¢ Traced deployment to ArgoCD application with external chart.
	‚Ä¢ Inspected chart source ‚Äì found embedded malicious hooks.
Root Cause: Lack of chart verification or provenance checks.
Fix/Workaround:
	‚Ä¢ Removed the chart and all related workloads.
	‚Ä¢ Enabled Helm OCI signatures and repo allow-lists.
Lessons Learned: Supply chain security is critical, even with GitOps.
How to Avoid:
	‚Ä¢ Only use verified or internal Helm repos.
	‚Ä¢ Enable ArgoCD Helm signature verification.

üìò Scenario #295: Node Compromise via Insecure Container Runtime
Category: Security
Environment: Kubernetes v1.22, CRI-O on Bare Metal
Summary: A CVE in the container runtime allowed a container breakout, leading to full node compromise.
What Happened: An attacker exploited CRI-O vulnerability (CVE-2022-0811) that allowed containers to overwrite host paths via sysctl injection.
Diagnosis Steps:
	‚Ä¢ Detected abnormal node CPU spike and external traffic.
	‚Ä¢ Inspected containers ‚Äì found sysctl modifications.
	‚Ä¢ Cross-verified with known CVEs.
Root Cause: Unpatched CRI-O vulnerability and default seccomp profile disabled.
Fix/Workaround:
	‚Ä¢ Upgraded CRI-O to patched version.
	‚Ä¢ Enabled seccomp and AppArmor by default.
Lessons Learned: Container runtimes must be hardened and patched like any system component.
How to Avoid:
	‚Ä¢ Automate CVE scanning for runtime components.
	‚Ä¢ Harden runtimes with security profiles.

üìò Scenario #296: Workload with Wildcard RBAC Access to All Secrets
Category: Security
Environment: Kubernetes v1.23, Self-Hosted
Summary: A microservice was granted get and list access to all secrets cluster-wide using *.
What Happened: Developers gave overly broad access to a namespace-wide controller, leading to accidental exposure of unrelated team secrets.
Diagnosis Steps:
	‚Ä¢ Audited RBAC for secrets access.
	‚Ä¢ Found RoleBinding with resources: [‚Äúsecrets‚Äù], verbs: [‚Äúget‚Äù, ‚Äúlist‚Äù], resourceNames: ["*"].
Root Cause: Overly broad RBAC permissions in service manifest.
Fix/Workaround:
	‚Ä¢ Replaced wildcard permissions with explicit named secrets.
	‚Ä¢ Enabled audit logging on all secrets API calls.
Lessons Learned: * in RBAC is often overkill and dangerous.
How to Avoid:
	‚Ä¢ Use least privilege principle.
	‚Ä¢ Validate RBAC via CI/CD linting tools.

üìò Scenario #297: Malicious Init Container Used for Reconnaissance
Category: Security
Environment: Kubernetes v1.25, GKE
Summary: A pod was launched with a benign main container and a malicious init container that copied node metadata.
What Happened: Init container wrote node files (e.g., /etc/resolv.conf, cloud instance metadata) to an external bucket before terminating.
Diagnosis Steps:
	‚Ä¢ Enabled audit logs for object storage.
	‚Ä¢ Traced writes back to a pod with suspicious init container.
	‚Ä¢ Reviewed init container image ‚Äì found embedded exfil logic.
Root Cause: Lack of validation on init container behavior.
Fix/Workaround:
	‚Ä¢ Blocked unknown container registries via policy.
	‚Ä¢ Implemented runtime security agents to inspect init behavior.
Lessons Learned: Init containers must be treated as full-fledged security risks.
How to Avoid:
	‚Ä¢ Verify init container images and registries.
	‚Ä¢ Use runtime tools (e.g., Falco) for behavior analysis.

üìò Scenario #298: Ingress Controller Exposed /metrics Without Auth
Category: Security
Environment: Kubernetes v1.24, NGINX Ingress
Summary: Prometheus scraping endpoint /metrics was exposed without authentication and revealed sensitive internal details.
What Happened: A misconfigured ingress rule allowed external users to access /metrics, which included upstream paths, response codes, and error logs.
Diagnosis Steps:
	‚Ä¢ Scanned public URLs.
	‚Ä¢ Found /metrics exposed to unauthenticated traffic.
	‚Ä¢ Inspected NGINX ingress annotations.
Root Cause: Ingress annotations missing auth and whitelist rules.
Fix/Workaround:
	‚Ä¢ Applied IP whitelist and basic auth for /metrics.
	‚Ä¢ Added network policies to restrict access.
Lessons Learned: Even observability endpoints need protection.
How to Avoid:
	‚Ä¢ Enforce auth for all public endpoints.
	‚Ä¢ Separate internal vs. external monitoring targets.

üìò Scenario #299: Secret Stored in ConfigMap by Mistake
Category: Security
Environment: Kubernetes v1.23, AKS
Summary: A sensitive API key was accidentally stored in a ConfigMap instead of a Secret, making it visible in plain text.
What Happened: Developer used a ConfigMap for application config, and mistakenly included an apiKey in it. Anyone with view rights could read it.
Diagnosis Steps:
	‚Ä¢ Reviewed config files for plaintext secrets.
	‚Ä¢ Found hardcoded credentials in ConfigMap YAML.
Root Cause: Misunderstanding of Secret vs. ConfigMap usage.
Fix/Workaround:
	‚Ä¢ Moved key to a Kubernetes Secret.
	‚Ä¢ Rotated exposed credentials.
Lessons Learned: Educate developers on proper resource usage.
How to Avoid:
	‚Ä¢ Lint manifests to block secrets in ConfigMaps.
	‚Ä¢ Train developers in security best practices.

üìò Scenario #300: Token Reuse After Namespace Deletion and Recreation
Category: Security
Environment: Kubernetes v1.24, Self-Hosted
Summary: A previously deleted namespace was recreated, and old tokens (from backups) were still valid and worked.
What Happened: Developer restored a backup including secrets from a deleted namespace. The token was still valid and allowed access to cluster resources.
Diagnosis Steps:
	‚Ä¢ Found access via old token in logs.
	‚Ä¢ Verified namespace was deleted, then recreated with same name.
	‚Ä¢ Checked secrets in restored backup.
Root Cause: Static tokens persisted after deletion and recreation.
Fix/Workaround:
	‚Ä¢ Rotated all tokens after backup restore.
	‚Ä¢ Implemented TTL-based token policies.
Lessons Learned: Tokens must be invalidated after deletion or restore.
How to Avoid:
	‚Ä¢ Don‚Äôt restore old secrets blindly.
	‚Ä¢ Rotate and re-issue credentials post-restore.
